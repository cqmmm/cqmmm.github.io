{"posts":[{"title":"Cisco","text":"在 Cisco 中，交换机设备主要分为以下几种模式： 用户模式，刚进入设备都是用户模式，只能用来看一些统计信息 1Switch&gt; 特权模式，可以查看并修改配置 12Switch&gt;enableSwitch# 全局配置模式，可以修改主机名等 12Switch#config terminalSwitch(config)# 接口模式，可以对指定接口进行配置 12Switch(config)#int f0/1Switch(config-if)# 1. Vlan划分Vlan 即虚拟局域网，能够有效防止广播风暴的发生。 Vlan 的划分是在交换机上进行，实际上就是给指定端口进行划分。 在划分 vlan 前，可以看到左边的主机与右边的主机都是能够通信的 接下来将左右边的主机分别划分为 vlan10、vlan20，进入全局配置模式 1234567891011121314151617# 创建vlanSwitch(config)#vlan 10Switch(config-vlan)#exitSwitch(config)#vlan 20Switch(config-vlan)#exit# 划分vlanSwitch(config)#int range f0/1-3Switch(config-if-range)#switchport mode accessSwitch(config-if-range)#switchport access vlan 10Switch(config-if-range)#exitSwitch(config)#int range f0/4-6Switch(config-if-range)#switchport mode accessSwitch(config-if-range)#switchport access vlan 20Switch(config-if-range)#exit# 查看配置Switch#show running-config... 再去进行 ping 测试连通性，会发现只能 ping 通同一 vlan 下的主机 2. 分类地址的通信IP 地址分为五类，如果不是同一网络号下的地址则不能实现通信，如下图 这时候可以在中间添加一台路由器，来作为各主机之间的默认网关（默认路由），即可实现通信。 路由配置 123456789Router&gt;enRouter#conf t# 进入端口进行配置Router(config)#int g0/0Router(config-if)#ip add 192.168.88.1 255.255.255.0Router(config-if)#no shutdownRouter(config)#int g0/1Router(config-if)#ip add 172.16.0.1 255.255.0.0Router(config-if)#no shutdown 主机配置默认网关 连通性测试，第一个 ICMP 包会丢失是因为路由器会发送 ARP 包去寻找目标主机的 Mac 地址，所以在规定时间内没有收到回送报文就会超时 3. 划分子网子网的划分实际上就是在原有的基础上进行更小的划分，划分子网后，通过使用掩码，把子网隐藏起来，使得从外部看网络没有变化，这就是子网掩码。 如下图，左右边主机看似好像是在同一网络号下，但由于子网掩码是 255.255.255.192，所以之间是不能够通信的，只有 IP 地址与子网掩码进行位与的操作之后网络号相同的才能够进行通信，例如右边两台主机与子网掩码进行位与后网络号都是 192.168.88.64，即处于同一网络号下能够进行通信。 每个子网内可以使用的 IP 个数由子网号来判断，例如 255.255.255.192，192 转化为二进制为 11000000，有 6 个 0，即 IP 个数为 2 ** 6 = 64 个，常见子网掩码有以下几个（子网掩码：可用主机 IP 个数）： 255.255.255.128：126 255.255.255.192：62 255.255.255.224：30 255.255.255.240：14 255.255.255.248：6 255.255.255.252：2 要解决上图网络通信问题，只需要在中间添加个路由器即可。 路由器配置 12345678Router&gt;enRouter#conf tRouter(config)#int g0/0Router(config-if)#ip add 192.168.88.12 255.255.255.192Router(config-if)#no shutdownRouter(config)#int g0/1Router(config-if)#ip add 192.168.88.102 255.255.255.192Router(config-if)#no shutdown 主机添加默认网关 测试连通性 4. 构造超网（路由聚合）CIDR 地址格式为 xxx.xxx.xxx.xxx/xx，如 192.168.88.10/24，24 用来判断地址块信息，二进制后的 IP 前 24 位不变，后 8 位为 0，即可得出最小地址，后 8 位为 1 时，即可得出最大地址，即 192.168.88.0、192.168.88.255。 如下图，再没有配置静态路由时，上下网络的主机是不能够与右边的主机通信的，这时候添加静态路由，设置下一跳地址即可实现通信。 左边路由配置 1Router(config)#ip route 192.168.88.196 255.255.255.252 192.168.88.194 右边路由配置 12Router(config)#ip route 192.168.88.0 255.255.255.128 192.168.88.193Router(config)#ip route 192.168.88.128 255.255.255.192 192.168.88.193 测试连通性 从右边路由配置可以看出，两条静态路由可以进行路由聚合，即 192.168.88 前三个字节是相同的，那么聚合后的地址块为 192.168.88.0/24，路由配置为 1Router(config)#ip route 192.168.88.0 255.255.255.0 192.168.88.193 5. 特定路由和默认路由特定路由即将数据报转发给特定的目标地址，而默认路由即除了特定的目标地址外都转发给默认的路由，流程如下图。 在以下网络中，目的地址只要是 192.168.4 网段，都会下一跳给 R1 路由器，再由 R2 进行路由转发。 R1 路由配置 12Router(config)#ip route 192.168.4.1 255.255.255.255 10.0.1.1Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.0.1 R2 路由配置 1Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.1.2 R3 路由配置 123Router(config)#ip route 192.168.0.0 255.255.255.0 10.0.0.2Router(config)#ip route 192.168.4.0 255.255.255.0 10.0.0.2Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.2.2 R4 路由配置 1Router(config)#ip route 0.0.0.0 0.0.0.0 10.0.2.1 6. 黑洞路由如果静态路由配置了不存在的网段，那么就可能会导致路由环路的问题，这时候可以添加一个黑洞路由进行解决，如下，只要目的地址是 192.168.88.0/24 地址快的数据包都会被丢弃。 1Router(config)#ip route 192.168.88.0 255.255.255.0 null0 7. 路由信息协议RIPRIP 是一种分布式的基于距离向量的路由选择协议，属于内部网关协议，主要适用于小规模的网络环境。 在 RIP 协议中，每次数据的发送都会选择跳数最少的路由，即经过节点最少的路由线路。 如下网络中，在给所有主机和路由端口配好 IP 后，还是不能够通信的，给每个路由器设置宣告地址段，即配置 RIP，路由器就会发送 RIP 报文来算出每个网段之间的最短路径，并写入路由表中，最终路径为 PC1 -&gt; R1 -&gt; R3 -&gt; PC2。 在每个路由器设置 RIP 1234Router(config)#router ripRouter(config-router)#network 10.0.0.0Router(config-router)#network 192.168.2.0Router(config-router)#network 192.168.1.0 路由表中 R 代表的就是 RIP 协议写入的路由 1Router#show ip route 8. 开放最短路径优先OSFPOSPF 也属于内部网关协议，用于在单一自治系统 AS 内决策路由，是基于链路状态的动态路由选择协议。 在 OSPF 中，当链路状态发生变化就会采用洪泛法去更新信息，每个路由器都与相邻的路由器成邻居关系，邻居再相互发送链路状态信息形成邻接关系，之后各自根据最短路径算法算出路由，放在OSPF路由表，OSPF路由与其他路由比较后优的加入全局路由表。整个过程使用了五种报文、三个阶段、四张表。 在以下网络中，通过 OSPF 配置路由后的路径为 PC1 -&gt; R1 -&gt; R2 -&gt; R3 -&gt; PC2，因为 R1 与 R3 之间采用的是串行接口，是低速链路。 R1 路由器配置 123456# 100为指定进程，可在1-65535之间选择Router(config)#route ospf 100# 0.0.0.255为反子网掩码，area为地区号Router(config-router)#network 10.0.0.0 0.0.0.255 area 0Router(config-router)#network 192.168.1.0 0.0.0.255 area 0Router(config-router)#network 10.0.1.0 0.0.0.255 area 0 R2 路由器配置 1234Router(config)#route ospf 100Router(config-router)#network 10.0.0.0 0.0.0.255 area 0Router(config-router)#network 10.0.1.0 0.0.0.255 area 0Router(config-router)#network 10.0.2.0 0.0.0.255 area 0 R3 路由器配置 1234Router(config)#route ospf 100Router(config-router)#network 10.0.1.0 0.0.0.255 area 0Router(config-router)#network 10.0.2.0 0.0.0.255 area 0Router(config-router)#network 192.168.2.0 0.0.0.255 area 0 通过测试可以看到路由线路的选择 如果线路断了会自动更新路由表，这时候再看线路的选择就会不一样，但依旧可以联通 9. 边界网关协议BGPBGP 是一种外部网关协议，是基于自治系统 AS 之间的路由协议，BGP交换的网络可达性信息提供了足够的信息来检测路由回路并根据性能优先和策略约束对路由进行决策。 每个 AS 会设置一个 BGP 发言人，即一个路由器，用来与相邻 AS 的 BGP 发言人交换网络可达性的信息。 如下图网络，分别有三个 AS，要将不同 AS 之间进行联通，就可以通过 BGP 来实现。 AS 100 中路由配置 1234567# 100为自治系统编号Router(config)#route bgp 100# 指定邻居，IP即与相邻路由器的连接端口IP，200/300为相邻AS的编号Router(config-router)#neighbor 10.0.1.2 remote-as 200Router(config-router)#neighbor 10.0.0.2 remote-as 300# 将自身AS中的网络信息通报出去Router(config-router)#network 192.168.10.0 mask 255.255.255.0 AS 200 中路由配置 123Router(config)#route bgp 200Router(config-router)#neighbor 10.0.1.1 remote-as 100Router(config-router)#network 192.168.20.0 mask 255.255.255.0 AS 300 中路由配置 123Router(config)#route bgp 300Router(config-router)#neighbor 10.0.0.1 remote-as 100Router(config-router)#network 192.168.30.0 mask 255.255.255.0 查看 BGP 路由信息，可以看到 B 代表的 BGP 路由信息 12345Router(config-router)#do show ip route...B 192.168.10.0/24 [20/0] via 10.0.1.1, 00:00:00B 192.168.30.0/24 [20/0] via 10.0.1.1, 00:00:00... 10. 多臂路由实现vlan之间的通信vlan 之间如果网段不同，就无法进行通信，可以在中间添加一台路由器来做双方的网关，已多臂路由的方式实现不同 vlan 不同网段之间的通信。 多臂路由即一个端口对应一个 vlan，如下图。 11. 单臂路由实现vlan之间的通信单臂路由与多臂路由的原理是相同的，区别在于在多臂路由中，交换机到路由器的链路配置的是 access，在单臂路由中用一个主干 trunk 链路替代。 在连接交换机的路由器上配置 12345678910111213# 创建逻辑子接口Router(config)#int g0/0.1# 可接受id为10的vlan数据包，并封装成802.1q桢进行转发Router(config-subif)#encapsulation dot1Q 10Router(config-subif)#ip add 192.168.1.254 255.255.255.0Router(config-subif)#exitRouter(config)#int g0/0.2Router(config-subif)#encapsulation dot1Q 20Router(config-subif)#ip add 192.168.2.254 255.255.255.0Router(config-subif)#exit# 开启接口Router(config)#int g0/0Router(config-if)#no shutdown 在交换机上将与路由器相接的端口设置为 trunk 口 12Switch(config)#int f0/7Switch(config-if)#switchport mode trunk 连通性测试 12. 三层交换机实现vlan之间的通信三层交换机就是具有部分路由器功能的交换机。 三层交换机配置 1234567891011121314151617181920212223Switch(config)#vlan 10Switch(config-vlan)#exit# 进入vlan添加IPSwitch(config)#int vlan10Switch(config-if)#ip add 192.168.1.254 255.255.255.0Switch(config-if)#no shutdownSwitch(config-if)#exitSwitch(config)#vlan 20Switch(config-vlan)#exitSwitch(config)#int vlan20Switch(config-if)#ip add 192.168.2.254 255.255.255.0Switch(config-if)#no shutdownSwitch(config-if)#exit Switch(config)#int range f0/1-3Switch(config-if-range)#switchport mode access Switch(config-if-range)#switchport access vlan 10Switch(config-if-range)#Switch(config-if-range)#exitSwitch(config)#int range f0/4-6Switch(config-if-range)#switchport mode access Switch(config-if-range)#switchport access vlan 20# 开启路由功能Switch(config)#ip routing 测试连通性","link":"/2024/02/18/cisco/"},{"title":"Docker","text":"一. Docker介绍1.1 Docker是什么 Docker 是一个开源的应用容器引擎，基于 Go 语言并遵从 Apache2.0 协议开源。 Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 二. Docker的基本操作2.1 Docker的安装121.安装Docker依赖环境yum -y install yum-utils device-mapper-persistent-data lvm2 122.下载Docker镜像源yum-config-manager --add-repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo 1233.安装Dockeryum makecache fastyum -y install docker-ce 1234.启动Docker并设为开机自启systemctl start dockersystemctl enable docker 123455.测试运行hello worlddocker run hello-world...Hello from Docker!... 2.2 Docker的中央仓库1、Docker官方（hub.docker.com）：镜像最全，下载最慢 2、国内镜像网站：网易蜂巢（163yun.com） ​ daocloud（hub.daocloud.io） 3、公司内部私服拉取镜像： 123456781.配置/etc/docker/daemon.json{ &quot;registry-mirrors&quot;:[&quot;https://registry.docker-cn.com&quot;], &quot;insecure-registries&quot;:[&quot;ip:port&quot;]}2.重启两个服务systemctl daemon-reloadsystemctl restart docker 2.3 Docker镜像的操作12341.拉取镜像到本地docker pull 镜像名称[:tag]举个栗子docker pull daocloud.io/library/nginx:1.18.0 122.查看本地镜像docker images 123.删除本地镜像docker rmi 镜像id(image id) 1234.本地镜像的导入导出docker save -o 导出路径 镜像iddocker load -i 镜像文件 125.给镜像添加标签docker tag 镜像id repository:tag 2.4 Docker容器的操作1234561.运行容器docker run 镜像id/镜像名称[:tag]docker run -d -p 宿主机端口:容器端口 --name 容器名称 镜像id/镜像名称[:tag]-d:代表后台运行容器-p:为了映射linux端口和容器端口--name:指定容器名称 12342.查看容器docker ps -qa-a:查看所有容器，包括没有运行的-q:只看容器id 1233.查看容器日志docker logs -f 容器id-f:可以滚动查看容器日志的最后几行 124.进入容器内部docker exec -it 容器id bash 123455.停止/删除容器docker stop 容器iddocker stop $(docker ps -qa)docker rm 容器iddocker rm $(docker ps -qa) 126.启动容器docker start 容器id 三. Docker数据卷3.1 数据卷操作数据卷就是将宿主机的一个目录映射到容器的一个目录中 可以在宿主机的目录中直接操作，容器的目录和文件也会跟着改变 创建好的数据卷会存放在一个默认的目录下：/var/lib/docker/volumes/ 121.创建数据卷docker volume create 数据卷名称 12342.查看数据卷详细信息docker volume inspect 数据卷名称查看全部数据卷docker volume ls 123.删除数据卷docker volume rm 数据卷名称 12345678910114.应用数据卷#如果映射数据卷不存在，docker会自动创建，会将容器内部的文件存储在数据卷中docker run -v 数据卷名称:容器内部路径 镜像id#直接指定一个路径作为存放路径，建议使用这种docker run -v 数据卷路径:容器内部路径 镜像id-v:映射数据卷#可以在指定路径后面加上权限，一旦设置权限就不可以改了，举个栗子docker run -v /root/nginx:/etc/nginx:rw nginxro:只读rw:可读可写 3.2 实现容器之间数据同步–volumes-from可以实现容器之间的数据同步，即使有一台容器挂了也不会影响到数据，别的容器仍然会有数据在。 举个栗子 12#启动容器一，并创建测试目录docker run -d -v /root/docker/test1:/test1 -v /root/docker/test2:/test2 --name nginx01 镜像id 123#启动容器二，挂载容器一的目录docker run -d --name nginx02 --volumes-from nginx01 镜像iddocker exec -it 容器二id bash 进入容器二内部后可以看见容器一的test1和test2，这时候无论在容器一还是容器二生成或删除文件都会进行同步 用docker inspect 容器id可以看到两个容器映射的都是同一个目录 四. Dockerfile自定义镜像Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 4.1 创建自定义镜像123456789101112131.创建一个Dockerfile文件，并指定自定义镜像信息#Dockerfile中常用信息FROM: 这个镜像的妈妈是谁?(指定基础镜像)MAINTAINER: 告诉别人，谁负责养它?(指定维护者信息)CMD: 你想让它干啥(指定容器run前要做什么，只有最后一个会生效)ENTRYPOINT: 你想让它干啥(指定容器run前要做什么，可以追加命令)RUN: 你想让它干啥(指定容器run后要做什么)COPY: 给它点创业资金(拷贝文件到镜像内部)ADD: 给它点创业资金(拷贝文件到镜像内部，如果是压缩包会解压了再就行拷贝)WORKDIR: 我是cd(配置工作目录，栗子:如果WORKDIR配置了/home,那么COPY和ADD使用.作为拷贝路径的话都会拷贝到/home下)VOLUME: 给一个存放行李的地方(设置数据卷，挂载到主机目录)EXPOSE: 给一个门(指定对外开放的端口号)ENV: 设置环境变量(environment，也就是参数-e) 122.在linux上通过docker指定镜像docker build -t 镜像名称:[tag] . 12343.举个nginx栗子cat DockerfileFROM daocloud.io/library/nginx:1.18.0COPY test.html /usr/share/nginx/html/test.html 4.2 CMD和ENTRYPOINT的区别用CMD和ENTRYPOINT做同一个测试 CMD 121.创建Dockerfile文件vim /root/docker/Dockerfile 1232.编写FROM daocloud.io/library/centos:latestCMD [&quot;ls&quot;,&quot;-a&quot;] 123.生成镜像文件docker build -f /root/docker/Dockerfile -t cmd_centos:1.0 . 124.启动容器，会发现执行了ls -adocker run cmd_centos镜像id 如果启动镜像的时候再加参数会报错，但完整的命令就可以 ENTRYPOINT 123#除了Dockerfile中的CMD换成了ENTRYPOINT外其它都一样FROM daocloud.io/library/centos:latestENTRYPOINT [&quot;ls&quot;,&quot;-a&quot;] 这时候我们再追加参数发现可以了，这就是ENTRYPOINT比起CMD有可以追加参数的不同 4.3 制作Tomcat镜像 准备tomcat压缩包和jdk压缩包 编写Dockerfile文件 12345678910111213141516171819FROM daocloud.io/library/centos:7MAINTAINER cqm's diy tomcat&lt;chenqiming13@qq.com&gt;ADD jdk-8u271-linux-x64.tar.gz /usr/localADD apache-tomcat-9.0.41.tar.gz /usr/localRUN yum -y install vimENV MYPATH /usr/localWORKDIR $MYPATH #进入容器后就会进入MYPATH目录ENV JAVA_HOME /usr/local/jdk1.8.0_271ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.41ENV CATALINA_BASH /usr/local/apache-tomcat-9.0.41ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/binEXPOSE 8080CMD /usr/local/apache-tomcat-9.0.41/bin/startup.sh &amp;&amp; tailf /usr/local/apache-tomcat-9.0.41/bin/logs/catalina.out 生成tomcat镜像 1docker build -t cqm_tomcat:1.0 . 启动容器 1docker run -d -p 8080:8080 --name cqm_tomcat -v /root/tomcat/test:/usr/local/apache-tomcat-9.0.41/webapps/test -v /root/tomcat/logs:/usr/local/apache-tomcat-9.0.41/logs cqm_tomcat镜像id 访问测试 上传项目 123cd /root/tomcat/testmkdir WEB-INF &amp;&amp; cd WEB-INFvim web.xml 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; id=&quot;WebApp_ID&quot; version=&quot;2.5&quot;&gt;&lt;display-name&gt;db&lt;/display-name&gt;&lt;welcome-file-list&gt;&lt;welcome-file&gt;index.html&lt;/welcome-file&gt;&lt;welcome-file&gt;index.htm&lt;/welcome-file&gt;&lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;&lt;welcome-file&gt;default.html&lt;/welcome-file&gt;&lt;welcome-file&gt;default.htm&lt;/welcome-file&gt;&lt;welcome-file&gt;default.jsp&lt;/welcome-file&gt;&lt;/welcome-file-list&gt;&lt;/web-app&gt; 1vim ../index.hsp 123456789101112131415&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;cqm's tomcat&lt;/title&gt;&lt;/head&gt;&lt;body&gt;this is cqm's tomcat web!!!&lt;br/&gt;&lt;%System.out.println(&quot;-----this is cqm's tomcat web logs-----&quot;);%&gt;&lt;/body&gt;&lt;/html&gt; 查看项目是否部署成功 4.4 上传自定义镜像到Docker Hub12345671.登录docker账号docker login -u docker用户名...Login Succeeded#退出账号docker logout 12345671.登录docker账号docker login -u docker用户名...Login Succeeded#退出账号docker logout 122.给要上传的镜像添加标签docker tag 镜像id docker用户名/镜像名称:版本号 123.上传镜像到Docker Hubdocker push 镜像id docker用户名/镜像名称:版本号 五. Docker网络Docker使用桥接模式在Linux主机上添加一个docker0的网卡，而Docker每启动一个容器就会添加一个新的网卡，使用的是evth-pair技术。evth-pair就是一对虚拟设备接口，添加的网卡都是成对出现的，一段连接镜像，一段连接docker0，所以evth-pair就是一座桥梁，用来连接各种虚拟网络设备。 利用ip addr来查看Linux主机的网卡情况，可以看到docker0网卡 我们拉取一个镜像，并启动 我们在使用ip addr可以看到出现了对新的网卡 进入容器内部查看网卡可以看到是对应的 由此可知docker网络的架构 5.1创建自定义网络 创建自定义网络 1234docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 cqmnet--driver:网络类型，bridge为桥接--subnet:配置子网--gateway:配置网关地址 启动两个tomcat容器并指定自定义网络 12docker run -d -p 8081:8080 --net cqmnet --name tomcat01 镜像iddocker run -d -p 8082:8080 --net cqmnet --name tomcat02 镜像id 测试两个容器之间是否连通 5.2 实现docker0和自定义网络之间的连通 创建两个docker0网段的容器 12docker run -d -p 8083:8080 --name tomcat03 镜像iddocker run -d -p 8084:8080 --name tomcat04 镜像id 利用coonnect进行连通 1docker network connect cqmnet tomcat03 这时候再看自定义网络的信息，可以看到tomcat03被添加进来了 测试 结论，由此可知，connect是将一个网络中的容器添加到另一个网络中，通俗来讲就是该容器同时拥有两个网络的地址，就实现了两个网络之间的互通。 5.3 不同主机之间容器的互联方法一：静态路由 静态路由方法原理就是将容器的请求转发到指定的主机上，再由主机转发给容器。 修改 daemon.json 添加以下内容，使之每个主机的 docker 默认网段不同 1234# 主机一&quot;bip&quot;: &quot;172.17.1.10/24&quot;# 主机二&quot;bip&quot;: &quot;172.17.2.10/24&quot; 添加路由规则 1234# 主机一，网关为主机二的IP地址，即访问172.17.2.0网段的数据包都会被转发到主机二上route add -net 172.17.2.0 netmask 255.255.255.0 gw 192.168.88.130# 主机二route add -net 172.17.1.0 netmask 255.255.255.0 gw 192.168.88.135 方法二：Macvlan Macvlan 原理是将一张物理网卡虚拟成多块虚拟网卡的技术，使得虚拟网卡与物理网卡的参数等都相同。 开启网卡的混杂模式 1ip link set ens33 promisc [on/off] / ifconfig ens33 [-]promisc 创建 Macvlan 网络 12# 子网和网关均和主机一致docker network create --driver macvlan --subnet 192.168.88.0/24 --gateway 192.168.88.1 -o parent=ens33 [network_name] 创建容器需要指定网络 1docker run -d -it --name centos --net [network_name] --ip 192.168.88.10 centos:7 5.4 Overlayoverlay 是一种在网络架构上进行叠加的虚拟化技术，即在原有的网络框架上叠加一层虚拟网络，从而实现应用在虚拟网络上承载，以及与其它网络业务分离。 在这个overlay网络模式里面，有一个类似于服务网关的地址，然后把这个包转发到物理服务器这个地址，最终通过路由和交换，到达另一个服务器的ip地址。 实现 overlay 网络需要有注册发现中心的键值数据库支持，可以用 consul、etcd、zookeeper 等。 区别： consul：服务发现/全局的分布式 key-value 存储。自带 DNS 查询服务，可以跨数据中心。提供节点的健康检查，可以实现动态的 consul 节点增减，docker 官方的用例推荐。 etcd：服务发现/全局的分布式 key-value 存储。静态的服务发现，如果实现动态的新增etcd节点，需要依赖第三方组件。 5.4.1 consulconsul 用于微服务下的服务治理，主要特点有：服务发现、服务配置、健康检查、键值存储、安全服务通信、多数据中心等。 注意：overlay 所需内核版本为 3.18+ 安装 consul 12curl -O https://releases.hashicorp.com/consul/1.10.3/consul_1.10.3_linux_amd64.zipunzip consul_1.10.3_linux_amd64.zip 准备 config.json 文件 123456789101112131415{ &quot;advertise_addr&quot;: &quot;192.168.88.130&quot;, # 更改我们向群集中其他节点通告的地址 &quot;bind_addr&quot;: &quot;192.168.88.130&quot;, # 内部群集通信绑定的地址，这是群集中所有其他节点都应该可以访问的IP地址 &quot;data_dir&quot;: &quot;/opt/consul&quot;, # 数据存放目录 &quot;server&quot;: true, # 是否是server agent节点 &quot;node_name&quot;: &quot;server1&quot;, # 节点名称 &quot;enable_syslog&quot;: true, &quot;enable_debug&quot;: true, &quot;log_level&quot;: &quot;info&quot;, # 日志级别 &quot;bootstrap_expect&quot;: 3, # 提供数据中心中预期服务器的数量，即需要3台consul server agent &quot;start_join&quot;: [&quot;192.168.88.130&quot;, &quot;192.168.88.135&quot;, &quot;192.168.88.100&quot;], # 启动时指定节点地址的字符串数组，指定是其他的consul server agent的地址 &quot;retry_join&quot;: [&quot;192.168.88.130&quot;, &quot;192.168.88.135&quot;, &quot;192.168.88.100&quot;], # 允许start_join时失败时，继续重新连接 &quot;ui&quot;: true, # 启动ui界面 &quot;client_addr&quot;: &quot;0.0.0.0&quot; # Consul将绑定客户端接口的地址，包括HTTP和DNS服务器} 启动 consul 查看状态 12345./consul agent -config-dir ./config.json# 查看集群状态./consul members# 查看leader./consul operator raft list-peers 访问 consul ui 配置 docker/daemon.json 123# 虽然是consul集群，但只能填写一个consul节点的信息&quot;cluster-store&quot;: &quot;consul://192.168.88.130:8500&quot;,&quot;cluster-advertise&quot;: &quot;192.168.88.130:2376&quot; 创建 docker 网络，去到其它主机查看 docker 网络可以看到是已经同步的 1docker network create --driver overlay consulnet 在不同的主机创建容器测试是否能够互联 1docker run -d --name centos1 --network consulnet centos:7 在容器内部可以看到有两张网卡，eth0 是 overlay 创建的，即一个 vxlan；eth1 则是服务网关的网卡 5.4.2 etcdetcd 是一个高可以的键值存储系统，主要用于分享配置和服务发现。 安装 etcd 1234567891011ETCD_VER=v3.5.1GITHUB_URL=https://github.com/etcd-io/etcd/releases/downloadDOWNLOAD_URL=${GOOGLE_URL}curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gztar -zvxf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz/tmp/etcd-download-test/etcd --version/tmp/etcd-download-test/etcdctl version/tmp/etcd-download-test/etcdutl version 启动 etcd 12345678910111213141516HOST=192.168.88.130CLUSTER_IPS=(192.168.88.130 192.168.88.135 192.168.88.136)ETCD_DATADIR=/root/etcd/datanohup ./etcd -name node1 \\ -initial-advertise-peer-urls http://$HOST:2380 \\ -listen-peer-urls http://$HOST:2380 \\ -listen-client-urls http://$HOST:2379,http://127.0.0.1:2379 \\ -advertise-client-urls http://$HOST:2379 \\ -initial-cluster-token etcd-cluster \\ -initial-cluster node1=http://${CLUSTER_IPS[0]}:2380,node2=http://${CLUSTER_IPS[1]}:2380,node3=http://${CLUSTER_IPS[2]}:2380 \\ -initial-cluster-state new \\ -data-dir $ETCD_DATADIR &amp;# listen-peer-urls:节点与节点之间数据交换, 因此需要监听在其他节点可以访问的IP地址上，默认端口2380# listen-client-urls:用户客户机访问etcd数据, 一般监听在本地, 如果需要集中管理, 可以监听在管理服务器可以访问的IP地址上，默认端口2379# initial-advertise-peer-urls:表示节点监听其他节点同步信号的地址，默认端口2380# advertise-client-urls:在加入proxy节点后, 会使用该广播地址, 因此需要监听在一个proxy节点可以访问的IP地址上，默认端口2379 查看 etcd 集群信息和健康状态 12./etcdctl member list./etcdctl cluster-health 修改 docker/daemon.json 12&quot;cluster-store&quot;: &quot;etcd://192.168.88.130:2379&quot;,&quot;cluster-advertise&quot;: &quot;192.168.88.130:2376&quot; 查看是否注册到 etcd 12./etcdctl ls //docker 创建 docker 网络 1docker network create --driver overlay etcdnet 创建容器测试是否互通 1docker run -d -it --name centos1 --network etcdnet centos:7 5.5 Flannel与Calico在 Kubernetes 中，使用的网络组件主要为 Flannel 和 Calico 两种，都可以实现不同主机之间容器的通信，使用前提是要有 etcd。 5.5.1 FlannelFlannel 网络架构如下 添加网络配置到 etcd 123# Network:flannel网段# Type:网络类型./etcdctl set /coreos.com/network/config '{ &quot;Network&quot;: &quot;10.0.0.0/24&quot;, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;} }' 安装 Flannel 1curl -O https://github.com/flannel-io/flannel/releases/download/v0.15.0/flannel-v0.15.0-linux-amd64.tar.gz 启动 Flannel，会生成 /run/flannel/subnet.env，路由表也会加上新规则 12# -etcd-endpoints:etcd地址./flanneld -etcd-endpoints &quot;http://192.168.88.30:2379&quot; 生成 docker 参数信息 1./mk-docker-opts.sh -d /root/etcd/docker -c 修改 /usr/lib/systemd/system/docker.service 12345vim /usr/lib/systemd/system/docker.service# 添加EnvironmentFile=/root/etcd/docker# 在ExecStart后加上ExecStart=/usr/bin/dockerd $DOCKER_OPTS 重启 docker 后测试容器是否能够互联 5.5.2 CalicoCalico 是目前企业在 k8s 集群上使用的最多的容器互通网络方案，比起 Flannel 能够实现更过复杂的需求。 Calico 是一种纯三层的解决方案，因此避免了与二层方案相关的数据包封装操作，中间没有任何 NAT 和 overlay，直接走 TCP/IP 协议栈，通过 iptables 实现复杂的网络规则。 Calico 组件如下： Felix：运行在每一台主机中，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。 etcd：分布式键值存储，保证网络数据的一致性。 BIDR（BGP Client）：每一个主机都会有个 BIDR，用来实现不同的路由协议，Calico 监听主机上 Felix 注入的信息，然后通过 BGP 协议告诉其它节点，从而实现互联。 BGP Route Reflector：用于解决网络规模过大的组件。 Calico 工作模式： IPIP：将 IP 数据包再次封装到一个 IP 包里，相当于一个基于网络层的网桥，普通的网桥是基于 Mac 地址的，而 IPIP 能将两端的路由做成一个 tunnel，将其连接。 BGP：即边界网关协议。 在所有主机上下载 Calico Controller 1curl -o calicoctl -O -L &quot;https://github.com/projectcalico/calicoctl/releases/download/v3.21.0/calicoctl&quot; 配置 calicoctl.cfg 12345apiVersion: projectcalico.org/v3kind: CalicoAPIConfigmetadata:spec: etcdEndpoints: http://192.168.88.30:2379 初始化 Calico 12# ip:宿主机ip./calicoctl node run --ip=192.168.88.30 -c ./calicoctl.cfg 查看 Calico 状态 1./calicoctl node status 由于新版本的 Calico 不再支持 docker 单独使用，但通过插件可以实现 1234git clone https://github.com/projectcalico/libnetwork-plugin.gitcd libnetwork-plugin# 下载插件镜像make image 运行 libnetwork 容器 1docker run -d --privileged --name calico-docker-network-plugin --net host --restart always -v /run/docker/plugins:/run/docker/plugins -e ETCD_ENDPOINTS=http://192.168.88.30:2379 calico/libnetwork-plugin 创建 ippool 123456789101112131415# 查看calico目前地址池./calicoctl get ippools# 创建地址池yaml文件cat ip.yamlapiVersion: projectcalico.org/v3kind: IPPoolmetadata: name: ippool-testspec: cidr: 10.0.0.0/24 ipipMode: Nerver natOutgoing: true disabled: false nodeSelector: all()./calicoctl apply -f ip.yaml 创建 GlobalNetworkPolicy 12345678910apiVersion: projectcalico.org/v3kind: GlobalNetworkPolicymetadata: name: gnp-testspec: selector: all ingress: - action: Allow egress: - action: Allow 创建 docker 网络 1docker create network -d calico --ipam-driver calico-ipam --subnet 10.0.0.0/24 caliconet 关闭 ipv6 内核 1echo 1 &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6 创建容器测试 1docker run -d -it --name centos1 --network caliconet centos:7 六. Docker-ComposeCompose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。 6.1 Docker-Compose使用步骤1231.使用 Dockerfile 定义应用程序的环境2.使用 docker-compose.yml 定义构成应用程序的服务，这样它们可以在隔离环境中一起运行3.执行 docker-compose up 命令来启动并运行整个应用程序 6.2 安装Docker-Compose121.下载curl -L https://get.daocloud.io/docker/compose/releases/download/1.27.4/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose 122.赋权chmod +x /usr/local/bin/docker-compose 6.3 编写Docker-Compose管理MySQL和Nginx容器123456789101112131415161718192021222324252627282930313233341.创建数据卷目录mkdir /root/docker_mysql_nginx/docker_mysql/mysql_datamkdir /root/docker_mysql_nginx/docker_nginx/nginx_confmkdir /root/docker_mysql_nginx/docker_nginx/nginx_html2.编写vim /root/docker_mysql_nginx/docker-compose.ymlversion: '3.8' #指定本 yml 依从的 compose 哪个版本制定的 #可参考https://docs.docker.com/compose/compose-file/services: mysql: #指定服务的名称 restart: always #只要docker启动，这个容器就一起启动 image: daocloud.io/library/mysql:5.7.4 #指定镜像路径 container_name: mysql #指定容器名称 ports: - 3306:3306 #指定端口号的映射 environment: MYSQL_ROOT_PASSWORD: toortoor #指定mysql的root用户登录密码 TZ: Asia/Shanghai #指定时区 volumes: - /root/docker_mysql_nginx/docker_mysql/mysql_data:/var/lib/mysql #映射数据卷 nginx: restart: always image: daocloud.io/library/nginx:1.18.0 container_name: nginx ports: - 80:80 environment: TZ: Asia/Shanghai volumes: - /root/docker_mysql_nginx/docker_nginx/nginx_conf:/etc/nginx - /root/docker_mysql_nginx/docker_nginx/nginx_html:/usr/share/nginx/html 6.4 使用Docker-Compose命令在使用Docker-Compose命令时，系统会在当前目录下寻找yml文件 1231.应用Docker-Compose启动容器docker-compose up -d-d:在后台运行容器 122.关闭并删除容器docker-compose down 123.开启|关闭|重启已由docker-compose管理的容器docker-compose start | stop | restart 124.查看由docker-compose管理的容器docker-compose ps 1235.查看docker-compose日志docker-compose logs -f-f:可以滚动查看 6.5 Docker-Compose配合Dockerfile使用docker-compose.yml 123456789101112version: '3.8'services: nginx: build: context: ./ #指定在当前目录下寻找dockerfile dockerfile: Dockerfile #指定dockerfile文件名 image: nginx:1.18.0 #使用上边制作好的镜像 container_name: nginx ports: - 8080:80 environment: TZ: Asia/Shanghai Dockerfile 12from daocloud.io/library/nginx:1.18.0copy test.html /usr/share/nginx/html/test.html 遇到的错误 12345678910111.ERROR: yaml.scanner.ScannerError: while scanning for the next tokenfound character '\\t' that cannot start any token#是因为yml文件里使用了tab，yml文件格式不允许使用，全部换成空格2.WARNING: Image for service nginx was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.#是因为运行docker-compose时自定义镜像不存在，会帮助生成自定义镜像#提供两种构建方法：#docker-compose build#docker-compose up --build 6.6 Docker-Compose部署wordpress 编写docker-compose.yml 1234567891011121314151617181920212223242526272829version: '3.8'services: db: container_name: mysql image: daocloud.io/library/mysql:latest volumes: - db_data:/var/lib/mysql restart: always ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: toortoor MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: toortoor wordpress: container_name: wordpress image: wordpress:latest ports: - 80:80 restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: toortoor WORDPRESS_DB_NAME: wordpressvolumes: db_data: {} 启动 1docker-compose up -d","link":"/2024/02/18/docker/"},{"title":"CKA","text":"记录刷题- - Job：创建一个固定结束次数的并行 Job，共 2 个 pod，运行 45 completion，Job pod 打印“Beijing”，镜像自选。 12345678910111213141516171819apiVersion: batch/v1kind: Jobmetadata: name: jobspec: parallelism: 2 completions: 45 restartPolicy: Never template: metadata: name: job spec: containers: - name: job image: busybox:latest command: - 'sh' - '-c' - 'echo Beijing' initContainer：pod 的 container 在启动时会检查 volume 中某个目录是否存在某个文件，若不存在则退出，若存在，则运行。要求修改 pod 的 yaml，通过 initContainer 创建该文件，使pod运行。 1 将名为 ek8s-node-1 的 node 设置为不可用，并重新调度该 node 上所有运行的 pods。 1234# cordon:将node设置为不可用，将阻止新pod调度到该节点之上，但不会影响任何已经在其上的 pod，这是重启节点或者执行其他维护操作之前的一个有用的准备步骤，DaemonSet创建的pod能够容忍节点的不可调度属性。kubectl cordon ek8s-node-1# drain:从节点安全的驱逐所有pods。kubectl drain ek8s-node-1 --delete-local-data --ignore-daemonsets --force 在现有的 namespace app-team1 中创建一个名为 cicd-token 的新 ServiceAccount，创建一个 deployment-clusterrole 且只能够创建 Deployment、DaemonSet、StatefulSet 资源的 Cluster Role，并将新的 deployment-clusterrole 绑定到 cicd-token。 12345kubectl create sa cicd-token -n app-team1kubectl create clusterrole deployment-clusterrole --verb=create --resource=Deployment,StatefulSet,DaemonSetkubectl create rolebinding cicd-rolebinding --clusterrole=deployment-clusterrolr --serviceaccount=app-team1:cicd-token k8s master 1.20.0 升级到 1.20.1，并升级 kubelet、kubectl。 12345678910# 将节点设为不可调度性kubectl cordon mk8s-master-0kubectl drain mk8s-master-0 --igonre-daemonsets# 升级apt-get updateapt-get -y install kubeadm=1.20.1-00kubeadm update apply v1.20.1 --etcd-upgrade=falseapt-get -y install kubelet=1.20.1-00 kubectl=1.20.1-00# 将不可调度性去除kubectl uncordon mk8s-master-0 etcd 备份/恢复。 1234# 备份ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; snapshot save &lt;backup-file-location&gt;# 恢复ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=&lt;trusted-ca-file&gt; --cert=&lt;cert-file&gt; --key=&lt;key-file&gt; snapshot restore &lt;backup-file-location&gt; 在 internal 的命名空间下创建一个名为 allow-port-from-namespace 的 NetworkPolicy，此 NetworkPolicy 允许 internal 中的 pod 访问 echo 命名空间中 9000 的端口。 123# 先获取echo租户的labelskubectl get ns echo --show-labelsecho-key: echo-value 1234567891011121314151617apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-port-from-namespace namespace: internalspec: podSelector: {} policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: echo-key: echo-value ports: - protocol: TCP port: 9000 在 ing-internal 下创建一个 ingress 名为 ping，通过 5678 端口暴露 hello svc 下的 /hello。 123456789101112131415apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ping namespace: ing-internalspec: rules: - http: paths: - path: /hello backend: service: name: hello port: number: 5678 将 deployment webserver 扩展至 6 pods。 1kubectl scale deployment webserver --replicas 6 创建一个名为 nginx-kusc00401 的 pod，镜像为 nginx，调度到 disk=ssd 的节点上。 1234567891011121314apiVersion: v1kind: Podmetadata: name: nginx-kusc00401spec: containers: - name: nginx image: nginx:1.20.1 imagePullPolicy: Always ports: - name: http containerPort: 80 NodeSelector: disk=ssd 将一个现有的 pod 集成到 k8s 内置日志记录体系结构中（kubectl logs），使用 busybox 来将名为 sidecar 容器添加到名为 11-factor-app 的 pod 中，sidecar 需运行 /bin/sh -c tail -n+1 -f /var/log/11-factor-app.log ，使用安装在 /var/log 的 volume，使日志文件 11-factor-app.log 可用于 sidecar 容器。 12345678910111213141516171819202122232425262728293031323334apiVersion: v1kind: Podmetadata: name: 11-factor-appspec: volumes: - name: log emptyDir: {} containers: - name: count image: busybox command: - /bin/sh - -c - &gt; i=0; while true; do echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/11-factor-app.log; i=$((i+1)); sleep 1; done volumeMounts: - name: log mountPath: /var/log - name: sidecar image: busybox command: - /bin/sh - -c - 'tail -n+1 -f /var/log/11-factor-app.log' volumeMounts: - name: log mountPath: /var/log 通过 pod label name=cpu-utilizer 寻找 cpu 占用最多的 pod，并写入 /opt/tmp/tmp.txt。 1kubectl top pod -A -l name=cpu-utilizer --sort-by=cpu &gt;&gt; /opt/tmp/tmp.txt","link":"/2024/02/18/cka/"},{"title":"Ansible","text":"一、Ansible简介ansbile是一个IT自动化的配置管理工具，自动化主要体现在Ansible集成了丰富的模块，可以通过一个命令完成一系列的操作，进而减少运维重复性的工作和维护成本，提高工作效率。 1.1 为什么需要ansible思考：假设我们要在10台服务器上安装并运行nginx，要如何操作？ ssh远程登录到服务器 执行yum -y install nginx 执行systemctl start nginx 执行systemctl enable nginx 重复十次。。。 可以看到简单的工作要做十次是很浪费时间的，这时候我们就需要ansible了。 1.2 ansible有哪些功能 批量执行远程命令，可以同时对N台主机执行命令 批量配置软件服务，可以用自动化的方式配置和管理服务 实现软件开发功能，jumpserver底层使用ansible来实现自动化管理 编排高级的IT任务，playbook是ansbile的一门编程语言，可以用来描绘一套IT架构 二、Ansible安装2.1 在控制端上安装ansible直接利用yum源安装即可，ansible配置文件一般不需要做变动 1yum -y install ansible 2.2 ansible配置文件的优先级 如果当前目录不存在ansible.cfg，会采用默认的配置文件 1234ansible --version...config file = /etc/ansible/ansible.cfg... 如果当前目录存在ansible.cfg，则会采用当前目录的配置文件 1234ansible --version...config file = /root/project1/ansible.cfg... 2.3 ansible的Inventory利用密钥来连接被控端 在项目目录下创建hosts文件 12345cd project1vim hosts[cqm]192.168.88.133192.168.88.134 创建密钥 123ssh-keygenls /root/.ssh/authorized_keys id_rsa id_rsa.pub known_hosts 将公钥传送至被控端主机 12ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.88.133ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.88.134 测试是否不需要密码就可以连接 12ssh root@192.168.88.133ssh root@192.168.88.134 三、Ansible的ad-hoc和常用模块3.1 ansible的ad-hocad-hoc简而言之就是“临时命令”，执行完就结束，并不会保存 主要格式为：ansible + -i + 指定主机清单 + 指定主机组名称 + -m + 指定模块 + -a + 具体命令 使用ad-hoc后返回结果的颜色： 绿色：被控端主机没有发生变化 黄色：被控端主机发生了变化 红色：出现故障 3.2 ansible基本命令 直接执行命令 1ansible -i hosts 主机组名 -m 模块 -a 命令 列出某个主机组的主机清单 1ansible -i hosts 主机组名 --list-hosts 查看某个模块使用教程 12ansible-doc 模块名称/EX 3.2 shell和command模块shell和command本质上都是用来执行linux的基础命令，如cat、ls等等，但command不支持用|这种管道符命令 例子，同样的命令在command上会报错 12ansible -i hosts cqm -m shell -a 'ps -ef | grep nginx'ansible -i hosts cqm -m command -a 'ps -ef | grep nginx' 3.3 yum模块1234567ansible -i hosts cqm -m yum -a 'name=httpd state=latest enablerepo=epel'name:指定安装软件名称state: 1.latest为安装最新版本 2.absent为卸载 3.present为安装enablerepo:指定在哪个yum源下安装 3.4 copy模块1234567ansible -i hosts cqm -m copy -a 'src=./httpd.conf dest=/etc/httpd/conf/httpd.conf owner=www group=www mode=0755'src:表示要复制的文件路径dest:要复制到被控端的哪个路径owner:复制文件的属主group:复制文件的属组mode:文件的权限，0755为rwxr-xr-x(r:4,w:2,x:1)，也可以写成(u+rwx,g+x,o-rwx)的形式backup:如果被控端有同名文件，是否保留 3.5 file模块123#更改文件权限ansible -i hosts cqm -m file -a 'path=/etc/nginx/nginx.conf owner=www group=www mode=0644'path:代表要改变文件的路径(被控端) 12345#创建符号连接(软连接)ansible -i hosts cqm -m file -a 'src=/root/test1 dest=/root/test2 mode=0755 state=link'src:源文件路径(被控端)dest:软连接路径(被控端)state:link创建软连接 12345678#创建文件和文件夹ansible -i hosts cqm -m file -a 'path=/root/text state=touch/directory mode=0644 recurse=yes'path:要创建在哪state: 1.touch表创建文件 2.directory表创建文件夹 3.absent表删除recurse: 递归授权 3.6 service模块12345678ansible -i hosts cqm -m service -a 'name=httpd state=started enabled=yes'name:服务名称state: 1.started启动 2.stopped关闭 3.restarted重启 4.reloaded重载enabled:是否开机自启 3.7 group模块1234567ansible -i hosts cqm -m group -a 'name=cqm state=present gid=1000 system=yes/no'name:创建组名称state: 1.present创建组 2.absent删除组gid:组idsystem:是否设置为系统组 3.8 user模块123456789ansible -i hosts cqm -m user -a 'name=cqm uid=1000 group=cqm shell=/bin/bash state=present create_home=no'name:创建用户名称uid:用户idgroup:添加到哪个组shell:使用/bin/bash创建用户，或/sbin/nologinstate: 1.present创建用户 2.absent删除用户create_home:是否创建家目录，默认为yes 1234ansible -i hosts cqm -m user -a 'name=cqm uid=1000 groups=cqm1,cqm2 append=yes state=present system:yes'groups:添加到哪些组append:添加到多个组时添加该项system:是否设置为系统用户 12ansible -i hosts cqm -m user -a 'name=cqm state=absent remove=yes'remove:是否删除家目录 3.9 cron模块1234567891011ansible -i hosts cqm -m cron -a 'name=&quot;check dirs&quot; minute=0 hour=5,2 job=&quot;ls -al &gt; /dev/null&quot;'name:创建定时任务名称minute:分钟hour:小时job:任务state:删除定时任务用absent#在被控端可查看crontab -l#Ansible: check dirs0 5,2 * * * ls -al &gt; /dev/null 3.10 mount模块1234567891011121314151617ansible -i hosts cqm -m mount -a 'path=/root/data src=/dev/sr0 fstype=iso9660 opts=&quot;ro&quot; state=present'path:挂载到哪src:被挂载的目录，可以是以下形式 1.UUID形式:src=&quot;UUID=......&quot; 2.ip形式:src=&quot;192.168.88.128:/data&quot;，一般用于挂载nfs服务器的目录fstype:挂载类型 1.iso9660:文件系统是一个标准CD-ROM文件系统 2.ext4:Linux系统下的日志文件系统 3.xfs:高性能的日志文件系统 4.nonestate:挂载类型 1.present:开机挂载，仅将挂载配置写入/etc/fstab 2.mounted:挂载设备，并将配置写入/etc/fstab 3.unmounted:卸载设备，不会清除/etc/fstab写入的配置 4.absent:卸载设备，会清理/etc/fstab写入的配置opts:权限等，默认填defaults/etc/fstab:磁盘被手动挂载之后都必须把挂载信息写入/etc/fstab这个文件中，否则下次开机启动时仍然需要重新挂载。 3.11 firewalld模块12345678910111213141516ansible -i hosts cqm -m firewalld -a 'service=https permanent=yes enable=yes immediate=yes state=enabled'service:放行服务port:放行端口，如80/tcpsource:放行指定ip地址范围，如192.168.88.0/24state:表防火墙策略状态 1.enabled策略生效 2.disabled禁用策略 3.present添加策略 4.absent删除策略zone:指定防火墙信任级别 1.drop:丢弃所有进入的包，而不给出任何响应 2.block:拒绝所有外部发起的连接，允许内部发起的连接 3.public:允许指定的进入连接 4.internal:范围针对所有互联网用户permanent:yes为永久生效immediate:yes为立即生效 3.12 unarchive模块unarchive模块能够实现解压再拷贝的功能 1234ansible -i hosts cqm -m unarchive -a 'src=./apache.tar.gz dest=/etc/httpd mode=0755 owner=www group=www'src:压缩文件路径dest:解压到哪remote_src:设置为yes时表示压缩包已经在被控端主机上，而不是ansible控制端本地 3.13 get_url模块1234...url:下载链接dest:保存文件地址mode:设置权限 3.14 template模块template与copy用法相同，但template可以实别变量，而copy不行。 3.15 yum_repository模块yum_repository可以用来配置yum源 123456789101112131415- name: Configure Nginx YUM Repo yum_repository: name: nginx description: Nginx YUM Repo file: nginx baseurl: http://nginx.org/packages/centos/7/$basearch/ gpgcheck: no enabled: yes state: presentname:相当于.repo文件中括号的[仓库名]description:相当于.repo文件中的namefile:相当于.repo文件的名称baseurl:相当于.repo文件中baseurlgpgcheck:相当于.repo文件中gpgcheckenabled:相当于.repo文件中enabled 四、Ansible的Playbookplaybook是ansible的另一种使用方式，被称为“剧本”，与ad-hoc不同的是，playbook可以实现持久使用。 playbook是由一个或多个play组成的列表，play的主要功能在于将事先归并为一组的主机装扮成事先通过ansible中的task定义好的角色，从根本上来讲，所谓的task无非就是调用ansible的一个module，将多个play组织在一个playbook中，即可实现同时完成多项任务。 playbook的核心元素 hosts：被控主机清单 tasks：任务集 vars：变量 templates：模板 handlers和notify：触发器 tags：标签 4.1 利用playbook安装apacheplaybook采用的是yml语法，举个栗子 12cd /etc/project1vim httpd.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647- hosts: cqm tasks: #安装apache服务 - name: Install Httpd Service yum: name: httpd state: latest #创建www组 - name: Create www Group group: name: www state: present #创建www用户 - name: Create www User user: name: www group: www shell: /sbin/nologin state: present #修改apache配置文件 - name: Configure Httpd Conf copy: src: ./httpd.conf dest: /etc/httpd/conf/httpd.conf owner: www group: www mode: 0644 #启动服务并设为开机自启 - name: Start Httpd Service service: name: httpd state: started enabled: yes #放行端口 - name: Configure Firewall firewalld: zone: public ports: 8080/tcp permanent: yes immediate: yes state: enabled 1234#检查语法ansible-playbook --syntax -i hosts httpd.yml#执行playbookansible-playbook -i hosts httpd.yml 4.2 利用playbook部署nfs 准备好nfs配置文件 123cd /root/project1cat exports/root/nfs_data 192.168.88.0/24(rw,no_root_squash) 编写nfs.yml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#nfs主机- hosts: 192.168.88.133 tasks: - name: Create Share Directory file: path: /root/nfs_data mode: 0744 owner: root group: root recurse: yes state: directory - name: Install NFS Service yum: name: nfs-utils state: latest - name: Configure NFS copy: src: ./exports dest: /etc/exports backup: yes - name: Start NFS Service service: name: nfs state: started enabled: yes - name: Stop Firewall Service service: name: firewalld state: stopped#挂载nfs目录的主机- hosts: 192.168.88.134 tasks: - name: Create Share Directory file: path: /root/nfs_data state: directory - name: Install NFS Service yum: name: nfs-utils state: latest - name: Start NFS Service service: name: nfs state: started enabled: yes - name: Mount NFS Share Directory mount: path: /root/nfs_data src: 192.168.88.133:/root/nfs_data fstype: nfs opts: defaults state: mounted 执行playbook 1ansible-playbook -i hosts nfs.yml 五、Ansible的变量5.1 在play中设置变量123456789101112- hosts: cqm vars: - web_package: httpd - ftp_package: vsftpd tasks: - name: Install {{ web_package }} and {{ ftp_package }} Service yum: name: - &quot;{{ web_package }}&quot; - &quot;{{ ftp_package }}&quot; state: latest 5.2 在vars_file中设置变量123cat vars.ymlweb_package: httpdftp_package: vsftpd 1234567891011- hosts: cqm vars_files: - ./vars.yml tasks: - name: Install {{ web_package }} and {{ ftp_package }} Service yum: name: - &quot;{{ web_package }}&quot; - &quot;{{ ftp_package }}&quot; state: latest 5.3 通过inventory主机清单设置变量 创建两个变量目录 12mkdir hosts_varsmkdir groups_vars 在groups_vars目录中针对某个组创建变量 12345cat group_vars/cqmweb_package: httpdftp_package: vsftpd#groups_vars目录中也可以新建all来设置变量，这样所有的主机组都可以调用 在hosts_vars目录中针对某个主机创建变量 123cat hosts_vars/192.168.88.133web_package: httpdftp_package: vsftpd 5.4 在执行playbook时通过 -e 参数设置变量1234567- hosts: cqm tasks: - name: Install {{ web_package }} Service yum: name: - &quot;{{ web_package }}&quot; state: latest 1ansible-playbook -i hosts -e 'web_package=httpd' test.yml 5.5 ansible变量的优先级优先级由上至下递减 -e（外置传参） vars_files play中的vars hosts_vars groups_vars中的某个主机组 groups_vars中的all 5.6 register注册变量1234567891011121314151617181920- hosts: cqm tasks: - name: Install Apache Service yum: name: httpd state: latest - name: Start Apache Service service: name: httpd state: started - name: Check Apache Process shell: ps -ef | grep httpd #将结果储存到变量里 register: check_apache #利用debug模块调用 - name: Output check_apache debug: #输出msg中的stdout_lines msg: &quot;{{ check_apache.stdout_lines }}&quot; 5.7 facts变量facts变量是ansible控制端采集被控端的变量，可以直接调用 为了方便可以将facts变量写到文本里 1ansible -i hosts cqm -m setup &gt; fasts.txt 六、Ansible语句6.1 条件判断whencentos安装apache是httpd，而ubuntu安装apache则是httpd2，所以这时候就需要条件判断了 例一：不同系统安装apache 12345678910111213141516- hosts: cqm tasks - name: Centos Install Apache Service yum: name: httpd state: latest when: - ( ansible_distribution == &quot;CentOS&quot; ) - name: Utunbu Install Apache2 Service yum: name: httpd2 state: latest when: - ( ansible_distribution == &quot;Utunbu&quot; ) 例二：给主机名带有web的主机配置yum源 1234567891011- hosts: cqm tasks: - name: Configure Cqm Yum Repo yum_repository: name: cqm description: cqm yum repo baseurl: http://www.cqmmmmm.com gpgcheck: no enabled: yes when: - ( ansible_fqdn is match (&quot;web*&quot;) ) 例三：判断nfs服务是否启动，没有则启动，否则重启 利用echo $？的返回值来查看是否启动 12345678910111213- hosts: cqm tasks: - name: Check NFS Service Status shell: systemctl is-active nfs ignore_errors: yes register: check_nfs - name: Restart NFS Service service: name: nfs state: restarted when: - ( check_nfs.rc == 0 ) 6.2 循环语句with_itemswith_items可以实现循环，可以减少playbook中同样模块的使用次数 例一：利用with_items重启nginx和php服务 123456789- hosts: cqm tasks: - name: Restart Nginx and PHP Service service: name: {{ item }} state: restarted with_items: - nginx - php-fpm 例二：利用变量循环安装多个服务 12345678910- hosts: cqm tasks: - name: Install Nginx and Mysql Service yum: name: {{ pack }} state: latest vars: pack: - nginx - mysql-server 例三：利用循环创建多个用户 12345678910- hosts: cqm tasks: - name: Create Multiple Users user: name: {{ item.name }} group: {{ item.group }} state: present with_items: - { name: 'aaa', group: 'aaa' } - { name: 'bbb', group: 'bbb' } 6.3 触发器handlershandlers是ansible的触发器，配合notify使用。 handlers只有在playbook执行到最后才会执行，简单来说可以将handlers看作一个特殊的tasks，只有在notify指定的某个模块运行了才会触发handlers中的模块。 例一：修改nginx.conf的话就重启nginx服务 1234567891011121314151617- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes notify: Restart Nginx Service handlers: - name: Restart Nginx Service service: name: nginx state: restarted 6.4 tags标签对tasks指定标签，可以在执行playbook的时候指定执行哪个tags的任务。 例一：利用tags来执行开启nginx和php服务 123456789101112- hosts: cqm tasks: - name: Start Nginx and PHP Service service: name: {{ item }} state: started enabled: yes with_items: - nginx - php-fpm tags: start_services...... 1ansible-playbook -i hosts cqm -t 'start_services' test.yml 如果要指定不执行哪个标签的任务，添加参数–skip-tags 1ansible-playbook -i hosts cqm --skip-tags 'start_services' test.yml 6.5 包含include如果每个playbook都会用到重启某个服务的任务，那么每个playbook都要写一次，利用include就只用写一次，让每个playbook调用即可。 例一：准备一个重启nginx的yml文件来给各个playbook调用 1vim nginx_restart.yml 1234- name: Restart Nginx Service service: name: nginx state: restarted 1234567891011- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes include: ./restart_nginx.yml 6.6 错误忽略ignore_errors就是字面意思- -，尽管某个任务出错了，也会继续执行下面的任务。 例一：忽略某个任务 123456- hosts: cqm tasks: - name: ignore errors test command: /bin/false ignore_errors: yes...... 6.7 错误模块failfail模块是一个专门用来“执行失败”的模块，我们都知道在shell中只要添加exit就可以停止执行，而playbook只有在某个任务出错了才会停止执行，这时候就可以利用fail来停止playbook的执行。 例一：利用fail来实现exit的功能 1234567891011121314151617181920- hosts: cqm tasks: - name: debug1 debug: msg: &quot;1&quot; - name: debug2 debug: msg: &quot;2&quot; #执行该模块后后面的debug都会执行 - name: fail fail: msg: &quot; this is fail module test &quot; - name: debug3 debug: msg: &quot;3&quot; - name: debug4 debug: msg: &quot;4&quot; 6.8 错误改变failed_whenfailed_when的作用就是将条件成立的任务状态设置为失败。 例一：判断是否输出了error，是则设置为任务失败 12345678910111213141516- hosts: cqm tasks: - name: debug1 debug: msg: &quot;i am debug1&quot; #因为输出里包含了error，所以条件成立将该任务设置为了fail，playbook中止 - name: Output Error shell: echo 'this is error' register: output_error failed_when: - ( 'error' in output_error.stdout ) - name: debug2 debug: msg: &quot;i am debug2&quot; 6.9 错误处理changed_whenchanged_when的作用就是将条件成立的任务状态设置为changed。 我们在调用handlers的时候，只有任务状态为changed才会调用，这时候就可以用changed_when改变任务状态为changed，就可以实现调用handlers了。 例一：改变任务状态为changed而调用触发器 12345678910111213141516171819- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes notify: Restart Nginx Service #尽管文件没有改变，任务状态为ok也会被改为changed而调用handlers changed_when: yes handlers: - name: Restart Nginx Service service: name: nginx state: restarted changed_when也可以使任务永远不会是changed。 例二：使任务永远为ok 12345678910111213141516171819- hosts: cqm tasks: - name: Configure Nginx File template: src: ./nginx.conf dest: /etc/nginx/nginx.conf mode: 0644 owner: www group: www backup: yes notify: Restart Nginx Service #尽管文件发生了改变，任务状态为changed也会被改为ok,永远不会调用handlers changed_when: false handlers: - name: Restart Nginx Service service: name: nginx state: restarted 七、Ansible的rolesroles就是通过分别将变量、文件、任务、模块及处理器放置于单独的目录中、并可以便捷地include它们的一种机制。 roles的目录结构： files：存放普通文件，比如copy调用的文件 handlers：触发器 meta：依赖关系 tasks：任务 templates：存放含有变量的文件 vars：变量 在每个目录里的yml文件都必须命名为main.yml 一键生成roles目录 1ansible-galaxy role init test 八、利用Ansible搭建Kodcloud项目架构图 8.1 准备项目环境 基本配置 123mkdir -p /root/project/{host_vars,group_vars}cd /root/projectcp /etc/ansible/ansible.cfg . 配置inventory 12345vim hosts[web]192.168.88.133[db]192.168.88.134 配置通用变量 123456vim group_vars/allnfs_server_ip: 192.168.88.134redis_server_ip: 192.168.88.134ip_address_range: 192.168.88.0/24web_user: wwwweb_group: www 创建各个role文件目录 1mkdir -p {db_base,redis,mariadb,nfs_server,web_base,nginx,php,nfs_client,kodcloud}/{files,handlers,tasks,templates,vars} 8.2 配置roles文件1vim kod.yml 1234567891011121314151617181920212223242526272829303132- hosts: db roles: - role: db_base tags: db_base - role: redis tags: redis - role: mariadb tags: mariadb - role: nfs_server tags: nfs_server- hosts: web roles: - role: web_base tags: web_base - role: nginx tags: nginx - role: php tags: php - role: nfs_client tags: nfs_client - role: kodcloud tags: kodcloud 8.3 配置db端基本环境 编写tasks 1vim db_base/tasks/main.yml 1234- name: Stop Firewall Service service: name: firewalld state: stopped 8.4 配置db端redis服务 编写tasks 1vim redis/tasks/main.yml 1234567891011121314151617- name: Install Redis Service yum: name: redis state: latest- name: Configure Redis Service template: src: redis.conf.j2 dest: /etc/redis.conf backup: yes notify: Restart Redis Service- name: Start Redis Service service: name: redis state: started enabled: yes 编写templates 123456vim redis/temlpates/redis.conf.j2egrep -v '^#|^$' redis/templates/redis.conf.j2bind {{ ansible_default_ipv4.address }}protected-mode yesport 6379...... 编写handlers 1cat redis/handlers/main.yml 1234- name: Restart Redis Service service: name: redis state: restarted 8.5 配置db端mariadb服务 编写tasks 1vim mariadb/tasks/main.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243- name: Install Mariadb Service yum: name: '{{ item }}' state: latest with_items: - mariadb - mariadb-server - MySQL-python- name: Configure Mariadb Service copy: src: my.cnf.j2 dest: /etc/my.cnf backup: yes- name: Start Mariadb Service service: name: mariadb state: started- name: Configure Mariadb Root User mysql_user: user: root password: toortoor- name: Create {{ website }} Databases mysql_db: login_user: root login_password: toortoor name: '{{ website }}' state: present collation: utf8_bin encoding: utf8- name: Create {{ website }} DB User mysql_user: login_user: root login_password: toortoor name: '{{ web_db_user }}' password: '{{ web_db_pass }}' host: '192.168.88.%' priv: '*.*:ALL,GRANT' state: present 编写files 1vim mariadb/files/my.cnf.j2 123456789101112131415161718[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# Settings user and group are ignored when systemd is used.# If you need to run mysqld under a different user or group,# customize your systemd unit file for mariadb according to the# instructions in http://fedoraproject.org/wiki/Systemd[mysqld_safe]log-error=/var/log/mariadb/mariadb.logpid-file=/var/run/mariadb/mariadb.pid## include all files from the config directory#!includedir /etc/my.cnf.d 编写vars 1vim mariadb/vars/main.yml 123website: kodcloudweb_db_user: kodcloudweb_db_pass: toortoor 8.6 配置db端nfs服务 编写tasks 1vim nfs_server/tasks/main.yml 123456789101112131415161718192021222324- name: Create NFS Share Directory file: path: '{{ nfs_share_directory }}' mode: 0757 recurse: yes state: directory- name: Install NFS Service yum: name: nfs-utils state: latest- name: Configure NFS template: src: exports.j2 dest: /etc/exports backup: yes notify: Restarted NFS Service- name: Start NFS Service service: name: nfs state: started enabled: yes 编写handlers 1vim nfs_server/handlers/main.yml 1234- name: Restarted NFS Service service: name: nfs state: restarted 编写vars 1vim nfs_server/vars/main.yml 1nfs_share_directory: /root/nfs_share 编写temlpates 12vim nfs_server/templates/exports.j2{{ nfs_share_directory }} {{ ip_address_range }}(rw) 8.7 配置web端基本环境 编写tasks 1vim web_base/tasks/main.yml 123456789101112- name: Create {{ web_group }} Group group: name: '{{ web_group }}' state: present- name: Create {{ web_user }} User user: name: '{{ web_user }}' group: '{{ web_group }}' state: present create_home: no shell: /sbin/nologin 8.8 配置web端nginx服务 编写tasks 1vim nginx/tasks/main.yml 1234567891011121314151617181920212223242526272829303132333435363738- name: Configure Firewall firewalld: zone: public port: 80/tcp permanent: yes immediate: yes state: enabled- name: Configure Nginx YUM Repo yum_repository: name: nginx description: Nginx YUM Repo file: nginx baseurl: http://nginx.org/packages/centos/7/$basearch/ gpgcheck: no enabled: yes state: present- name: Install Nginx Service yum: name: nginx state: latest- name: Configure Nginx template: src: '{{ item.src }}' dest: '{{ item.dest }}' backup: yes notify: Restart Nginx Service with_items: - { src: 'nginx.conf.j2', dest: '/etc/nginx/nginx.conf' } - { src: 'default.conf.j2', dest: '/etc/nginx/conf.d/default.conf' }- name: Started Nginx Service service: name: nginx state: started enabled: yes 编写handlers 1vim nginx/handlers/main.yml 1234- name: Restart Nginx Service service: name: nginx state: restarted 编写templates 1234567891011121314151617181920212223242526272829303132vim nginx/templates/nginx.conf.j2user {{ web_user }};worker_processes {{ ansible_processor_count * 1 }};error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events { worker_connections {{ ansible_processor_count * 1024 }};}http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546vim nginx/templates/default.conf.j2server { listen {{ nginx_port }}; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/host.access.log main; location / { root /usr/share/nginx/html; index index.php index.html index.htm; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { root /usr/share/nginx/html; # fastcgi_pass 127.0.0.1:9000; fastcgi_pass unix:/etc/nginx/php-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #}} 编写vars 1vim nginx/vars/main.yml 1nginx_port: 80 8.9 配置web端php服务 编写tasks 1vim php/tasks/main.yml 123456789101112131415161718192021222324252627282930313233343536373839- name: Configure PHP YUM Repo yum: name: http://rpms.remirepo.net/enterprise/remi-release-7.rpm state: present- name: Install PHP Service yum: name: '{{ item }}' state: present with_items: - '{{ php }}' - &quot;{{ php_version }}-cli&quot; - &quot;{{ php_version }}-common&quot; - &quot;{{ php_version }}-devel&quot; - &quot;{{ php_version }}-embedded&quot; - &quot;{{ php_version }}-gd&quot; - &quot;{{ php_version }}-mcrypt&quot; - &quot;{{ php_version }}-mbstring&quot; - &quot;{{ php_version }}-pdo&quot; - &quot;{{ php_version }}-xml&quot; - &quot;{{ php_version }}-fpm&quot; - &quot;{{ php_version }}-mysqlnd&quot; - &quot;{{ php_version }}-opcache&quot; - &quot;{{ php_version }}-pecl-memcached&quot; - &quot;{{ php_version }}-pecl-redis&quot; - &quot;{{ php_version }}-pecl-mongodb&quot;- name: Configure PHP Service copy: src: www.conf.j2 dest: &quot;{{ php_route }}/php-fpm.d/www.conf&quot; backup: yes notify: Restart Nginx and PHP Service- name: Start PHP Service service: name: &quot;{{ php_version }}-fpm&quot; state: started enabled: yes 编写handlers 1vim php/handlers/main.yml 1234567- name: Restart Nginx and PHP Service service: name: '{{ item }}' state: restarted with_items: - &quot;{{ php_version }}-fpm&quot; - nginx 编写files 123456789vim php/files/www.conf.j2[www]user = wwwgroup = wwwlisten = /etc/nginx/php-fpm.socklisten.owner = wwwlisten.group = wwwlisten.mode = 0660...... 编写vars 1vim php/vars/main.yml 123php: php74php_version: php74-phpphp_route: /etc/opt/remi/php74 8.10 配置web端nfs服务 编写tasks 1vim nfs_client/tasks/main.yml 1234567891011121314151617181920212223- name: Create NFS Share Directory file: path: '{{ nfs_share_directory }}' state: directory- name: Install NFS Service yum: name: nfs-utils state: latest- name: Start NFS Service service: name: nfs state: started enabled: yes- name: Mount NFS Share Directory mount: path: '{{ nfs_share_directory }}' src: &quot;{{ nfs_server_ip }}:{{ nfs_share_directory }}&quot; fstype: nfs opts: defaults state: mounted 编写vars 1vim nfs_client/tasks/main.yml 1nfs_share_directory: /root/nfs_share 8.11 配置web端kodcloud 编写tasks 1vim kodcloud/tasks/main.yml 123456789101112131415161718192021- name: Create Kodcloud Directory file: path: /usr/share/nginx/html/kodcloud owner: www group: www mode: 0777 state: directory- name: Input Kodcloud File unarchive: src: '{{ kod_version }}' dest: /usr/share/nginx/html/kodcloud owner: www group: www mode: 0777- name: Configure Nginx Virtual Host copy: src: kodcloud.conf.j2 dest: /etc/nginx/conf.d/kodcloud.conf notify: Restart Nginx and PHP Service 编写handlers 1vim kodcloud/handlers/main.yml 1234567- name: Restart Nginx and PHP Service service: name: '{{ item }}' state: restarted with_items: - nginx - &quot;{{ php_version }}-fpm&quot; 编写vars 1vim kodcloud/vars/main.yml 12kod_version: kodbox.1.15.zipphp_version: php74-php 编写files 1wget http://static.kodcloud.com/update/download/kodbox.1.15.zip kodcloud/files/kodbox.1.15.zip 12345678910vim kodcloud/files/kodcloud.conf.j2server { listen 80; server_name localhost; location / { root /usr/share/nginx/html/kodcloud; index index.php index.html index.htm; }} 8.12 执行playbook1ansible-playbook -i hosts kod.yml 8.13 测试 登录到web安装kodcloud 登录kodcloud","link":"/2024/02/18/ansible/"},{"title":"Kafka","text":"一、kafka架构kafka 是一个分布式的基于发布和订阅模式的消息队列（message queue），主要用于大数据实时处理领域。 1.1 消息队列同步通信 异步通信 使用消息队列的好处如下： 解耦：允许独立的扩展或修改两边的处理过程 可恢复性：系统的一部分组件失效时，不会影响到整个系统 缓冲：控制和优化数据经过系统的速度 灵活性和峰值处理能力：在特殊情况下，通信中的请求量会急剧增大（如双十一等活动），但这种情况并不常见，如果投入资源来待命是很浪费的，使用消息队列就可以顶住突发的访问压力，而保护集群不会因为请求量过多而崩溃 异步通信：有时候用户不想也不需要立即处理消息，这种时候就允许用户把消息放在队列中，等待处理 消息队列分为点对点模式和发布/订阅模式 点对点模式 消息生产者生产消息发送到 queue 中，消费者再从 queue 拉取消息并消费，消息被消费之后就不再存在于 queue 中，queue 可以有多个消费者，但对于消息而言只能有一个消费者。 发布/订阅模式 消息生产者将消息发送到 topic 中，可以同时有多个消费者订阅该消息，发布到 topic 的消息可以被所有消费者订阅。 1.2 kafka基础架构kafka 有四个核心的 API： The Producer API：允许一个应用程序发布一串流式的数据到一个或者多个 topic The Consumer API：允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理 The Streams API：允许一个应用程序作为一个流处理器，消费一个或者多个 topic 产生的输入流，然后生产一个输出流到一个或多个 topic 中去，在输入输出流中进行有效的转换 The Connector API：允许构建并运行可重用的生产者或者消费者，将 topics 连接到已存在的应用程序或者数据系统 producer：消息生产者 consumer：消息消费者 topic：数据主题，是数据订阅/发布的地方，可以被多个消费者订阅 partition：分区，每个 topic 包含一个或多个分区，分区是一个有序的队列，分区中的消息都会被分配一个 offset，kafka 保证按每一个分区中的顺序将消息发送给消费者，但不保证按每一个 topic 中的顺序将消息发送给消费者；且 partition 是消费者和生产者操作的最小单元 offset（偏移量）：kafka 的存储文件都是按照 offset.kafka 来命名，方便查找数据 leader 副本：消息的订阅/发布都由 leader 来完成 follower 副本：进行消息数据的备份，当 leader 挂了之后，就成为新的 leader 来代替旧的 leader broker：一个 kafka 就是一个 broker，一个集群由多个 broker 组成 consumer group：消费者组，组中有多个消费者，组中的消费者都只能够接收一个分区的消息 zookeeper：保存 kafka 集群状态的信息，2.8.0版本开始支持 kraft 模式，可不依赖 zk 运行 kafka 集群 replicas：副本数 1.3 kafka 存储机制kafka 内部会自己创建一个 _consumer_offsets 并包含 50 个分区，这些主题用来保存消费者消费某个 topic 的偏移量。 每个 partitions 都被切割成相同大小的 segment，segment 由四个部分构成，具体如下： log：数据文件 index：索引文件，用来保存消息的索引，能够使查找数据更加高效 timeindex：具体时间日志 leader-epoch-checkpoint：保存了每一任 leader 开始写入消息时的 offset，会定时更新 1.4 生产者分区策略kafka 允许为每条消息定义消息 key，可以根据 key 来为消息选择分区。 分区策略即决定生产者将消息发送到哪个分区的算法，主要有以下几种： 轮询策略（没给定分区号和 key 值）：可以提供优秀的负载均衡能力，保证消息被平均的分配到各个分区上，但不能保证消息的有序性。 随即策略：瞎分，基本不用。 hash 策略（没给定分区号，但给了 key 值）：将 key 的 hash 值与 topic 的 partition 数进行取余得到partition值 自定义分区 1.5 生产者ISRISR 即同步副本，leader 维护了一个动态的 ISR，为 leader 保持同步的 follower 集合，当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 acks，如果 follower 长时间没有向 leader 同步数据，则该 follower 将被踢出 ISR。leader 发生故障之后就会在 ISR 中选取出新的 leader。 1.6 生产者ACKSkafka 发送确认参数 acks 的几种模式： acks = 0：意味着生产者能够通过网络把消息发送出去，broker 接收到数据还没写入就发送，容易丢数据 acks = 1：等待 leader 写入数据（不等待 follower 同步），就可以发送数据，可能会丢数据 acks = all/-1：等待 leader 和 follower（ISR 中的）写入数据，就可以发送数据，数据不容易丢，但效率底 1.7 kafka数据一致性原理 HW：所有副本中的最小 LEO LEO：每个副本的最后一个（最大的） offset 假设分区的副本为3，副本0为 leader，副本1、2为 follower，并且都在 ISR 列表中。虽然 leader 已经写入了 message3，但消费者只能读到 message1，因为所有的 ISR 都同步了 message1，只有在 HW 之上的消息才支持被消费，而 HW 取决于 LEO，原理类似于木桶原理。 这么做的原因是如果 leader 崩溃了，一个 follower 成为新的 leader 后，由于该新的 leader 没有做好数据的同步，如果允许消费者读取 HW 之下的数据的话，那么就会出现数据不一致的问题。 1.7 Exactly-once当生产者发送消息到 topic 时，很可能会出现宕机的情况，或者出现了网络故障，导致生产者消息发送失败，而生产者要如何处理这样的错误，就产生了如下几种不同的语义： At least once：至少一次，如果生产者收到了 broker 发送过来的 acks，且 acks 设置为了 all/-1，这就代表消息已经被写入 topic 且同步好了。如果生产者在时间内没受到 acks 或收到的 acks 有误，那么就会重新发送消息。如果 broker 恰好在消息已经写入 topic 时，发送 acks 前出了故障，那么就会导致生产者发送同样的消息两次，消费者消费两次。 At more once：最多一次，如果生产者在接收 acks 超时或返回有问题的时候不重新发送消息，那么消息很可能没写入 topic 中，因此消费者也不会消费该条消息，不会出现数据重复的现象，但很容易缺数据。 Exactly once：精确一次，即使生产者重新发送消息，也只会被消费者消费一次，实际上就是在 kafka 集群中做一次去重的操作。kafka 集群会给生产者分配一个 PID，生产者每次发送消息到 broker 时都会附带 PID、分区以及序列号，broker 就会对数据做一个保存，如果生产者再发送同样消息，那么 broker 就会对数据进行去重，但如果生产者宕机重启了，就会被分配一个新的 PID，所以去重无法做到精准的数据写入。要开启 exactly-once，需要在 broker 中配置 enable.idempotence = true ，这时候 acks 默认被设置为 -1。 1.8 消费者分区分配策略消费者是采用 pull 的方式在 kafka 集群中获取消息的。 push 模式很难适应消费速率不同的消费者，因为消息的发送速率是由 broker 决定的；而 pull 方式当 kafka 集群没有数据的时候，消费者可能会陷入循环之中，一直取到空数据。 一个消费者组里由多个消费者，一个 topic 里由多个分区，那么数据在消费的时候就会涉及到分配的问题，即确定那些分区由消费者组里的哪些消费者来消费。 kakfa 有三种分配策略，如下： RangeAssignor（默认）：RangeAssignor 是针对 topic 而言的，首先会对 topic 中的分区按照序号进行排列，并对消费者根据字典序排列，然后用分区个数除以消费者线程的总数来决定分区的分配，但如果除不尽，那么前面的消费者就会多分配一些分区。 RoundRobin：将消费组内所有消费者以及消费者所订阅的所有 topic 的 partition 按照字典序排序，然后通过轮询的方式逐个分配给组内的消费者。如果同时消费多个 topic，那么消费者会将这些 topic 视为一个。但如果同一个消费组内的消费者所订阅的信息是不相同的，那么在执行分区分配的时候就不是完全的轮询分配，有可能会导致分区分配的不均匀。 StickyAssignor：StickyAssignor 要实现的是分区的分配要尽可能的均匀，分配给消费者者的主题分区数最多相差一个，假设一个消费者组有三个消费者，都订阅了四个 topic，且每个 topic 中有二个分区，那么这时候分配的结果与 RoundRobin 会很相似，即消费者A三个分区、消费者B三个分区、消费者C两个分区，假设消费者C退出了消费者组，这时候 StickyAssignor 会保留之前消费者A和B分配到的分区，然后再将消费者C之前分配到的分区再分配给消费者A和B，即体现了粘性，不需要消费者将之前处理过的数据送到新的消费者再处理一次。 1.9 offset存储kafka 在0.9版本之前，offset 是存储在 zk 中，在之后的版本则是存储在 kafka 内置的一个 topic 中，即 _consumer_offsets，一个 offset 提交的信息包括以下： Fields Content Key Consumer Group、Topic、Partition Payload Offset、Metadata、Timestamp offset 的提交会根据消费者组的 key（GTP） 进行分区，对于一个给定的消费者，它所有的消息都会发送到唯一的 broker，这样消费者就不需要对多个 broker 拉取数据，但如果消费者组消费很多数量的分区，会对 broker 造成性能瓶颈。 1.10 kafka写机制 顺序写入磁盘 零拷贝 1.11 kafka-controllerkafka 集群中会有一个 broker 被选举为 controller，用来负责管理 broker 的上下线，所有 topic 的分区、副本分配和 leader 选举等。 1.12 kafka事务事务可以保证在 Exactly-Once 的基础上，生产和消费可以跨分区跨会话，即生产者在同一个事务内提交到多个分区的消息，要么同时提交成功，要么同时失败，这样就保证了生产者在运行时出现异常或宕机之后仍然成立。 生产者事务 为了实现跨分区跨会话的事务，需要引入全局唯一的 Transaction ID（由客户端给定），并将生产者的 PID 与之绑定，这样以来生产者即使宕机重启也可以与原来的 PID 进行绑定。 为了管理 Transaction ID，kafka 中引入了一个新的组件 Transaction Coordinator，生产者就是与 Transaction Coordinator 交互来获得 Transaction ID 对应的任务状态。Transaction Coordinator 还负责将事务写入 topic，这样即使服务重启，也可以得到恢复。 消费者事务 事务主要是为生产者作保证，无法保证消费者的精准消费，是因为消费者可以根据 offset 来访问消息。 二、kafka基础2.1 kafka安装通过二进制包部署 下载 kafka 12curl -O https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.8.0/kafka_2.13-2.8.0.tgztar -xf kafka_2.13-2.8.0.tgz 修改 server.properties 1234# broker的id必须为唯一的整数，设置为-1时随机生成broker.id=-1# 修改为实际zookeeper节点地址+2181端口zookeeper.connect=... 开启 zookeeper 和 kafka 123# daemon参数:不输出启动日志信息./bin/zookeeper-server-start.sh -daemon ../config/zookeeper.properties./bin/kafka-server-start.sh -daemon ./config/server.properties 通过 docker-compose 部署 docker-compose.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647version: &quot;3.8&quot;services: zookeeper: container_name: zookeeper image: docker.io/bitnami/zookeeper:3.7 ports: - &quot;12181:2181&quot; volumes: - &quot;./data/zookeeper_data:/zookeeper_data&quot; environment: - ALLOW_ANONYMOUS_LOGIN=yes kafka1: container_name: kafka1 image: docker.io/bitnami/kafka:2.8.0 ports: - &quot;19092:9092&quot; volumes: - &quot;./data/kafka1_data:/data&quot; environment: - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper kafka2: container_name: kafka2 image: docker.io/bitnami/kafka:2.8.0 ports: - &quot;19093:9092&quot; volumes: - &quot;./data/kafka2_data:/data&quot; environment: - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper kafka3: container_name: kafka3 image: docker.io/bitnami/kafka:2.8.0 ports: - &quot;19094:9092&quot; volumes: - &quot;./data/kafka3_data:/data&quot; environment: - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181 - ALLOW_PLAINTEXT_LISTENER=yes depends_on: - zookeeper 进入 zookeeper 内部查看 brokers 是否存在 123zkCli.shls /brokers/ids... kraft 模式部署 kraft/server.properties 1234# 根据节点数改node.id=1# 控制节点controller.quorum.voters=1@master:9093,2@slave1:9093,3@slave2:9093 生成集群ID 1./bin/kafka-storage.sh random-uuid 生成 /tmp/kraft-combined-logs 目录 1./bin/kafka-storage.sh format -t &lt;uuid&gt; -c ./config/kraft/server.properties 各节点启动 kafka 1./bin/kafka-server-start.sh -daemon ./config/kraft/server.properties kraft 模式 docker-compose 部署 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647version: &quot;3.8&quot;services: kafka1: container_name: kafka1 image: docker.io/bitnami/kafka:2.8.0 command: - /bin/bash - -c - | kafka-storage.sh format -t &lt;uuid&gt; -c /opt/bitnami/kafka/config/kraft/server.properties kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties ports: - &quot;19092:9092&quot; volumes: - &quot;./conf/kafka1:/opt/bitnami/kafka/config/kraft&quot; environment: - ALLOW_PLAINTEXT_LISTENER=yes kafka2: container_name: kafka2 image: docker.io/bitnami/kafka:2.8.0 command: - /bin/bash - -c - | kafka-storage.sh format -t &lt;uuid&gt; -c /opt/bitnami/kafka/config/kraft/server.properties kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties ports: - &quot;19093:9092&quot; volumes: - &quot;./conf/kafka2:/opt/bitnami/kafka/config/kraft&quot; environment: - ALLOW_PLAINTEXT_LISTENER=yes kafka3: container_name: kafka3 image: docker.io/bitnami/kafka:2.8.0 command: - /bin/bash - -c - | kafka-storage.sh format -t &lt;uuid&gt; -c /opt/bitnami/kafka/config/kraft/server.properties kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties ports: - &quot;19094:9092&quot; volumes: - &quot;./conf/kafka3:/opt/bitnami/kafka/config/kraft&quot; environment: - ALLOW_PLAINTEXT_LISTENER=yes 2.2 kafka基本使用 列出某个 zookeeper 中的 topic 1./kafka-topics.sh --list --zookeeper zookeeper_ip:2181 创建 topic 123# --replication-factor:副本数，总副本数为(分区数 * 副本数参数)，下面的例子总副本数为4# --partitions:分区数./kafka-topics.sh --create --zookeeper zookeeper_ip:2181 --replication-factor 2 --partitions 2 --topic test 删除 topic 12# delete.topic.enable为true时才会真正删除./kafka-topics.sh --delete --zookeeper zookeeper_ip:2181 --topic test 查看 topic 1./kafka-topics.sh --describe --zookeeper zookeeper_ip:2181 --topic test 启动 producer 12# --broker-list:针对生产者使用，指定集群中的一个或者多个kafka服务器./kafka-console-producer.sh --broker-list kafka_id:9092 --topic test 启动 consumer 123# --bootstrap-server:针对消费者使用，指定集群中的一个或者多个kafka服务器# --from-beginning:查看所有消息数据./kafka-console-consumer.sh --bootstrap-server kafka_id:9092 --topic test --from-beginning 2.3 单播消息如果多个消费者在同一个消费者组，只有一个消费者可以收到同一个订阅的 topic 的消息。 如果 topic 进行了分区，那么一个 partition 只能被消费者组内的一个消费者消费，但消费者可以消费多个不同的 partition，而这些 partition 也可以被不同消费者组的消费者消费。 12# --consumer-property group.id:指定该消费者从属于哪个消费者组./kafka-console-consumer.sh --bootstrap-server kafka_id:9092 --consumer-property group.id=testgroup --topic test 2.4 多播消息如果多个不同消费者组中的消费者消费同一个订阅的 topic 的消息，那么这些消费者都可以消费同样的消息。 2.5 查看消费者组信息 查看指定节点有哪些消费者组 1./kafka-consumer-groups.sh --bootstrap-server kafka_id:9092 --list 查看消费者组详细信息 1234./kafka-consumer-groups.sh --bootstrap-server kafka_id:9092 --describe --group testgroup# CURRENT-OFFSET:上次消费消息偏移量# LOG-END-OFFSET:当前topic最后消息偏移量# LAG:未消费信息的数量 2.6 kafka集群操作 消息发送 1./kafka-console-producer.sh --broker-list kafka_01:9092 kafka_02:9092 kafka_03:9092 --topic test 消息消费 1./kafka-console-consumer.sh --bootstrap-server kafka_01:9092 kafka_02:9092 kafka_03:9092 --from-beginning --topic test 消费者组消费信息 1./kafka-consumer-groups.sh --bootstrap-server kafka_01:9092 kafka_02:9092 kafka_03:9092 --from-beginning --consumer-property group.id=testgroup --topic test 三、kafka优化3.1 如何防止消息丢失 发送方：设置 ack 的值为 1 或 -1/all 消费方：设置 offset 为手动提交 3.2 如何防止消息的重复消费如果 broker 收到了消息并发送了 ack 给生产者，因为网络原因生产者在一定时间内没有收到 ack 就会进行消息的重新发送，这样 broker 就会有两条一样的消息，就可能会造成消费者重复消费。 在消费者端进行非幂等性（多次访问的结果是一样的）消费问题，就可以解决。 方法一：在数据库中创建一个联合主键（id，uuid），这样只有联合主键匹配成功才能写入数据 方法二：使用分布式锁，例如 Redission.lock（uuid） 3.3 如何做到顺序消费顺序消费的使用场景并不多，因为会牺牲较多的性能。 发送方：ack 不能设置为 0（否则可能会丢失消息），关闭重试并使用同步发送（重试可能会导致消息重复，异步发送会导致消息发送顺序不一致），消息发送成功后才会发送下一条，以保证消息的顺序发送 消费方：消息都是发送到一个 partition 中，只能有一个消费者来消费该 partition 的消息 3.4 消息积压当消费者的消费速度远远跟不上生产者的消息生产速度，那么 kafka 中就会有大量的消息没有被消费，消费者寻址的性能也会越来越差，最后导致服务雪崩。 方法一：使用多线程 方法二：创建多个消费者组和多个消费者，一起消费 方法三：创建一个消费者，该消费者新建一个 topic，并划分多个分区，多个分区再划分给多个消费者，这时候该消费者将消息拉取下来但不进行消费，而是放在新的 topic 上让多个消费者进行消费。 3.5 延时队列如果在订单创建成功后 30 分钟内没有付款，则自动取消订单，在这就可以通过延时队列来实现。 方法： kafka 创建相应 topic 消费者轮询消费该 topic 的消息 消费者判断该 topic 的创建时间与当前时间是否超过 30 分钟（前提是订单未支付） 如果是：在数据库中修改订单状态为取消 如果不是：记录当前消息的 offset，并不再继续消费 四、kafka-eagle监控平台 下载 kafka-eagle 1curl -O https://github.com/smartloli/kafka-eagle-bin/archive/v2.0.8.tar.gz 修改 system-config.properties 12345678# zk地址efak.zk.cluster.alias=cluster1cluster1.zk.list=localhost:12181# mysql地址efak.driver=com.mysql.cj.jdbc.Driverefak.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNullefak.username=rootefak.password=toortoor 修改环境变量 1234567vim /etc/profileexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar.:$JAVA_HOME/lib/dt.jar.:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/binexport KE_HOME=/root/kafka/kafka-eagle/efak-web-2.0.8export PATH=$PATH:$KE_HOME/binsource /etc/profile 访问","link":"/2024/02/18/kafka/"},{"title":"Jenkins","text":"Jenkins是一个开源的持续集成的服务器，Jenkins开源帮助我们自动构建各类项目。Jenkins强大的插件式，使得Jenkins可以集成很多软件，可能帮助我们持续集成我们的工程项目。 一、什么是CI、CD Devops也就是开发运维一体化，而随着Devops的兴起，出现了持续集成、持续交付以及持续部署的新方法，传统的软件开发和交付方法正在迅速变得过时。从历史上看，在敏捷时代，大多数公司会每月、每季度、每两年甚至每年发布部署或发布软件。然而，在DevOps时代，每周、每天、甚至每天多次。当SaaS正在占领世界时，我们可以轻松地动态更新应用程序，而无需强迫客户下载新组件。很多时候，他们甚至都不会意识到正在发生变化。开发团队通过软件交付流水线（Pipeline）实现自动化，以缩短交付周期，大多数团队都有自动化流程来检查代码并部署到新环境。 持续集成的重点是将各个开发人员的工作集合到一个代码仓库中。通常，每天都要进行几次，主要目的是尽早发现集成错误，使团队更加紧密结合，更好地协作。 持续交付的目的是最小化部署或释放过程中固有的摩擦。它的实现通常能够将构建部署的每个步骤自动化，以便任何时刻能够安全地完成代码发布（理想情况下）。 持续部署是一种更高程度的自动化，无论何时对代码进行重大更改，都会自动进行构建/部署。 而Jenkins就是来实现以上持续集成工作的服务器。 二、持续集成环境搭建Jenkins流程图： 2.1 Gitlab代码托管服务器安装Gitlab 是一个用于仓库管理系统的开源项目，使用Git作为代码管理工具，并在此基础上搭建起来的Web服务。 开启postfix服务（支持gitlab发信功能） 12systemctl start postfixsystemctl enable postfix 配置gitlab的yum源 123456789vim /etc/yum.repos.d/gitlab-cd.repo[gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/gpgcheck=0enabled=1yum clean allyum makecache 安装gitlab 1yum -y install gitlab-ce 修改gitlab配置文件 123456789vim /etc/gitlab/gitlab.rb#用户访问所使用的URL，域名或者IP地址external_url 'http://192.168.88.133'#时区gitlab_rails['time_zone'] = 'Asia/Shanghai'#启用SMTP邮箱功能gitlab_rails['smtp_enable'] = 'ture'#使用SSH协议拉取代码所使用的连接端口gitlab_rails['gitlab_shell_ssh_port'] = '22' 刷新配置 1gitlab-ctl reconfigure 启动服务 123gitlab-ctl restartgitlab-ctl status...... docker-compose.yaml 123456789101112131415161718version: '3.8'services: gitlab: container_name: gitlab image: gitlab/gitlab-ce:latest restart: always environment: GITLAB_OMNIBUS_CONFIG: | external_url 'http://192.168.88.30' gitlab_rails['time_zone'] = 'Asia/Shanghai' gitlab_rails['gitlab_shell_ssh_port'] = '22' ports: - '80:80' - '23:22' volumes: - '/root/cicd/gitlab/config:/etc/gitlab' - '/root/cicd/gitlab/logs:/var/log/gitlab' - '/root/cicd/gitlab/data:/var/opt/gitlab' 进入容器查看默认用户名和修改密码 123456789gitlab-rails console# 查找root用户u=User.where(id:1).first# 修改密码u.password='toortoor'# 确认修改密码u.password_confirmation='toortoor'# 保存修改u.save 2.2 Gitlab创建组、用户、项目Gitlab创建组： 添加组 设置组名和权限 Gitlab创建用户： 添加用户 配置选项 将用户添加到项目组 Gitlab创建项目： 在指定组中添加项目 设置项目名称 2.5 项目上传到Gitlab 2.4 Jenkins安装 安装JDK 1yum -y install java-1.8.0-openjdk* 放行端口 12firewall-cmd --zone=public --add-port=8888/tcp --permanentfirewall-cmd --reload 添加Jenkins官方源并安装 123wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.reporpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.keyyum -y install jenkins 修改配置文件 123vim /etc/sysconfig/jenkinsJENKINS_USER=&quot;root&quot;JENKINS_PORT=&quot;8888&quot; 查看初始密码 12cat /var/lib/jenkins/secrets/initialAdminPassword...... docker-compose.yaml 1234567891011version: '3.8'services: jenkins: container_name: jenkins image: jenkins:2.60.3 restart: always ports: - '8080:8080' - '50000:50000' volumes: - '/root/cicd/jenkins/home:/var/jenkins_home' 2.5 Jenkins中文插件安装 由于Jenkins官方下载插件很慢，我们修改为国内Jenkins插件地址，jenkins -&gt; Manage jenkins -&gt; Manage Plugins 更换配置文件中的地址 12345cp /var/lib/jenkins/updates/default.json /var/lib/jenkins/updates/default.json.backupsed -i 's/https:\\/\\/updates.jenkins.io\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g' /var/lib/jenkins/updates/default.json &amp;&amp; sed -i 's/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g' /var/lib/jenkins/updates/default.jsonsystemctl restart jenkins 安装中文插件 2.6 Jenkins用户权限管理 由于Jenkins功能较为简介，都要靠安装插件来丰富体验，所以用户管理需要安装role-based插件 在Configure Global Security开启刚刚安装的插件 在Manage and Assign Roles中创建角色baserole并分配具体权限 用户没加入角色前是没有访问权限的，将用户加入到角色baserole即可，但只有部分权限 2.7 Jenkins安装凭证管理插件 安装Credential Binding插件，安装完就可以看到多了凭据的功能 2.8 Jenkins普通用户密码认证 为了Jenkins能够拉取gitlab的代码，需要安装git插件 1yum -y install git 创建普通用户凭证，注意这里的用户是gitlab创建好的用户 创建项目添加普通用户凭证 build now构建 Jenkins主机上查看是否构建成功 2.9 Jenkins使用ssh免密认证 在Jenkins主机上生产公钥私钥 1ssh-keygen -t rsa 在gitlab上添加公钥 在Jenkins上添加ssh凭证 创建项目添加ssh凭证 build now构建项目后 2.10 Jenkins安装Maven 下载maven 1234wget https://mirrors.bfsu.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gzmkdir maventar -xf apache-maven-3.6.3-bin.tar.gzcp apache-maven-3.6.3-bin.tar.gz/* maven/ 配置环境变量 123456vim /etc/profileexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdkexport MAVEN_HOME=/root/mavenexport PATH=$PATH:$JAVA_HOME/bin:/$MAVEN_HOME/binsource /etc/profilemvn -v 全局工具配置里新增jdk和maven 在系统配置里添加三个变量 修改maven配置文件 123456789mkdir /root/repovim /root/maven/conf/settings.xml&lt;localRepository&gt;/root/repo&lt;/localRepository&gt;&lt;mirror&gt; &lt;id&gt;aliyunmaven&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;阿里云公共仓库&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt;&lt;/mirror&gt; 测试maven构建项目 2.11 Tomcat安装和配置 安装jdk和tomcat 123456yum -y install java-1.8.0-openjdk*wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gztar -xf apache-tomcat-9.0.43.tar.gzmkdir /root/tomcatmv apache-tomcat-9.0.43.tar.gz/* /root/tomcat./root/tomcat/bin/startup.sh 添加tomcat管理用户 123456789vim /root/tomcat/conf/tomcat-users.xml &lt;role rolename=&quot;tomcat&quot;/&gt; &lt;role rolename=&quot;role1&quot;/&gt; &lt;role rolename=&quot;manager-gui&quot;/&gt; &lt;role rolename=&quot;manager-script&quot;/&gt; &lt;role rolename=&quot;admin-gui&quot;/&gt; &lt;role rolename=&quot;admin-script&quot;/&gt; &lt;role rolename=&quot;manager-status&quot;/&gt; &lt;user username=&quot;tomcat&quot; password=&quot;toortoor&quot; roles=&quot;manager-gui,manager-script,tomcat,admin-gui,admin-script&quot;/&gt; 12345vim /root/tomcat/webapps/manager/META-INF/context.xml&lt;!-- &lt;Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot; allow=&quot;127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1&quot; /&gt;--&gt; 测试 三、Jenkins构建项目Jenkins构建的项目类型主要为以下三种： 自由风格软件项目（freestyle project） Maven项目（maven project） 流水线项目（pipeline project） 3.1 自由风格软件项目构建 要将项目部署到tomcat服务器上，需要安装deploy to container插件 Jenkins -&gt; 新建items 使用ssh免密登录来拉取代码 使用maven编译打包 部署 再次构建后可以回到tomcat查看是否部署成功 3.2 Maven项目构建 安装Maven Integration插件 创建maven项目 构建设置，其余设置都相同 构建后查看是否部署成功 3.3 Pipeline project简介pipeline就是一套运行在Jenkins上的工作流框架，将独立运行的单个或多个节点的任务连接起来，实现复杂流程的编排和可视化的工作。 优点有： 自动地为所有分支创建流水线构建过程并拉取请求。 在流水线上代码复查/迭代 (以及剩余的源代码)。 对流水线进行审计跟踪。 该流水线的真正的源代码，可以被项目的多个成员查看和编辑。 3.4 Pipeline流水线项目构建 安装pipeline插件 新建流水线项目 声明式流水线 3.5 Jenkinsfile脚本文件pipeline脚本内容放在Jenkins服务器不好管理，所以就在项目添加一个Jenkinsfile文件并推送到gitlab上，实现直接拉取执行。 在项目下添加一个Jenkinsfile，将pipeline内容复制进去并推送到gitlab上 在pipeline项目中修改为在gitlab拉取Jenkinsfile文件 四、Jenkins构建细节4.1 Jenkins常用的构建触发器Jenkins内置4种构建触发器： 触发远程构建 其它工程构建后触发 定时构建 轮询SCM 4.2 触发远程构建 在项目中设置触发器 在浏览器输入JENKINS_URL/job/pipeline-project/build?token=TOKEN_NAME即可触发 4.3 其它工程构建后触发是指某一工程构建后才会触发构建，这里测试freestyle工程构建后触发pipeline构建 在pipeline配置构建后触发 freestyle工程构建后就会触发pipeline构建 4.4 定时构建五个*分别代表分时日月周，H/2则代表每分时日月周 4.5 轮询SCM和定时构建一样是设置时间来进行构建，不过轮询只有在代码仓库发生变化后才会触发，相当于定时检查 注意：轮询会定时扫描仓库的代码，增大系统的开销，不建议使用 4.6 Gitlab Hook自动触发构建SCM轮询是Jenkins服务器主动检测gitlab中的代码有没有发生变化，而gitlab的webhook可以实现代码变更后向Jenkins服务器主动发送构建请求，实现自动构建。 在Jenkins安装gitlab和gitlab hook插件 在gitlab配置中允许发出请求 在项目中添加第一步中的地址 配置Jenkins允许接受gitlab发送过来的请求 4.7 Jenkins参数化构建前边实操演示的都是默认从master下拉取代码，如果要实现在其他分支拉取代码，就需要设置参数化构建。 在Jenkins的项目中添加参数，这里选择字符串参数 修改Jenkinsfile文件中拉取代码部分的master为变量，并推送到gitlab 在项目中添加一个v1分支 在Jenkins中拉取v1代码构建 4.8 配置邮件服务器发送构建结果 安装Email Extension插件 在Jenkins中配置 在项目中创建一个email.html文件，添加以下内容并推送到gitlab中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;UTF-8&quot;&gt;&lt;title&gt;${PROJECT_NAME}-第${BUILD_NUMBER}次构建日志&lt;/title&gt;&lt;/head&gt; &lt;body leftmargin=&quot;8&quot; marginwidth=&quot;0&quot; topmargin=&quot;8&quot; marginheight=&quot;4&quot;&gt; &lt;table width=&quot;95%&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif&quot;&gt; &lt;tr&gt; &lt;td&gt;(本邮件是程序自动下发的，请勿回复！)&lt;br/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;h2&gt; &lt;font color=&quot;#0000FF&quot;&gt;构建结果 - ${BUILD_STATUS}&lt;/font&gt; &lt;/h2&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;br /&gt; &lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建信息&lt;/font&gt;&lt;/b&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;项目名称 ： ${PROJECT_NAME}&lt;/li&gt; &lt;li&gt;构建编号 ： 第${BUILD_NUMBER}次构建&lt;/li&gt; &lt;li&gt;触发原因： ${CAUSE}&lt;/li&gt; &lt;li&gt;构建日志： &lt;a href=&quot;${BUILD_URL}console&quot;&gt;${BUILD_URL}console&lt;/a&gt;&lt;/li&gt; &lt;li&gt;构建 Url ： &lt;a href=&quot;${BUILD_URL}&quot;&gt;${BUILD_URL}&lt;/a&gt;&lt;/li&gt; &lt;li&gt;工作目录 ： &lt;a href=&quot;${PROJECT_URL}ws&quot;&gt;${PROJECT_URL}ws&lt;/a&gt;&lt;/li&gt; &lt;li&gt;项目 Url ： &lt;a href=&quot;${PROJECT_URL}&quot;&gt;${PROJECT_URL}&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;Changes Since Last Successful Build:&lt;/font&gt;&lt;/b&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;历史变更记录 : &lt;a href=&quot;${PROJECT_URL}changes&quot;&gt;${PROJECT_URL}changes&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; ${CHANGES_SINCE_LAST_SUCCESS,reverse=true, format=&quot;Changes for Build #%n:&lt;br /&gt;%c&lt;br /&gt;&quot;,showPaths=true,changesFormat=&quot;&lt;pre&gt;[%a]&lt;br /&gt;%m&lt;/pre&gt;&quot;,pathFormat=&quot; %p&quot;} &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建情况总览:&lt;/font&gt;&lt;/b&gt;${TEST_COUNTS,var=&quot;fail&quot;}&lt;br/&gt; &lt;hr size=&quot;2&quot; width=&quot;100%&quot; align=&quot;center&quot; /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;textarea cols=&quot;80&quot; rows=&quot;30&quot; readonly=&quot;readonly&quot; style=&quot;font-family: Courier New&quot;&gt;${BUILD_LOG,maxLines=23}&lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 在Jenkinsfile中添加发送email功能，post即指构建后操作 123456789post { always { emailext( subject: '构建通知：${PROJECT_NAME} - Build # ${BUILD_NUMBER} - ${BUILD_STATUS}!', body: '${FILE,path=&quot;email.html&quot;}', to: 'chenqiming13@qq.com' ) }} 构建测试是否收到邮件 五、Jenkins+SonarQube代码审查 sonarqube是一个用于管理代码质量的开放平台，可以快速定位代码中潜在的错误。 环境要求： JDK11 PostgreSQL 5.1 安装PostgreSQL 新版本的sonarqube不再支持MySQL，所以在Jenkins服务器上安装PostgreSQL并创建一个sonar数据库 1234567891011121314151617181920212223#安装postgresqlyum -y install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-redhat-repo-latest.noarch.rpmyum -y install postgresql96-server postgresql96-contrib#初始化postgresqlpostgresql-9.6-setup initdbsystemctl enable postgresql-9.6.servicesystemctl start postgresql-9.6.service#初始化之后会自动创建postgres用户，切换用户su - postgres#psql进入命令行模式配，\\q退出psqlalter user postgres with password 'toortoor';create database sonar;create user sonar;alter user sonar with password 'toortoor';alter role sonar createdb;alter role sonar superuser;alter role sonar createrole;alter database sonar owner to sonar;\\q 开启远程访问和远程连接 12345vim /var/lib/pgsql/9.6/data/postgresql.conflisten_addresses = '*'vim /var/lib/pgsql/9.6/data/pg_hba.confhost all all 127.0.0.1/32 trusthost all all 192.168.88.1/32 trust 重启服务 5.2 安装SonarQube 安装sonarqube 123456wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.7.1.42226.zipunzip sonarqube-8.7.1.42226.zipmv sonarqube-8.7.1.42226 sonarqubeuseradd sonarchown -R sonar:sonar sonarqube/*mv sonarqube /opt/sonarqube 修改sonar配置文件 123456vim sonarqube/conf/sonar.propertiessonar.jdbc.username=sonarsonar.jdbc.password=toortoorsonar.jdbc.url=jdbc:postgresql://localhost/sonarqube?currentSchema=my_schemasonar.jdbc.url=jdbc:postgresql://localhost/sonar?currentSchema=publicsonar.web.port=9000 启动sonarqube 123#只能用sonar用户启动su sonar./opt/sonarqube/bin/linux-x86-64/sonar.sh start 遇到的问题 123456#sonar用户线程数不够，*代表所有用户cat /etc/security/limits.conf* soft nofile 65536* hard nofile 65536* soft noproc 65535* hard noproc 65535 deploy.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374apiVersion: apps/v1kind: Deploymentmetadata: name: sonarqube labels: app: sonarqubespec: replicas: 1 selector: matchLabels: app: sonarqube template: metadata: name: sonarqube labels: app: sonarqube spec: initContainers: - name: init-sysctl image: busybox imagePullPolicy: IfNotPresent command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;] securityContext: privileged: true containers: - name: sonarqube image: sonarqube:lts-community imagePullPolicy: IfNotPresent ports: - containerPort: 9000 env: # 此处用到了集群现有的pg # 数据库用户sonarqube # 数据库名sonarqube - name: SONARQUBE_JDBC_USERNAME value: &quot;sonarqube&quot; - name: SONARQUBE_JDBC_PASSWORD value: &quot;dangerous&quot; - name: SONARQUBE_JDBC_URL value: &quot;jdbc:postgresql://dcs-installer-gitlab-postgresql.dcs-system:5432/sonarqube&quot; livenessProbe: httpGet: path: /sessions/new port: 9000 initialDelaySeconds: 60 periodSeconds: 30 readinessProbe: httpGet: path: /sessions/new port: 9000 initialDelaySeconds: 60 periodSeconds: 30 failureThreshold: 6 resources: limits: cpu: 2000m memory: 2048Mi requests: cpu: 1000m memory: 1024Mi ---apiVersion: v1kind: Servicemetadata: name: sonarqubespec: type: NodePort selector: app: sonarqube ports: - port: 9000 targetPort: 9000 protocol: TCP 5.3 Jenkins整合SonarQube 安装sonarqube scanner插件 在Jenkins安装sonarqube scanner 在sonarqube中生成密钥 在系统配置中配置sonarqube server，利用刚刚生成的密钥 5.4 实现代码审查非流水线项目添加代码审查 在项目中添加构建步骤 123456789101112131415161718192021#Must be unique in a given SonarQube instance# SonarQube创建项目时的keysonar.projectKey=freestyle_project#This is the name and version displayed in the SonarQube UI.# 项目名称sonar.projectName=freestyle_project# 项目版本sonar.projectVersion=1.0#Path is relative to the sonar-project.properties file.#This property is optional if sonar.modules is set.sonar.sources=.# 忽略扫描的目录sonar.exclusions=**/test/**,**/target/**sonar.java.source=11sonar.java.target=11#Encoding of the source code.Default is default system encoding.sonar.sourceEncoding=UTF-8 构建项目 回到sonarqube就可以看到提交的代码审查了 流水线项目添加代码审查 在项目中创建sonar-project.properties 1234567891011121314151617#Must be unique in a given SonarQube instancesonar.projectKey=pipeline_project#This is the name and version displayed in the SonarQube UI.sonar.projectName=pipeline_projectsonar.projectVersion=1.0#Path is relative to the sonar-project.properties file.#This property is optional if sonar.modules is set.sonar.sources=.sonar.exclusions=**/test/**,**/target/**sonar.java.source=11sonar.java.target=11#Encoding of the source code.Default is default system encoding.sonar.sourceEncoding=UTF-8 修改Jenkinsfile文件 123456789101112stage('code checking'){ steps { script { //引入scanner工具 scannerHome = tool 'sonar-scanner' } //引入sonarqube服务器环境 withSonarQubeEnv('sonarqube') { sh &quot;${scannerHome}/bin/sonar-scanner&quot; } }} 将Jenkinsfile和sonar-project.properties推送到gitlab 构建项目后在sonarqube就可以看到代码审查了 六、Jenkins+Docker+SpringCloud微服务持续集成 大致流程： 开发人员push代码到gitlab。 Jenkins服务器进行编译打包并构建镜像推送到harbor镜像仓库（服务器的JAVA环境要与项目对应）。 Jenkins服务器触发远程命令使生产服务器（tomcat）从私有仓库拉取镜像。 生产服务器（tomcat）生成容器，项目上线。 用户访问。 6.1 Harbor部署 下载 1curl -O https://github.com/goharbor/harbor/releases/download/v2.3.4/harbor-offline-installer-v2.3.4.tgz 修改 harbor.yml 123456789101112131415161718192021222324252627282930313233343536cp harbor.yml.tmpl harbor.ymlegrep -v '^#|^$|*#' harbor.ymlhostname: 192.168.88.30http: port: 81harbor_admin_password: toortoordatabase: password: toortoor max_idle_conns: 100 max_open_conns: 900data_volume: /root/cicd/harbor/datatrivy: ignore_unfixed: false skip_update: false insecure: falsejobservice: max_job_workers: 10notification: webhook_job_max_retry: 10chart: absolute_url: disabledlog: level: info local: rotate_count: 50 rotate_size: 200M location: /root/cicd/harbor/var_version: 2.3.0proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy docker-compose.yaml 文件中指定 harbor 访问端口为 80，避免与 gitlab 冲突， 修改为 81 通过脚本部署 12./prepare./install.sh 创建私有仓库 修改 daemon.json，让 docker 信任此仓库 1234{ &quot;registry-mirrors&quot;: [&quot;https://5v5rh037.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [&quot;192.168.88.30:81&quot;]} 登录远程镜像仓库 1docker login -u admin -p toortoor 192.168.88.30:81 6.2 提交代码到Gitlab 6.3 Jenkins创建流水线工程 创建工程 在项目根目录下创建 Jenkinsfile，通过语法生成器进行编写 Jenkinsfile 拉取代码部分 在流水线工程中创建参数，如果有多个服务就可以根据参数选择进行构建 Jenkinsfile 添加编译打包微服务工程部分，以及通过 Dockerfile 构建镜像部分 在 pom.xml 中添加 Dockerfile 依赖 12345678910&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.4.10&lt;/version&gt; &lt;configuration&gt; &lt;repository&gt;${project.artifactId}&lt;/repository&gt; &lt;buildArgs&gt; &lt;JAR_NAME&gt;target/${project.build.finalname}.jar&lt;/JAR_NAME&gt; &lt;/buildArgs&gt; &lt;/plugin&gt; 在项目根目录添加 Dockerfile 123456FROM openjdk:11ARG JAR_NAMEWORKDIR /usr/src/myappEXPOSE 8080COPY ./${JAR_NAME} /usr/src/myapp/ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/usr/src/myapp/${JAR_NAME}&quot;] Jenkinsfile 添加上传镜像至 Harbor 部分 在 Jenkins 中添加 Harbor 账户凭证，以便于上传代码，这里用到的是 Harbor 的用户名和密码 保留好凭证ID 在语法生成器中生成新语法用于登录 Harbor 仓库 Jenkins 安装 Publish Over SSH 插件，能够对远程主机发送 shell 命令，安装完后先给远程主机发送公钥 1ssh-copy-id root@192.168.88.30 发送完后在系统配置进行配置 接着生成流水线语法，去远程执行脚本文件 test.sh test.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#!/bin/bashharbor_url=$1project_name=$2image_name=$3service_port=$4# 删除none容器echo &quot;docker rmi none...&quot;docker images | grep none | awk '{ print &quot;docker rmi &quot;$3 }' | sh &amp;&gt;/dev/null# 检查是否有已运行容器num=`docker ps | grep $project_name | wc -l`if [ $num -gt 0 ]then docker stop $project_name &amp;&amp; docker rm $project_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker stop $project_name &amp;&amp; docker rm $project_name [OK]&quot; else echo &quot;ERROR: docker stop $project_name &amp;&amp; docker rm $project_name&quot; fifi# 检查是否有重命名镜像temp=`docker images | grep $image_name | awk '{ print $1&quot;:&quot;$2 }' | wc -l`if [ $temp -eq 1 ]then docker rmi $image_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker rmi $image_name [OK]&quot; else echo &quot;ERROR: docker rmi $image_name&quot; fifi# 登录镜像仓库echo &quot;docker login repository...&quot;docker login -u admin -p toortoor $harbor_url &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to login repository&quot;else echo &quot;docker login repository [OK]&quot;fi# 拉取镜像echo &quot;docker pull image...&quot;docker pull $image_name &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to pull image&quot;else echo &quot;docker pull image [OK]&quot;fi# 运行容器echo &quot;docker run container...&quot;docker run -d --name $project_name -p $service_port:8080 $image_name &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to run container&quot;else echo &quot;docker run container [OK]&quot;fi 最终的 Jenkinsfile 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// git凭证iddef git_auth = &quot;bad37d0d-7260-43a2-a350-9abf6e99753a&quot;// git凭证urldef git_url = &quot;http://192.168.88.30/test_group/test_project.git&quot;// Harbor地址def harbor_url = &quot;192.168.88.30:81&quot;// 镜像仓库名称def harbor_repository = &quot;test&quot;// Harbor用户凭证IDdef harbor_auth = &quot;c147d241-a254-48d6-be1c-0d5f0964ce9a&quot;// 镜像版本号def image_version = &quot;1.0&quot;// 拉取的微服务名称def project_name = &quot;test&quot;// 微服务所需端口号def service_port = &quot;8081&quot;node { // 拉取代码 stage('Pull Code') { git branch: &quot;${branch}&quot;, credentialsId: &quot;${git_auth}&quot;, url: &quot;${git_url}&quot; } // 编译，打包微服务工程，构建镜像 stage('Mvn clean package and Docker build') { sh &quot;mvn clean package dockerfile:build&quot; } // 上传镜像至Harbor stage('Push image to Harbor') { // 镜像命名，打标签 def image_name = &quot;${project_name}:${image_version}&quot; sh &quot;docker tag ${project_name}:latest ${harbor_url}/${harbor_repository}/${image_name} &amp;&amp; docker rmi ${project_name}:latest&quot; // 登录Harbor仓库 withCredentials([usernamePassword(credentialsId: &quot;${harbor_auth}&quot;, passwordVariable: 'harbor_password', usernameVariable: 'harbor_user')]) { sh &quot;docker login -u ${harbor_user} -p ${harbor_password} ${harbor_url}&quot; } // 镜像上传 sh &quot;docker push ${harbor_url}/${harbor_repository}/${project_name}:${image_version}&quot; } // 远程连接服务器拉取镜像运行 stage('Pull image and Running container') { // 获取镜像命名 def image_name = &quot;${project_name}:${image_version}&quot; sshPublisher(publishers: [sshPublisherDesc(configName: '192.168.88.30', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: './root/test.sh $harbor_url $project_name $harbor_url/$harbor_repository/image_name $service_port', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) }} 6.4 微服务架构CICD优化从以上配置中可以看到，每次构建都只能选择一个服务，且部署的服务器也只有一台，是不符合生产环境的，主要需求有以下三个： 多个微服务同时构建流水线工程 批量处理镜像 将微服务部署服务器集群 安装 Extended Choice Parameter 插件，实现多个微服务同时构建 在创建流水线时设置选项 将公钥下发，并在系统设置的 Publish over SSH 中添加多台主机 在流水线工程中添加多一个多选项配置，用于选择要部署的服务器 修改流水线语法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// git凭证iddef git_auth = &quot;bad37d0d-7260-43a2-a350-9abf6e99753a&quot;// git凭证urldef git_url = &quot;http://192.168.88.30/test_group/test_project.git&quot;// Harbor地址def harbor_url = &quot;192.168.88.30:81&quot;// 镜像仓库名称def harbor_repository = &quot;test&quot;// Harbor用户凭证IDdef harbor_auth = &quot;c147d241-a254-48d6-be1c-0d5f0964ce9a&quot;// 镜像版本号def image_version = &quot;1.0&quot;node { // 获取服务名 def selectedProjectNames = &quot;${project_name}&quot;.split(&quot;,&quot;) // 获取集群地址 def selectedServices = &quot;${publish_server}&quot;.split(&quot;,&quot;) // 拉取代码 stage('Pull Code') { git branch: &quot;${branch}&quot;, credentialsId: &quot;${git_auth}&quot;, url: &quot;${git_url}&quot; } // 编译，打包微服务工程，构建镜像 stage('Mvn clean package and Docker build') { sh &quot;mvn clean package dockerfile:build&quot; } // 上传镜像至Harbor stage('Push image to Harbor') { for(int i=0; i&lt;selectedProjectNames.length; i++) { // 获取每个选项 def project_info = selectedProjectNames[i] // 获取选项中的微服务名称 def current_project_name = &quot;${project_info}&quot;.split(&quot;@&quot;)[0] // 镜像命名，打标签 def image_name = &quot;${current_project_name}:${image_version}&quot; sh &quot;docker tag ${current_project_name}:latest ${harbor_url}/${harbor_repository}/${image_name} &amp;&amp; docker rmi ${current_project_name}:latest&quot; // 登录Harbor仓库 withCredentials([usernamePassword(credentialsId: &quot;${harbor_auth}&quot;, passwordVariable: 'harbor_password', usernameVariable: 'harbor_user')]) { sh &quot;docker login -u ${harbor_user} -p ${harbor_password} ${harbor_url}&quot; } // 镜像上传 sh &quot;docker push ${harbor_url}/${harbor_repository}/${current_project_name}:${image_version}&quot; } } // 远程连接服务器拉取镜像运行 stage('Pull image and Running container') { for(int i=0; i&lt;selectedProjectNames.length; i++) { // 获取每个选项 def project_info = selectedProjectNames[i] // 获取选项中的微服务名称 def current_project_name = &quot;${project_info}&quot;.split(&quot;@&quot;)[0] // 获取选项中的微服务所需端口 def current_project_port = &quot;${project_info}&quot;.split(&quot;@&quot;)[1] // 需要拉取的镜像 def image_name = &quot;${current_project_name}:${image_version}&quot; // 遍历所有服务器，分别部署 for(int j=0; j&lt;selectedServices.length; j++) { // 获取当前服务器 def current_server = selectedServices[j] // 加上参数配置，根据配置文件的不同选择不同的服务器部署 if(current_server == &quot;192.168.88.31&quot;) { activeProfile = activeProfile+&quot;serviceName-server1&quot; }else if(current_server == &quot;192.168.88.32&quot;) { activeProfile = activeProfile+&quot;serviceName-server2&quot; } // 远程执行脚本 sshPublisher(publishers: [sshPublisherDesc(configName: &quot;${current_server}&quot;, transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: './root/test.sh $harbor_url $current_project_name $harbor_url/$harbor_repository/$image_name $current_project_port $activeProfile', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) } } }} 修改 test.sh 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/bin/bashharbor_url=$1project_name=$2image_name=$3service_port=$4profile=$6# 删除none容器echo &quot;docker rmi none...&quot;docker images | grep none | awk '{ print &quot;docker rmi &quot;$3 }' | sh &amp;&gt;/dev/null# 检查是否有已运行容器num=`docker ps | grep $project_name | wc -l`if [ $num -gt 0 ]then docker stop $project_name &amp;&amp; docker rm $project_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker stop $project_name &amp;&amp; docker rm $project_name [OK]&quot; else echo &quot;ERROR: docker stop $project_name &amp;&amp; docker rm $project_name&quot; fifi# 检查是否有重命名镜像temp=`docker images | grep $image_name | awk '{ print $1&quot;:&quot;$2 }' | wc -l`if [ $temp -eq 1 ]then docker rmi $image_name &amp;&gt;/dev/null if [ `echo $?` -eq 0 ] then echo &quot;docker rmi $image_name [OK]&quot; else echo &quot;ERROR: docker rmi $image_name&quot; fifi# 登录镜像仓库echo &quot;docker login repository...&quot;docker login -u admin -p toortoor $harbor_url &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to login repository&quot;else echo &quot;docker login repository [OK]&quot;fi# 拉取镜像echo &quot;docker pull image...&quot;docker pull $image_name &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to pull image&quot;else echo &quot;docker pull image [OK]&quot;fi# 运行容器echo &quot;docker run container...&quot;docker run -d --name $project_name -p $service_port:8080 $image_name $profile &amp;&gt;/dev/nullif [ `echo $?` -ne 0 ]then echo &quot;ERROR: failed to run container&quot;else echo &quot;docker run container [OK]&quot;fi 七、基于K8S的CICD基于 Kubernetes 平台来实现 CICD 功能。 7.1 Jenkins对接K8S 首先通过 bitnami helm 部署 Jenkins（Master），需开放 jnlp 50000 端口 1234567# 添加repo源helm repo add bitnami https://charts.bitnami.com/bitnami# 下载chart到本地helm pull bitnami/jenkins# 根据需求修改values.yaml# 部署helm install jenkins jeknins/ RBAC 授权，SA 名为 jenkins，后续会用到 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647apiVersion: v1kind: ServiceAccountmetadata: name: jenkins namespace: nextcloud---kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: jenkins namespace: nextcloudrules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;pods/exec&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;pods/log&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;] - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: jenkins namespace: nextcloudroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: jenkinssubjects: - kind: ServiceAccount name: jenkins namespace: nextcloud 获取刚刚创建的 ca.crt 信息 1kubectl get secret jenkins-token-xxx -oyaml | awk '/ca.crt/{ print $2 }' | base64 -d 在 Jenkins 创建凭据 添加云，这里用到刚刚创建的凭据 设置 Pod Template，即 Jenkins Slave，将包含 jnlp 一个容器，来实现编译打包等功能。 这里需挂载宿主机的 docker.sock、docker 和 kubectl 的二进制文件。 然后使用到刚刚创建的 SA。 测试链接。 7.2 构建前准备需要在项目的根目录下准备好 Dockerfile、Jenkinsfile、deploy.yaml。 Jenkinsfile 123456789101112131415161718192021222324252627282930313233343536373839404142// gitlab及harbur凭据认证等信息def git_auth = &quot;764f29f3-8955-4551-a23a-659aa4c8deaa&quot;def git_url = &quot;http://192.168.159.12:30098/root/nextcloud.git&quot;def harbor_url = &quot;192.168.159.101&quot;def harbor_repository = &quot;nextcloud&quot;def harbor_auth = &quot;7ebc36f8-73a3-417f-864f-24856490b1a2&quot;def project_name = &quot;nextcloud&quot;// jenkins-slave为pod template中的namenode('jenkins-slave') { // jnlp为pod template中的容器名称 container('jnlp') { // pipeline步骤 stage('Git Clone') { git branch: &quot;${branch}&quot;, credentialsId: &quot;${git_auth}&quot;, url: &quot;${git_url}&quot; } stage('Docker Build') { sh &quot;docker build -t ${harbor_url}/${harbor_repository}/${project_name}:${branch}-${version} .&quot; } stage('Docker Push') { withCredentials([usernamePassword(credentialsId: &quot;${harbor_auth}&quot;, passwordVariable: 'harbor_password', usernameVariable: 'harbor_user')]) { sh &quot;docker login -u ${harbor_user} -p ${harbor_password} ${harbor_url}&quot; } sh &quot;docker push ${harbor_url}/${harbor_repository}/${project_name}:${branch}-${version}&quot; sh &quot;docker rmi ${harbor_url}/${harbor_repository}/${project_name}:${branch}-${version}&quot; } } // 在deploy.yaml中将镜像tag设为了build-tag,方便通过sed修改 stage('Sed YAML') { sh &quot;sed -i 's#build-tag#${branch}-${version}#g' deploy.yaml&quot; } stage('Deploy to K8s') { sh &quot;kubectl apply -f deploy.yaml&quot; }} 7.3 创建流水线项目这里添加了两个字符参数，用于构建前选择分支和 tag 的标签定义。 构建测试，会发现集群中创建了一个 jenkins-slave-xxx 的 pod，pod 中包含 jnlp 一个容器，通过观察日志可发现与 Jenkins Master 进行了连接。 Jenkins 中查看流水线进度。 流水线结束后，通过 kubectl 可看到项目已部署，且使用的镜像是刚刚构建好的。","link":"/2024/02/18/jenkins/"},{"title":"linux基础网络服务","text":"一、DHCP服务1.1 DHCP服务介绍DHCP服务即动态主机配置协议，被运用在局域网中，主要的作用是分配IP地址。 DHCP服务采用的是UDP协议，发送采用UDP67端口，接受则采用UDP68端口。 1.2 DHCP服务部署 安装DHCP 1yum -y install dhcp DHCP配置文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150cp /usr/share/doc/dhcp*/dhcpd.conf.example /etc/dhcp/dhcpd.confcat /etc/dhcp/dhcpd.conf# DHCP服务配置文件分为全局配置和作用域配置，很好区分：subnet的就是作用域 不在subnet里面的就是全局设置。# dhcpd.conf## Sample configuration file for ISC dhcpd## DNS全局选项，指定DNS服务器的地址，可以是IP，也可以是域名。# option definitions common to all supported networks...# DNS的域名option domain-name &quot;example.org&quot;;# 具体的DNS服务器option domain-name-servers ns1.example.org, ns2.example.org;# 租约设置，默认租约为600sdefault-lease-time 600;# 租约设置，最大租约为7200s，当客户端未请求明确的租约时间。max-lease-time 7200;# 动态DNS更新方式(none:不支持；interim:互动更新模式；ad-hoc:特殊更新模式)# Use this to enble / disable dynamic dns updates globally.# ddns-update-style none;# 如果该DHCP服务器是本地官方DHCP就将此选项打开，避免其他DHCP服务器的干扰。# 当一个客户端试图获得一个不是该DHCP服务器分配的IP信息，DHCP将发送一个拒绝消息，而不会等待请求超时。# 当请求被拒绝，客户端会重新向当前DHCP发送IP请求获得新地址。# 保证IP是自己发出去的## If this DHCP server is the official DHCP server for the local# network, the authoritative directive should be uncommented.# 开启此项表权威DHCP# authoritative;# Use this to send dhcp log messages to a different log file (you also# have to hack syslog.conf to complete the redirection).# 日志级别log-facility local7;# No service will be given on this subnet, but declaring it helps the # DHCP server to understand the network topology.#作用域相关设置指令# subnet 定义一个作用域# netmask 定义作用域的掩码# range 允许发放的IP范围# option routers 指定网关地址# option domain-name-servers 指定DNS服务器地址# option broadcast-address 广播地址### 案例:定义一个作用域 网段为10.152.187.0 掩码为255.255.255.0# 此作用域不提供任何服务subnet 10.152.187.0 netmask 255.255.255.0 {}# This is a very basic subnet declaration.# 案例:定义一个基本的作用域# 网段10.254.239.0 掩码255.255.255.224# 分发范围10.254.239.10-20# 网关为rtr-239-0-1.example.org, rtr-239-0-2.example.orgsubnet 10.254.239.0 netmask 255.255.255.224 { range 10.254.239.10 10.254.239.20; option routers rtr-239-0-1.example.org, rtr-239-0-2.example.org;}# This declaration allows BOOTP clients to get dynamic addresses,# which we don't really recommend.# 案例:允许采用bootp协议的客户端动态获得地址# bootp DHCP的前身# BOOTP用于无盘工作站的局域网中，可以让无盘工作站从一个中心服务器上获得IP地址。通过BOOTP协议可以为局域网中的无盘工作站分配动态IP地址，# 这样就不需要管理员去为每个用户去设置静态IP地址。subnet 10.254.239.32 netmask 255.255.255.224 { range dynamic-bootp 10.254.239.40 10.254.239.60; option broadcast-address 10.254.239.31; option routers rtr-239-32-1.example.org;}# 案例:一个简单的作用域案例# A slightly different configuration for an internal subnet.subnet 10.5.5.0 netmask 255.255.255.224 { range 10.5.5.26 10.5.5.30; option domain-name-servers ns1.internal.example.org; option domain-name &quot;internal.example.org&quot;; option routers 10.5.5.1; option broadcast-address 10.5.5.31; default-lease-time 600; max-lease-time 7200;}# Hosts which require special configuration options can be listed in# host statements. If no address is specified, the address will be# allocated dynamically (if possible), but the host-specific information# will still come from the host declaration.## 保留地址:可以将指定的IP分发给指定的机器，根据网卡的MAC地址来做触发# host: 启用保留。# hardware:指定客户端的mac地址# filename:指定文件名# server-name:指定下一跳服务器地址# fixed-address: 指定保留IP地址### 案例:这个案例中分发给客户端的不是IP地址信息，而是告诉客户端去找toccata.fugue.com服务器，并且下载vmunix.passacaglia文件host passacaglia { hardware ethernet 0:0:c0:5d:bd:95; filename &quot;vmunix.passacaglia&quot;; server-name &quot;toccata.fugue.com&quot;;}# Fixed IP addresses can also be specified for hosts. These addresses# should not also be listed as being available for dynamic assignment.# Hosts for which fixed IP addresses have been specified can boot using# BOOTP or DHCP. Hosts for which no fixed address is specified can only# be booted with DHCP, unless there is an address range on the subnet# to which a BOOTP client is connected which has the dynamic-bootp flag# set.# 案例:保留地址，将指定IP(fantasia.fugue.com对应的IP)分给指定客户端网卡(MAC:08:00:07:26:c0:a5)host fantasia { hardware ethernet 08:00:07:26:c0:a5; fixed-address fantasia.fugue.com;}# 超级作用域# 超级作用域是DHCP服务中的一种管理功能，使用超级作用域，可以将多个作用域组合为单个管理实体。# You can declare a class of clients and then do address allocation# based on that. The example below shows a case where all clients# in a certain class get addresses on the 10.17.224/24 subnet, and all# other clients get addresses on the 10.0.29/24 subnet.# 在局域网中，可以配置策略根据各个机器的具体信息分配IP地址和其他的网络参数，客户机的具体信息：客户机能够给dhcp服务提供的信息由两个，# 第一个就是网卡的dhcp-client-identifier（mac地址），# 第二个就是设备的vendor-class-identifier。# 管理员可以根据这两个信息给不同的机器分组。# 案例:# 按client某种类型分组DHCP,而不是按物理接口网段# 例子: SUNW 分配地址段10.17.224.0/24# 非SUNW的主机,分配地址段10.0.29.0/24# 定义一个dhcp类:foo# request广播中vendor-class-identifier字段对应的值前四个字节如果是&quot;SUNW&quot;,则视合法客户端.class &quot;foo&quot; { match if substring (option vendor-class-identifier, 0, 4) = &quot;SUNW&quot;;}# 定义一个超级作用域: 224-29shared-network 224-29 {# 定义第一个作用域 subnet 10.17.224.0 netmask 255.255.255.0 { option routers rtr-224.example.org; }# 定义第二个作用域 subnet 10.0.29.0 netmask 255.255.255.0 { option routers rtr-29.example.org; }# 关连池,如果客户端匹配foo类，将获得该池地址 pool { allow members of &quot;foo&quot;; range 10.17.224.10 10.17.224.250; }# 关连池,如果客户端配置foo类，则拒绝获得该段地址 pool { deny members of &quot;foo&quot;; range 10.0.29.10 10.0.29.230; }} 1.3 配置作用域 配置作用域 12345678subnet 192.168.88.0 netmask 255.255.255.0 { range 192.168.88.150 192.168.88.160; # 发放地址范围 option routers 192.168.88.0; # 网关 option broadcast-address 192.168.88.255; # 广播地址 option domain-name-servers 8.8.8.8, 114.114.114.114; # 设置DNS default-lease-time 7200; # 默认租约2小时 max-lease-time 10800; # 最大租约3小时} 将另一台主机网卡设置为DHCP模式 12# 重启网络服务systemctl restart network 用dhclient命令进行测试 1234# 释放IPdhclient -r ens33# 获取IPdhclient -d ens33 查看是否获取了150-160网段内的地址 1.4 保留地址当租约到期的时候，client端只能乖乖交出IP地址，下一次获取就未必是同样的地址了，但公司中往往有些机器要用固定的地址，例如打印机、文件服务器等等，所以在DHCP中可以设置保留地址为其使用。 DHCP是根据主机网卡的MAC地址来做匹配，将保留的IP地址分给相应的主机网卡MAC地址。 获取网卡MAC地址 添加保留地址配置 12345vim /etc/dhcp/dhcpd.confhost fantasia { hardware ethernet 00:0c:29:6c:6f:0d; fixed-address 192.168.88.155;} 查看是否获取相应地址 12dhclient -r ens37dhclient -d ens37 1.5 超级作用域超级作用域简单来说就是将两个或两个以上的不同网段的作用域合成一个作用域。 添加超级作用域 12345678910111213141516171819202122# 添加作用域之前DHCP必须拥有两个网段的网卡shared-network supernet { subnet 192.168.88.0 netmask 255.255.255.0 { range 192.168.88.150 192.168.88.160; option routers 192.168.88.2; option broadcast-address 192.168.88.255; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 7200; max-lease-time 10800; } subnet 192.168.99.0 netmask 255.255.255.0 { range 192.168.99.150 192.168.99.160; option routers 192.168.99.0; option broadcast-address 192.168.99.255; option domain-name-servers 8.8.8.8, 114.114.114.114; default-lease-time 7200; max-lease-time 10800; }} 二、DNS服务2.1 DNS服务介绍DNS即域名系统，在互联网中为域名和IP地址进行相互映射的一个分布式数据库。 DNS采用UDP协议，使用UDP53端口进行传输。 DNS记录类型： A：ipv4 记录，将域名映射到 ipv4 地址 AAAA：ipv6 记录，将域名映射到 ipv6 地址 CNAME：别名记录，将域名映射到另一个域名 MX：电邮交互记录，将域名映射到邮件服务器地址 TXT：文本记录，是任意可读的文本 DNS 记录 SRV：服务器资源记录，用来标识某个服务器使用了某个服务，创建于微软系统的目录管理 NS：名称服务器记录，支持将子域名委托给其他 DNS 服务商解析 CAA：CAA 资源记录，可以限定域名颁发证书和 CA 之间的关系 2.2 DNS服务部署 安装DNS 12yum -y install bind bind-chroot# bind-chroot是bind的一个功能,使bind可以在一个chroot的模式下运行.也就是说,bind运行时的/(根)目录,并不是系统真正的/(根)目录,只是系统中的一个子目录而已.这样做的目的是为了提高安全性.因为在chroot的模式下,bind可以访问的范围仅限于这个子目录的范围里,无法进一步提升,进入到系统的其他目录中。bind的默认启动方式就是chroot方式。 将配置文件和区域数据库文件拷贝到chroot目录下 12345cp -p /etc/named.conf /var/named/chroot/etc/cp -pr /var/named/name* /var/named/chroot/var/named/chown -R named:named /var/named/chroot/*# 配置文件 /var/named/chroot/etc/named.conf# 区域数据库文件 /var/named/chroot/var/named/ 配置文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274/* Sample named.conf BIND DNS server 'named' configuration file for the Red Hat BIND distribution. See the BIND Administrator's Reference Manual (ARM) for details about the configuration located in /usr/share/doc/bind-{version}/Bv9ARM.html*/options{ // Put files that named is allowed to write in the data/ directory: #指定区域数据库文件的路径目录 directory &quot;/var/named&quot;; // &quot;Working&quot; directory #CACHE文件路径,指定服务器在收到rndc dump命令时，转储数据到文件的路径。默认named_dump.db dump-file &quot;data/cache_dump.db&quot;; #静态文件路径,指定服务器在收到rndc stats命令时，追加统计数据的文件路径。默认named.stats statistics-file &quot;data/named_stats.txt&quot;; #内存静态文件路径,服务器在退出时，将内存统计写到文件的路径。默认named.memstats memstatistics-file &quot;data/named_mem_stats.txt&quot;; # 指定服务器在通过rndc recursing命令指定转储当前递归请求到的文件路径。默认named.recursing recursing-file &quot;data/named.recursing&quot;; #在收到rndc secroots指令后，服务器转储安全根的目的文件的路径名。默认named.secroots secroots-file &quot;data/named.secroots&quot;; /* Specify listenning interfaces. You can use list of addresses (';' is delimiter) or keywords &quot;any&quot;/&quot;none&quot; */ #IPV4监听端口为53,允许任何人连接 //listen-on port 53 { any; }; #IPv4监听端口为53，只允许本机连接 listen-on port 53 { 127.0.0.1; }; #IPV6监听端口为53,允许任何人连接 //listen-on-v6 port 53 { any; }; #IPv6监听端口为53，只允许本机连接 listen-on-v6 port 53 { ::1; }; /* 访问控制 Access restrictions 两个重要选项 There are two important options: allow-query { argument; }; - allow queries for authoritative data 允许查询来自权威数据 allow-query-cache { argument; }; - allow queries for non-authoritative data (mostly cached data) 允许查询来自非权威数据 You can use address, network address or keywords &quot;any&quot;/&quot;localhost&quot;/&quot;none&quot; as argument 大括号中可以使用IP地址、网段、或者关键字 any任何人 localhost本机 none任何人不允许 Examples: allow-query { localhost; 10.0.0.1; 192.168.1.0/8; }; allow-query-cache { ::1; fe80::5c63:a8ff:fe2f:4526; 10.0.0.1; }; */ #指定允许哪些主机可以进行普通的DNS查询,可以是关键字:any/localhost/none,也可以是IPV4,IPV6地址 allow-query { localhost; }; #指定允许哪些主机可以对缓存的访问 allow-query-cache { localhost; }; /* Enable/disable recursion - recursion yes/no; 递归查询开关 - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion. 假如你建立的是一个权威DNS你不需要开启递归 - If you are building a RECURSIVE (caching) DNS server, you need to enable recursion. 假如你建立的是一个递归DNS,你需要开启递归服务 - If your recursive DNS server has a public IP address, you MUST enable access 如果你的递归DNS是具有公网IP，你必须要设置访问控制来限制对合法用户的查询. control to limit queries to your legitimate users. Failing to do so will cause your server to become part of large scale DNS amplification 否者你的DNS会被大规模的攻击 attacks. Implementing BCP38 within your network would greatly 在您的网络中实现BCP38将非常重要减少此类攻击面 reduce such attack surface */ #开启递归 recursion yes; #Domain Name System Security Extensions (DNS安全扩展） /* DNSSEC related options. See information about keys (&quot;Trusted keys&quot;, bellow) */ /* Enable serving of DNSSEC related data - enable on both authoritative and recursive servers DNSSEC aware servers */ #开启DNSSEC在权威或者递归服务器之间信任服务 dnssec-enable yes; /* Enable DNSSEC validation on recursive servers */ #开启DNSSEC验证在递归服务器 dnssec-validation yes; /* In RHEL-7 we use /run/named instead of default /var/run/named so we have to configure paths properly. */ #PID文件路径 pid-file &quot;/run/named/named.pid&quot;; #session-keyfile文件路径 session-keyfile &quot;/run/named/session.key&quot;; #指定目录，其中保存着跟踪被管理DNSSEC密钥文件。默认为工作目录。 managed-keys-directory &quot;/var/named/dynamic&quot;;};logging {#开启DNS日志记录/* If you want to enable debugging, eg. using the 'rndc trace' command, * named will try to write the 'named.run' file in the $directory (/var/named). * By default, SELinux policy does not allow named to modify the /var/named directory, * so put the default debug log file in data/ : */ channel default_debug { file &quot;data/named.run&quot;; severity dynamic; };/*##日志分为两种 告警和访问logging { channel warning { file &quot;data/dns_warning&quot; versions 10 size 10m; severity warning; print-category yes; print-severity yes; print-time yes; }; channel general_dns { file &quot;data/dns_log&quot; versions 10 size 100m; severity info; print-category yes; print-severity yes; print-time yes; }; #默认日志 warning category default { warning; }; #访问日志级别 general_dns info category queries { general_dns; };};*/};/*通过Views指令配置智能查询DNS Views let a name server answer a DNS query differently depending on who is asking. By default, if named.conf contains no &quot;view&quot; clauses, all zones are in the &quot;default&quot; view, which matches all clients. Views are processed sequentially. The first match is used so the last view should match &quot;any&quot; - it's fallback and the most restricted view. If named.conf contains any &quot;view&quot; clause, then all zones MUST be in a view.*/#配置一个明称为localhost_resolver的智能访问视图view &quot;localhost_resolver&quot;{/* This view sets up named to be a localhost resolver ( caching only nameserver ). * If all you want is a caching-only nameserver, then you need only define this view: */ #允许使用该视图解析的客户端 localhost本机 any 任何机器 或者网段 match-clients { localhost; }; #允许递归 recursion yes; # all views must contain the root hints zone: #根域 zone &quot;.&quot; IN { #域类型为hint,还有master slave forward等类型 type hint; #区域数据库文件路径 file &quot;/var/named/named.ca&quot;; }; /* these are zones that contain definitions for all the localhost * names and addresses, as recommended in RFC1912 - these names should * not leak to the other nameservers: */ #包含子配置文件 include &quot;/etc/named.rfc1912.zones&quot;;};#定义视图internalview &quot;internal&quot;{/* This view will contain zones you want to serve only to &quot;internal&quot; clients that connect via your directly attached LAN interfaces - &quot;localnets&quot; . */ match-clients { localnets; }; recursion yes; zone &quot;.&quot; IN { type hint; file &quot;/var/named/named.ca&quot;; }; /* these are zones that contain definitions for all the localhost * names and addresses, as recommended in RFC1912 - these names should * not leak to the other nameservers: */ include &quot;/etc/named.rfc1912.zones&quot;; // These are your &quot;authoritative&quot; internal zones, and would probably // also be included in the &quot;localhost_resolver&quot; view above : /* NOTE for dynamic DNS zones and secondary zones: DO NOT USE SAME FILES IN MULTIPLE VIEWS! If you are using views and DDNS/secondary zones it is strongly recommended to read FAQ on ISC site (www.isc.org), section &quot;Configuration and Setup Questions&quot;, questions &quot;How do I share a dynamic zone between multIPle views?&quot; and &quot;How can I make a server a slave for both an internal and an external view at the same time?&quot; */ zone &quot;my.internal.zone&quot; { type master; file &quot;my.internal.zone.db&quot;; }; zone &quot;my.slave.internal.zone&quot; { type slave; file &quot;slaves/my.slave.internal.zone.db&quot;; masters { /* put master nameserver IPs here */ 127.0.0.1; } ; // put slave zones in the slaves/ directory so named can update them }; zone &quot;my.ddns.internal.zone&quot; { type master; allow-update { key ddns_key; }; file &quot;dynamic/my.ddns.internal.zone.db&quot;; // put dynamically updateable zones in the slaves/ directory so named can update them }; };#设置DDNS_key#主从复制加密使用key ddns_key{ #加密方式 hmac-md5 algorithm hmac-md5; secret &quot;use /usr/sbin/dnssec-keygen to generate TSIG keys&quot;;};view &quot;external&quot;{/* This view will contain zones you want to serve only to &quot;external&quot; clients * that have addresses that are not match any above view: */ match-clients { any; }; zone &quot;.&quot; IN { type hint; file &quot;/var/named/named.ca&quot;; }; recursion no; // you'd probably want to deny recursion to external clients, so you don't // end up providing free DNS service to all takers // These are your &quot;authoritative&quot; external zones, and would probably // contain entries for just your web and mail servers: zone &quot;my.external.zone&quot; { type master; file &quot;my.external.zone.db&quot;; };};/* Trusted keys#定义信任的dnssec密钥。 This statement contains DNSSEC keys. If you want DNSSEC aware resolver you have to configure at least one trusted key. Note that no key written below is valid. Especially root key because root zone is not signed yet.*//*trusted-keys {// Root Key&quot;.&quot; 257 3 3 &quot;BNY4wrWM1nCfJ+CXd0rVXyYmobt7sEEfK3clRbGaTwSJxrGkxJWoZu6I7PzJu/ E9gx4UC1zGAHlXKdE4zYIPRhaBKnvcC2U9mZhkdUpd1Vso/HAdjNe8LmMlnzY3 zy2Xy4klWOADTPzSv9eamj8V18PHGjBLaVtYvk/ln5ZApjYghf+6fElrmLkdaz MQ2OCnACR817DF4BBa7UR/beDHyp5iWTXWSi6XmoJLbG9Scqc7l70KDqlvXR3M /lUUVRbkeg1IPJSidmK3ZyCllh4XSKbje/45SKucHgnwU5jefMtq66gKodQj+M iA21AfUVe7u99WzTLzY3qlxDhxYQQ20FQ97S+LKUTpQcq27R7AT3/V5hRQxScI Nqwcz4jYqZD2fQdgxbcDTClU0CRBdiieyLMNzXG3&quot;;// Key for forward zoneexample.com. 257 3 5 &quot;AwEAAaxPMcR2x0HbQV4WeZB6oEDX+r0QM65KbhTjrW1ZaARmPhEZZe 3Y9ifgEuq7vZ/zGZUdEGNWy+JZzus0lUptwgjGwhUS1558Hb4JKUbb OTcM8pwXlj0EiX3oDFVmjHO444gLkBO UKUf/mC7HvfwYH/Be22GnC lrinKJp1Og4ywzO9WglMk7jbfW33gUKvirTHr25GL7STQUzBb5Usxt 8lgnyTUHs1t3JwCY5hKZ6CqFxmAVZP20igTixin/1LcrgX/KMEGd/b iuvF4qJCyduieHukuY3H4XMAcR+xia2 nIUPvm/oyWR8BW/hWdzOvn SCThlHf3xiYleDbt/o1OTQ09A0=&quot;;// Key for reverse zone.2.0.192.IN-ADDRPA.NET. 257 3 5 &quot;AQOnS4xn/IgOUpBPJ3bogzwcxOdNax071L18QqZnQQQA VVr+iLhGTnNGp3HoWQLUIzKrJVZ3zggy3WwNT6kZo6c0 tszYqbtvchmgQC8CzKojM/W16i6MG/ea fGU3siaOdS0 yOI6BgPsw+YZdzlYMaIJGf4M4dyoKIhzdZyQ2bYQrjyQ 4LB0lC7aOnsMyYKHHYeRv PxjIQXmdqgOJGq+vsevG06 zW+1xgYJh9rCIfnm1GX/KMgxLPG2vXTD/RnLX+D3T3UL 7HJYHJhAZD5L59VvjSPsZJHeDCUyWYrvPZesZDIRvhDD 52SKvbheeTJUm6EhkzytNN2SN96QRk8j/iI8ib&quot;;};*/ 配置主配文件 12345678910111213141516171819202122232425262728293031vim /var/named/chroot/etc/named.confoptions { listen-on port 53 { 192.168.88.132; }; #listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query { any; }; recursion yes; dnssec-enable yes; dnssec-validation yes; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;};zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;;};include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;;systemctl start named-chroot 区域数据库文件详解 123456789101112131415161718192021222324252627282930# 正向解析 named.localhost;缓存时间$TTL 1D;@表示相应的域名@ IN SOA @ rname.invalid. (;解析的域名 类型 授权域 授权域名服务器 管理员邮箱 0 ; serial 序列号,每次更新该文件系列号都应该变大 1D ; refresh 刷新时间,即规定从域名服务器多长时间查询一个主服务器，以保证从服务器的数据是最新的 1H ; retry 重试时间,即当从服务试图在主服务器上查询更时，而连接失败了，则这个值规定了从服务多长时间后再试 1W ; expire 过期时间,从服务器在向主服务更新失败后多长时间后清除对应的记录 3H ) ; minimum 这个数据用来规定缓冲服务器不能与主服务联系上后多长时间清除相应的记录 NS @ ;NS 名称服务器，表示这个主机为域名服务器 A 127.0.0.1;主机头 A记录 IP AAAA ::1; AAAA 解析为IPV6地址# 反向解析 named.loopback$TTL 1D@ IN SOA @ rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS @ PTR localhost;IP 反向指针 域名;PTR 反向指针 反解 2.3 正向解析 修改主配文件 1234zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone&quot;;}; 创建区域数据库文件 12345678910111213141516171819202122232425cp /var/named/chroot/var/named/named.localhost /var/named/chroot/var/named/cqm.com.zonechgrp named cqm.com.zonevim /var/named/chroot/var/named/cqm.com.zone$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum# A:IPv4解析为域名# PTR:域名解析为IP# MX# CNAME:设置别名 NS dns.cqm.com.# 解析dns为192.168.88.132dns A 192.168.88.132# 解析www为192.168.88.132www A 192.168.88.132# 用news访问也解析为wwwnews CNAME www# 检测文件是否有误named-checkzone cqm.com cqm.com.zone 在客户端上配置DNS 12vim /etc/resolve.confnameserver 192.168.88.132 通过host命令进行测试 1234567yum -y install bind-utilshost www.cqm.comwww.cqm.com has address 192.168.88.132host news.cqm.comnews.cqm.com is an alias for www.cqm.com.www.cqm.com has address 192.168.88.132 2.4 反向解析 修改主配文件 1234zone &quot;88.168.192.in-addr.arpa&quot; IN { type master; file &quot;192.168.88.arpa&quot;;}; 创建区域数据库文件 1234567891011121314cp /var/named/chroot/var/named/named.loopback /var/named/chroot/var/named/192.168.88.arpavim /var/named/chroot/var/named/192.168.88.arpa$TTL 1D88.168.192.in-addr.arpa. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.132 PTR www.cqm.com.named-checkzone 88.168.192.in-addr.arpa 192.168.88.arpa 测试 12host 192.168.88.132132.88.168.192.in-addr.arpa domain name pointer www.cqm.com. 2.5 主从同步即配置两台DNS服务器，由于上边我们以及配置过主DNS了，接下来再配置一台辅DNS服务器即可。 主DNS服务器IP：192.168.88.132 辅DNS服务器IP：192.168.88.135 安装DNS 1yum -y install bind bind-chroot 配置主配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445scp root@192.168.88.132:/var/named/chroot/etc/named.conf /var/named/chroot/etc/named.confchgrp named /var/named/chroot/etc/named.confvim /var/named/chroot/etc/named.confoptions { listen-on port 53 { 192.168.88.135; }; #listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query { any; }; # 从主DNS服务器拷过来的数据不进行加密 masterfile-format text; recursion yes; dnssec-enable yes; dnssec-validation yes; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;};zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;;};zone &quot;cqm.com&quot; IN { type slave; file &quot;cqm.com.zone&quot;; masters { 192.168.88.132; };};zone &quot;88.168.192.in-addr.arpa&quot; IN { type slave; file &quot;192.168.88.arpa&quot;; masters { 192.168.88.132; };};include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;; 配置和主DNS服务器相同的区域数据库文件 1234vim cqm.com.zone...vim 192.168.88.arpa... 将DNS服务器设为自己后进行测试 1234vim /etc/reslove.comfnameserver 192.168.88.135host www.cqm.com... 2.6 智能解析在DNS中植入全世界的IP库以及IP对应的地域，当用户发来请求时，会根据用户属于哪个地区来找那个地区的区域数据库文件来进行解析，从而使得不同地域的用户解析不同。 例子： 部署一台智能解析DNS服务器，对cqm.com进行解析 深圳用户解析为1.1.1.1 广州用户解析为2.2.2.2 佛山用户解析为3.3.3.3 修改主配文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071options { listen-on port 53 { 192.168.88.132; }; #listen-on-v6 port 53 { ::1; }; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; allow-query { any; }; recursion yes; dnssec-enable yes; dnssec-validation yes; pid-file &quot;/run/named/named.pid&quot;; session-keyfile &quot;/run/named/session.key&quot;;};acl sz { # 假设该网段是深圳的IP地址段 192.168.77.0/24;};acl gz { 192.168.88.0/24;};acl fs { 192.168.99.0/24;};view shenzhen {match-clients { sz; }; zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;; }; zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone.SZ&quot;; };};view guangzhou {match-clients { gz; }; zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;; }; zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone.GZ&quot;; };};view foshan {match-clients { fs; }; zone &quot;.&quot; IN { type hint; file &quot;named.ca&quot;; }; zone &quot;cqm.com&quot; IN { type master; file &quot;cqm.com.zone.FS&quot;; };}; 添加区域数据库文件 12345678910111213141516171819202122232425262728293031323334353637383940414243cp cqm.com.zone cqm.com.zone.SZcp cqm.com.zone cqm.com.zone.GZcp cqm.com.zone cqm.com.zone.FSchgrp named cqm.com.zone.*# 深圳$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.dns A 192.168.88.132www A 1.1.1.1news CNAME www# 广州$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.dns A 192.168.88.132www A 2.2.2.2news CNAME www# 佛山$TTL 1Dcqm.com. IN SOA dns.cqm.com. rname.invalid. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum NS dns.cqm.com.dns A 192.168.88.132www A 3.3.3.3news CNAME www 测试 123# 测试主机的地址段为192.168.88.0/24网段的，所以属于广州区域，即匹配解析到2.2.2.2host www.cqm.comwww.cqm.com has address 2.2.2.2 三、FTP文件传输服务3.1 FTP服务介绍FTP即文件传输协议，是TCP/IP协议组的协议之一。 FTP默认采用TCP20和21端口，20用于传输数据，21用于控制传输信息。 FTP分别有主动传输方式和被动传输方式两种，当FTP为主动传输方式时运用20和21端口，而当FTP为被动传输方式时则会随即打开一个大于1024的端口来进行数据的传输。 3.2 FTP服务部署 安装vsftpd 1yum -y install vsftpd 主配文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167# Example config file /etc/vsftpd/vsftpd.conf## The default compiled in settings are fairly paranoid. This sample file# loosens things up a bit, to make the ftp daemon more usable.# Please see vsftpd.conf.5 for all compiled in defaults.## READ THIS: This example file is NOT an exhaustive list of vsftpd options.# Please read the vsftpd.conf.5 manual page to get a full idea of vsftpd's# capabilities.##匿名用户访问,YES是允许，NO是拒绝# Allow anonymous FTP? (Beware - allowed by default if you comment this out).anonymous_enable=YES## Uncomment this to allow local users to log in.# 本地用户登录,YES是允许，NO是拒绝.默认访问的是本地用户家目录，如果你开启了selinux# 请设置开启布尔值ftp_home_dir为ON# When SELinux is enforcing check for SE bool ftp_home_dirlocal_enable=YES##允许本地用户上传# Uncomment this to enable any form of FTP write command.write_enable=YES## Default umask for local users is 077. You may wish to change this to 022,# 上传的权限是022，使用的是umask权限。对应的目录是755，文件是644# if your users expect that (022 is used by most other ftpd's)local_umask=022## Uncomment this to allow the anonymous FTP user to upload files. This only# has an effect if the above global write enable is activated. Also, you will# obviously need to create a directory writable by the FTP user.# When SELinux is enforcing check for SE bool allow_ftpd_anon_write, allow_ftpd_full_access# 开启匿名用户上传功能，默认是拒绝的#anon_upload_enable=YES## Uncomment this if you want the anonymous FTP user to be able to create# new directories.# 开启匿名用户创建文件或文件夹权限#anon_mkdir_write_enable=YES## Activate directory messages - messages given to remote users when they# go into a certain directory.# 开启目录欢迎消息，一般对命令行登陆有效dirmessage_enable=YES## Activate logging of uploads/downloads.# 开启上传和下载日志记录功能xferlog_enable=YES##使用标准模式# Make sure PORT transfer connections originate from port 20 (ftp-data).connect_from_port_20=YES## If you want, you can arrange for uploaded anonymous files to be owned by# a different user. Note! Using &quot;root&quot; for uploaded files is not# recommended!# 声明匿名用户上传文件的所有者# 允许更改匿名用户上传文件的所有者#chown_uploads=YES#所有者为whoever#chown_username=whoever## You may override where the log file goes if you like. The default is shown# below.# 日志文件路径#xferlog_file=/var/log/xferlog## If you want, you can have your log file in standard ftpd xferlog format.# Note that the default log file location is /var/log/xferlog in this case.# 日志文件采用标准格斯xferlog_std_format=YES## You may change the default value for timing out an idle session.# 会话超时时间#idle_session_timeout=600## You may change the default value for timing out a data connection.# 数据传输超时时间#data_connection_timeout=120## It is recommended that you define on your system a unique user which the# ftp server can use as a totally isolated and unprivileged user.# FTP子进程管理用户#nopriv_user=ftpsecure## Enable this and the server will recognise asynchronous ABOR requests. Not# recommended for security (the code is non-trivial). Not enabling it,# however, may confuse older FTP clients.# 是否允许客户端发起“async ABOR”请求，该操作是不安全的默认禁止。#async_abor_enable=YES## By default the server will pretend to allow ASCII mode but in fact ignore# the request. Turn on the below options to have the server actually do ASCII# mangling on files when in ASCII mode. The vsftpd.conf(5) man page explains# the behaviour when these options are disabled.# Beware that on some FTP servers, ASCII support allows a denial of service# attack (DoS) via the command &quot;SIZE /big/file&quot; in ASCII mode. vsftpd# predicted this attack and has always been safe, reporting the size of the# raw file.# ASCII mangling is a horrible feature of the protocol.# 该选项用于指定是否允许上传时以ASCII模式传输数据#ascii_upload_enable=YES#该选项用于指定是否允许下载时以ASCII模式传输数据#ascii_download_enable=YES## You may fully customise the login banner string:# FTP文本界面登陆欢迎词#ftpd_banner=Welcome to blah FTP service.## You may specify a file of disallowed anonymous e-mail addresses. Apparently# useful for combatting certain DoS attacks.# 是否开启拒绝的Email功能#deny_email_enable=YES# (default follows)# 指定保存被拒接的Email地址的文件#banned_email_file=/etc/vsftpd/banned_emails## You may specify an explicit list of local users to chroot() to their home# directory. If chroot_local_user is YES, then this list becomes a list of# users to NOT chroot().# (Warning! chroot'ing can be very dangerous. If using chroot, make sure that# the user does not have write access to the top level directory within the# chroot)# 是否开启对本地用户chroot的限制，YES为默认所有用户都不能切出家目录，NO代表默认用户都可以切出家目录# 设置方法类似于：YES拒绝所有允许个别；NO允许所有拒绝个别#chroot_local_user=YES# 开启特例列表#chroot_list_enable=YES# (default follows)# 如果chroot_local_user的值是YES则该文件中的用户是可以切出家目录，如果是NO，该文件中的用户则不能切出家目录# 一行一个用户。#chroot_list_file=/etc/vsftpd/chroot_list## You may activate the &quot;-R&quot; option to the builtin ls. This is disabled by# default to avoid remote users being able to cause excessive I/O on large# sites. However, some broken FTP clients such as &quot;ncftp&quot; and &quot;mirror&quot; assume# the presence of the &quot;-R&quot; option, so there is a strong case for enabling it.# 是否开启ls 递归查询功能 ls -R#ls_recurse_enable=YES## When &quot;listen&quot; directive is enabled, vsftpd runs in standalone mode and# listens on IPv4 sockets. This directive cannot be used in conjunction# with the listen_ipv6 directive.# 是否开启ftp独立模式在IPV4listen=NO## This directive enables listening on IPv6 sockets. By default, listening# on the IPv6 &quot;any&quot; address (::) will accept connections from both IPv6# and IPv4 clients. It is not necessary to listen on *both* IPv4 and IPv6# sockets. If you want that (perhaps because you want to listen on specific# addresses) then you must run two copies of vsftpd with two configuration# files.# Make sure, that one of the listen options is commented !!# 是否开启ftp独立模式在ipv6listen_ipv6=YES#启用pam模块验证pam_service_name=vsftpd#是否开启userlist功能.定义对列表中的用户做定义userlist_deny=NO#NO拒绝所有人访问，对应列表中的用户可以访问，YES允许所有人访问，列表中的用户无法访问。#只有userlist_file=/etc/vsftpd/user_list定义的用户才可以访问或拒绝访问userlist_enable=YES#是否开启tcp_wrappers管理，TCP_Wrappers是一个工作在第四层（传输层）的的安全工具，#对有状态连接的特定服务进行安全检测并实现访问控制tcp_wrappers=YES 匿名用户和本地用户 需要注意的是，匿名用户访问的是/var/ftp，而本地用户访问的话是家目录。 关于权限，在主配文件中设置的权限是反码，文件实际权限 = 666 - 反码。 假如主配文件中设置为022，那么文件实际权限 = 666 - 022 = 644，文件夹实际权限 = 777 - 022 = 755。 在linux端访问FTP服务器时，无论FTP服务器是否开启了匿名用户访问，客户访问时都要输入用户名和密码，匿名用户用户名ftp，密码随意，但是需要为带有@的email地址。 开启chroot 12345678910# 是否开启对本地用户chroot的限制，YES为默认所有用户都不能切出家目录，NO代表默认用户都可以切出家目录# 设置方法类似于：YES拒绝所有允许个别；NO允许所有拒绝个别chroot_local_user=YESchroot_list_enable=YES# 特例列表chroot_list_file=/etc/vsftpd/chroot_list# 如果用户家目录有写权限的话，则该用户登陆不上# 如果想在有写权限的家目录登录的话，需在配置文件加上allow_writeable_chroot=YES 3.3 FTP命令登录到FTP服务器后，基本命令如下 123456789help # 打印命令菜单!+linux命令 # 执行linux命令lcd 目录路径 # 切换linux当前路径put mput # 上传 批量上传get mget # 下载 批量下载ls dir # 列出目录内容mkdir cd delete rmdir # 创建目录 进入目录 删除文件 删除目录pwd # 现实FTP当前路径open close bye # 开启/关闭/退出FTP 3.4 虚拟用户由于FTP是采用本地用户来进行登录的，所以会将本地用户暴露在互联网中，如果没有相关安全设置，就会造成FTP不安全。 因此FTP可以设置虚拟用户来解决该问题。 在主配文件中开启虚拟用户 123456guest_enable=YESguest_username=cqm# 虚拟用户不用本地用户的权限virtual_use_local_privs=NO# 用户文件存放地址user_config_dir=/etc/vsftpd/vconf.d 3.5 基于虚拟用户配置的安全FTP案例要求： 公司公共文件可以通过匿名下载 公式A部门、B部门、C部门分别由自己的文件夹，并相互隔离 部门之间只有主管拥有上传权限，部门员工只有下载权限 禁止用户查看家目录以外的数据 确保FTP账号安全 创建虚拟用户映射本地账号 1useradd -s /sbin/nologin -d /var/tmp/vuser_ftp cqm 创建目录，所有操作都只能在此目录进行 1234chmod 500 /var/tmp/vuser_ftpmkdir /var/tmp/vuser_ftp/{A,B,C}chmod 700 /var/tmp/vuser_ftp/*chown -R cqm:cqm /var/tmp/vuser_ftp 创建虚拟用户账号密码文件，并生成db文件 123456789101112131415vim /etc/vsftpd/vuserA_01123B_01123C_01123A_02123B_02123C_02123db_load -T -t hash -f /etc/vsftpd/vuser /etc/vsftpd/vuser.dbchmod 600 /etc/vsftpd/vuser.db 设置虚拟用户pam认证 123vim /etc/pam.d/vsftpdauth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/vuseraccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser 要求不能切出家目录，所以要设置chroot_list 1234567vim /etc/vsftpd/chroot_listA_01B_01C_01A_02B_02C_02 创建子配置文件 1234567891011121314151617181920mkdir /etc/vsftpd/vconf.d# A主管文件vim /etc/vsftpd/vconf.d/A_01# 指定家目录local_root=/var/tmp/vuser_ftp/A# 指定权限anon_umask=077# 下载权限anon_world_readable_only=NO# 上传权限anon_upload_enable=YES# 创建目录权限anon_mkdir_write_enable=YES# 删除和重命名目录权限anon_other_write_enable=YES# A员工文件vim /etc/vsftpd/vconf.d/A_02local_root=/var/tmp/vuser_ftp/Aanon_world_readable_only=NO 配置主配文件 123456789101112131415161718192021vim /etc/vsftpd/vsftpd.confanonymous_enable=YESlocal_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YESxferlog_std_format=YESchroot_local_user=YESchroot_list_enable=YESchroot_list_file=/etc/vsftpd/chroot_listlisten=YESlisten_ipv6=NOpam_service_name=vsftpduserlist_enable=YEStcp_wrappers=YESguest_enable=YESguest_username=cqmvirtual_use_local_privs=NOuser_config_dir=/etc/vsftpd/vconf.d 测试主管用户和员工用户的权限 1234ftp 192.168.88.132A_01123230 Login successful 四、Samba服务4.1 Samba服务介绍Samba是可以实现不同计算机系统之间文件共享的服务，即是在Linux和UNIX系统上实现SMB协议的一个免费软件。 SMB（Server Messages Block，信息服务块）是一种在局域网上共享文件和打印机的一种通信协议，它为局域网内的不同计算机之间提供文件及打印机等资源的共享服务。 Samba采用到的端口有： UDP 137：NetBIOS 名字服务 UDP 138：NetBIOS 数据报服务 UDP 139：SMB TCP 389：用于 LDAP (Active Directory Mode) TCP 445：NetBIOS服务在windos 2000及以后版本使用此端口, (Common Internet File System，CIFS，它是SMB协议扩展到Internet后，实现Internet文件共享) TCP 901：用于 SWAT，用于网页管理Samba 4.2 Samba服务部署 安装samba 12yum -y install samba samba-clientsystemctl start smb nmb 主配文件详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351cat /etc/samba/smb.conf.example# This is the main Samba configuration file. For detailed information about the# options listed here, refer to the smb.conf(5) manual page. Samba has a huge# number of configurable options, most of which are not shown in this example.## The Samba Wiki contains a lot of step-by-step guides installing, configuring,# and using Samba:# https://wiki.samba.org/index.php/User_Documentation## In this file, lines starting with a semicolon (;) or a hash (#) are# comments and are ignored. This file uses hashes to denote commentary and# semicolons for parts of the file you may wish to configure.## NOTE: Run the &quot;testparm&quot; command after modifying this file to check for basic# syntax errors.##---------------##SAMBA selinux相关设置，如果你开启了selinux，请注意下面的说明###Security-Enhanced Linux (SELinux) Notes:## Turn the samba_domain_controller Boolean on to allow a Samba PDC to use the# useradd and groupadd family of binaries. Run the following command as the# root user to turn this Boolean on:# 如果你在域环境中使用samba那么请设置下面的bool值# setsebool -P samba_domain_controller on## Turn the samba_enable_home_dirs Boolean on if you want to share home# directories via Samba. Run the following command as the root user to turn this# Boolean on:## 假如希望通过samba共享用户家目录请设置下面的bool值# setsebool -P samba_enable_home_dirs on## If you create a new directory, such as a new top-level directory, label it# with samba_share_t so that SELinux allows Samba to read and write to it. Do# not label system directories, such as /etc/ and /home/, with samba_share_t, as# such directories should already have an SELinux label.##加入你想将目录通过samba共享，请确认其目录标签为sambe_share_t# Run the &quot;ls -ldZ /path/to/directory&quot; command to view the current SELinux# label for a given directory.## Set SELinux labels only on files and directories you have created. Use the# chcon command to temporarily change a label:# 标签设置方法# chcon -t samba_share_t /path/to/directory## Changes made via chcon are lost when the file system is relabeled or commands# such as restorecon are run.## Use the samba_export_all_ro or samba_export_all_rw Boolean to share system# directories. To share such directories and only allow read-only permissions:# 对共享目录的权限的bool设置，只读或读写# setsebool -P samba_export_all_ro on# To share such directories and allow read and write permissions:# setsebool -P samba_export_all_rw on## To run scripts (preexec/root prexec/print command/...), copy them to the# /var/lib/samba/scripts/ directory so that SELinux will allow smbd to run them.# Note that if you move the scripts to /var/lib/samba/scripts/, they retain# their existing SELinux labels, which may be labels that SELinux does not allow# smbd to run. Copying the scripts will result in the correct SELinux labels.# Run the &quot;restorecon -R -v /var/lib/samba/scripts&quot; command as the root user to# apply the correct SELinux labels to these files.##--------------##======================= Global Settings =====================================#全局设置，对整个服务都生效[global]#网络设置# ----------------------- Network-Related Options -------------------------## workgroup = the Windows NT domain name or workgroup name, for example, MYGROUP.## server string = the equivalent of the Windows NT Description field.## netbios name = used to specify a server name that is not tied to the hostname,# maximum is 15 characters.## interfaces = used to configure Samba to listen on multiple network interfaces.# If you have multiple interfaces, you can use the &quot;interfaces =&quot; option to# configure which of those interfaces Samba listens on. Never omit the localhost# interface (lo).## hosts allow = the hosts allowed to connect. This option can also be used on a# per-share basis.## hosts deny = the hosts not allowed to connect. This option can also be used on# a per-share basis.##定义计算机的工作组,如果希望和windows共享，可以设置为workgroup，这样就可以在windows的网上邻居中找到linux计算机 workgroup = MYGROUP#对samba服务器的描述信息 server string = Samba Server Version %v#设置netbios计算机名称; netbios name = MYSERVER#samba使用本机的那块网卡; interfaces = lo eth0 192.168.12.2/24 192.168.13.2/24#允许那个网段访问samba服务器共享; hosts allow = 127. 192.168.12. 192.168.13.##日志选项# --------------------------- Logging Options -----------------------------## log file = specify where log files are written to and how they are split.## max log size = specify the maximum size log files are allowed to reach. Log# files are rotated when they reach the size specified with &quot;max log size&quot;.# #samba日志文件路径 # log files split per-machine: log file = /var/log/samba/log.%m #日志文件大小，0为不限制，注意不建议这样设置 # maximum size of 50KB per log file, then rotate: max log size = 50#独立服务选项# ----------------------- Standalone Server Options ------------------------## security = the mode Samba runs in. This can be set to user, share# (deprecated), or server (deprecated).## passdb backend = the backend used to store user information in. New# installations should use either tdbsam or ldapsam. No additional configuration# is required for tdbsam. The &quot;smbpasswd&quot; utility is available for backwards# compatibility.##samba安全级别#share: 不需要账号密码，公开共享#user: 需要提供sam账号密码才能访问共享，私密共享#server：依靠其他Windows NT/2000或Samba Server来验证用户的账号和密码,是一种代理验证。此种安全模式下,系统管理员可以把所有的Windows用户和口令集中到一个NT系统上,&gt;使用Windows NT进行Samba认证, 远程服务器可以自动认证全部用户和口令,如果认证失败,Samba将使用用户级安全模式作为替代的方式。#domain：域安全级别,使用主域控制器(PDC)来完成认证。##一般情况下我们使用share和user的比较多，除非公司有完整的域环境 security = user#该方式则是使用一个数据库文件来建立用户数据库。数据库文件叫passdb.tdb，默认在/etc/samba目录下。passdb.tdb 用户数据库可以使用smbpasswd –a来建立Samba用户，不过要建立的Samba用户必须先是系统用户。我们也可以使用pdbedit命令来建立Samba账户并由其pdbedit管理。 passdb backend = tdbsam#域成员选项# ----------------------- Domain Members Options ------------------------## security = must be set to domain or ads.## passdb backend = the backend used to store user information in. New# installations should use either tdbsam or ldapsam. No additional configuration# is required for tdbsam. The &quot;smbpasswd&quot; utility is available for backwards# compatibility.## realm = only use the realm option when the &quot;security = ads&quot; option is set.# The realm option specifies the Active Directory realm the host is a part of.## password server = only use this option when the &quot;security = server&quot;# option is set, or if you cannot use DNS to locate a Domain Controller. The# argument list can include My_PDC_Name, [My_BDC_Name], and [My_Next_BDC_Name]:## password server = My_PDC_Name [My_BDC_Name] [My_Next_BDC_Name]## Use &quot;password server = *&quot; to automatically locate Domain Controllers.#设置域共享; security = domain; passdb backend = tdbsam#定义域名称; realm = MY_REALM#域验证服务器; password server = #域控选项# ----------------------- Domain Controller Options ------------------------## security = must be set to user for domain controllers.## passdb backend = the backend used to store user information in. New# installations should use either tdbsam or ldapsam. No additional configuration# is required for tdbsam. The &quot;smbpasswd&quot; utility is available for backwards# compatibility.## domain master = specifies Samba to be the Domain Master Browser, allowing# Samba to collate browse lists between subnets. Do not use the &quot;domain master&quot;# option if you already have a Windows NT domain controller performing this task.## domain logons = allows Samba to provide a network logon service for Windows# workstations.## logon script = specifies a script to run at login time on the client. These# scripts must be provided in a share named NETLOGON.## logon path = specifies (with a UNC path) where user profiles are stored.##; security = user; passdb backend = tdbsam; domain master = yes; domain logons = yes # the following login script name is determined by the machine name # (%m):; logon script = %m.bat # the following login script name is determined by the UNIX user used:; logon script = %u.bat; logon path = \\\\%L\\Profiles\\%u # use an empty path to disable profile support:; logon path = # various scripts can be used on a domain controller or a stand-alone # machine to add or delete corresponding UNIX accounts:; add user script = /usr/sbin/useradd &quot;%u&quot; -n -g users; add group script = /usr/sbin/groupadd &quot;%g&quot;; add machine script = /usr/sbin/useradd -n -c &quot;Workstation (%u)&quot; -M -d /nohome -s /bin/false &quot;%u&quot;; delete user script = /usr/sbin/userdel &quot;%u&quot;; delete user from group script = /usr/sbin/userdel &quot;%u&quot; &quot;%g&quot;; delete group script = /usr/sbin/groupdel &quot;%g&quot;#这些设置选项主要用于SMB网络中进行浏览时，设置samba服务器的行为。缺省情况不让 samba服务器参加broswser的推举过程，为了使得samba服务器能成为browser，就需要设定local master =yes。然后samba服务就可以根据os level设置的权重进行推举，缺省的os level为0，这个权重不会赢得推举。但可以取消注释，将os level设置为33，这将在与所有Windows计算机（包括Windows NT）的推举竞赛中获得胜利，因为NT server的权重为32。设置比33更高的权重，只是在不同的samba 服务器之间进行选择时才有意义。## preferred master 可以设置自己优先成为浏览服务器候选人## ----------------------- Browser Control Options ----------------------------## local master = when set to no, Samba does not become the master browser on# your network. When set to yes, normal election rules apply.## os level = determines the precedence the server has in master browser# elections. The default value should be reasonable.## preferred master = when set to yes, Samba forces a local browser election at# start up (and gives itself a slightly higher chance of winning the election).#; local master = no; os level = 33; preferred master = yes###wins服务，如果网络中配置了wins服务器可以在此设置wins相关项#----------------------------- Name Resolution -------------------------------## This section details the support for the Windows Internet Name Service (WINS).## Note: Samba can be either a WINS server or a WINS client, but not both.## wins support = when set to yes, the NMBD component of Samba enables its WINS# server.## wins server = tells the NMBD component of Samba to be a WINS client.## wins proxy = when set to yes, Samba answers name resolution queries on behalf# of a non WINS capable client. For this to work, there must be at least one# WINS server on the network. The default is no.## dns proxy = when set to yes, Samba attempts to resolve NetBIOS names via DNS# nslookups.#设置nmb进程支持wins服务; wins support = yes#设置wins服务器ip; wins server = w.x.y.z#设置wins代理IP; wins proxy = yes#设置Samba服务器是否在无法联系WINS服务器时通过DNS去解析主机的NetBIOS名; dns proxy = yes#该部分包括Samba服务器打印机相关设置# --------------------------- Printing Options -----------------------------## The options in this section allow you to configure a non-default printing# system.## load printers = when set you yes, the list of printers is automatically# loaded, rather than setting them up individually.## cups options = allows you to pass options to the CUPS library. Setting this# option to raw, for example, allows you to use drivers on your Windows clients.## printcap name = used to specify an alternative printcap file.##是否启用共享打印机 load printers = yes cups options = raw#打印机配置文件; printcap name = /etc/printcap # obtain a list of printers automatically on UNIX System V systems:; printcap name = lpstat#打印机的系统类型,现在支持的打印系统有：bsd, sysv, plp, lprng, aix, hpux, qnx,cups; printing = cups#该部分包括Samba服务器如何保留从Windows客户端复制或移动到Samba服务器共享目录文件的Windows文件属性的相关配置.# --------------------------- File System Options ---------------------------## The options in this section can be un-commented if the file system supports# extended attributes, and those attributes are enabled (usually via the# &quot;user_xattr&quot; mount option). These options allow the administrator to specify# that DOS attributes are stored in extended attributes and also make sure that# Samba does not change the permission bits.## Note: These options can be used on a per-share basis. Setting them globally# (in the [global] section) makes them the default for all shares.#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的存档属性。默认no。; map archive = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的隐藏属性。默认no。; map hidden = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的只读属性。默认为no。; map read only = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的系统文件属性。默认为no。; map system = no#当Windows客户端将文件复制或移动到Samba服务器共享目录时，是否保留文件在Windows中的相关属性（只读、系统、隐藏、存档属性）。默认为yes。; store dos attributes = yes#共享设置#============================ Share Definitions ==============================#用户家目录共享#共享名称[homes]#描述 comment = Home Directories#是否支持浏览 browseable = no#是否允许写入 writable = yes#允许访问该共享资源的smb用户，@组; valid users = %S; valid users = MYDOMAIN\\%S#打印机共享[printers]#描述 comment = All Printers#路径 path = /var/spool/samba#是否可浏览，no类似隐藏共享 browseable = no#是否支持guest访问，和public指令类似 guest ok = no#是否可写 writable = no#是否允许打印 printable = yes# Un-comment the following and create the netlogon directory for Domain Logons:; [netlogon]; comment = Network Logon Service; path = /var/lib/samba/netlogon; guest ok = yes; writable = no; share modes = no# Un-comment the following to provide a specific roaming profile share.# The default is to use the user's home directory:; [Profiles]; path = /var/lib/samba/profiles; browseable = no; guest ok = yes# A publicly accessible directory that is read only, except for users in the# &quot;staff&quot; group (which have write permissions):; [public]; comment = Public Stuff; path = /home/samba; public = yes; writable = no; printable = no#定义允许哪些smb用户写入; write list = +staff 4.3 Samba共享案例：在Windows上访问Samba服务器，共享目录为/common，指定用cqm1、cqm2用户才能访问，且只有cqm2有写权限。 创建Samba用户 12345678910# smbpasswd用户命令# -a 添加用户 smbpasswd -a cqm# -x 删除用户 smbpasswd -x cqm# -d 禁用帐号 smbpasswd -d cqm# -e 取消禁用 smbpasswd -e cqm# -n 清除密码 smbpasswd -a cqmuseradd -s /sbin/nologin cqm1useradd -s /sbin/nologin cqm2smbpasswd -a cqm1smbpasswd -a cqm2 创建共享目录 123mkdir /common# 设置757是为了让cqm2有写权限chmod 757 /common 修改主配文件 1234567891011[global] workgroup = WORKGROUP ...[common] comment = samba share directory path = /common browseable = YES hosts allow = 10.0.0.0/8,192.168.88.0/24 valid users = cqm1,cqm2 writable = No write list = cqm2 在Windows中输入 \\\\192.168.88.132 访问 首先是cqm1用户 可以看到cqm1用户没有写入权限 cqm2用户 可以看到cqm2用户有创建文件夹的权限 4.4 Linux挂载 在客户端上安装samba-client 1yum -y install samba-client 通过smbclient命令访问 1smbclient //192.168.88.132/common -U cqm2%toortoor 通过mount命令挂载 1234mkdir /commonmount -o username=cqm2,password=toortoor -t cifs //192.168.88.132/common /commonmount//192.168.88.132/common on /common type cifs (rw,relatime,vers=default,cache=strict,username=cqm2,domain=LOCALHOST,uid=0,noforceuid,gid=0,noforcegid,addr=192.168.88.132,file_mode=0755,dir_mode=0755,soft,nounix,serverino,mapposix,rsize=1048576,wsize=1048576,echo_interval=60,actimeo=1) 五、NFS文件服务5.1 NFS服务介绍NFS即网络文件系统，它允许网络中的计算机之间通过TCP/IP网络共享资源。在NFS的应用中，本地NFS的客户端应用可以透明地读写位于远端NFS服务器上的文件，就像访问本地文件一样。 NFS应用场景： 共享存储服务器：图片服务器、视频服务器 家目录漫游：域用户家目录服务器 文件服务器：文件存储服务器 5.2 NFS实现共享 安装NFS 123yum -y install nfs-utilssystemctl start rpcbindsystemctl start nfs /etc/exports共享文件 12345678910111213141516171819202122# 共享格式# 共享目录绝对路径 IP地址或网段地址(权限1,权限2)/test 192.168.88.132(rw,sync)# 权限说明ro 只读访问 rw 读写访问 sync 所有数据在请求时写入共享 async NFS在写入数据前可以相应请求 secure NFS通过1024以下的安全TCP/IP端口发送 insecure NFS通过1024以上的端口发送 wdelay 如果多个用户要写入NFS目录，则归组写入（默认） no_wdelay 如果多个用户要写入NFS目录，则立即写入，当使用async时，无需此设置。 hide 在NFS共享目录中不共享其子目录 no_hide 共享NFS目录的子目录 subtree_check 如果共享/usr/bin之类的子目录时，强制NFS检查父目录的权限（默认） no_subtree_check 和上面相对，不检查父目录权限 all_squash 共享文件的UID和GID映射匿名用户anonymous，适合公用目录。 no_all_squash 保留共享文件的UID和GID（默认） root_squash root用户的所有请求映射成如anonymous用户一样的权限（默认） no_root_squash root用户具有根目录的完全管理访问权限 anonuid=xxx 指定NFS服务器/etc/passwd文件中匿名用户的UID anongid=xxx 指定NFS服务器/etc/passwd文件中匿名用户的GID exportfs共享管理命令 12345678910exportfs命令：-a 打开或取消所有目录共享。-o options,... 指定一列共享选项，与 exports(5) 中讲到的类似。-i 忽略 /etc/exports 文件，从而只使用默认的和命令行指定的选项。-r 重新共享所有目录。它使/var/lib/nfs/xtab和/etc/exports同步。它将/etc/exports中已删除的条目从 /var/lib/nfs/xtab中删除，将内核共享表中任何不再有效的条目移除。-u 取消一个或多个目录的共享。-f 在“新”模式下，刷新内核共享表之外的任何东西。 任何活动的客户程序将在它们的下次请求中得到mountd 添加的新的共享条目。-v 输出详细信息。当共享或者取消共享时，显示在做什么。 显示当前共享列表的时候，同时显示共享的选项。 设置共享 12345678910# 创建被共享目录mkdir /test# 设置exportsvim /etc/exports/test 192.168.88.0/24(rw,sync)# 共享exportfs -r# 查看是否共享exportfs -vshowmount -e 192.168.88.132 客户端挂载 客户端挂载是使用nfsnobody用户进行的，如果是root创建的共享目录，且客户端挂载后要进行读写的话，得给目录757的权限。 1mount -t nfs 192.168.88.132:/test /test 六、iSCSI服务6.1 iSCSI介绍iSCSI即网络小型计算机系统接口，又被称为IPSAN。实际就是通过网络来共享设备。 数据存储技术： DSA（Direct Attached Storage 直接附加存储）：IDE SATA SAS SCSI（本地磁盘） NSA（Network Attached Storage 网络附加存储）：Samba NFS（共享文件夹） SAN（Storage Attached Network 网络附加存储）：iSCSI（共享设备） 6.2 iSCSI服务部署 准备好要被挂载的磁盘，这里共享sdb1 安装iSCSI 12yum -y install targetclisysetmctl start target 通过targetcli命令添加设备共享 123456789101112131415161718192021222324# 进入命令行targetclils...# backstores 代表后端存储,iscsi通过使用文件、逻辑卷或任何类型的磁盘作为底层存储来仿真呈现为目标的scsi设备# block 后端存储是个块设备# fileio 后端存储是一个文件# pscsi 物理scsi设备# ramdisk 后端存储是内存上的空间，在内存上创建一个指定大小的ramdisk设备可以通过help命令来打印可用命令# 将要共享的设备添加到backstores存储库中cd backstores/block/ creat block /dev/sdb1# 设置IQN标识# 格式：iqn.年-月.二级域名倒写:共享名cd ..iscsi/ create iqn.2021-04.com.cqm:storage# 设置TPG组中对应的三个问题 谁 从哪里 访问什么设备cd iscsi/iqn.2021-04.com.cqm:storage/tpg1/acls/ create iqn.2021-04.com.cqm:clientluns/ create /backstores/block/blockexit 6.3 iSCSI客户端挂载 安装iSCSI客户端 1yum -y install iscsi-initiator-utils 设置客户端名称 123vim /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2021-04.com.cqm:clientsystemctl start iscsi 发现共享设备 1iscsiadm --mode discoverydb --type sendtargets --portal 192.168.88.132:3260 --discover 连接远程设备 1234iscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.88.132:3260 --loginlsblk...sdb 分区格式化 1234fdisk /dev/sdb...mkfs.ext4 /dev/sdb1... 挂载共享磁盘 1234mkdir /root/sdb1vim /etc/fstab/dev/sdb1 /root/sdb1 ext4 _netdev 0 0mount -a 6.4 iSCSI取消挂载 客户端 123iscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.88.132:3260 --logoutrm -rf /var/lib/iscsi/nodes/iqn.2021-04.com.cqm\\:storage/rm -rf /var/lib/iscsi/send_targets/192.168.88.132,3260/ 服务端 1234567# 倒着删targetcliiscsi/iqn.2021-04.com.cqm:storage/tpg1/portals/ delete 0.0.0.0 3260iscsi/iqn.2021-04.com.cqm:storage/tpg1/luns/ delete lun=0iscsi/iqn.2021-04.com.cqm:storage/tpg1/acls/ delete iqn.2021-04.com.cqm:clientiscsi/ delete iqn.2021-04.com.cqm:storagebackstores/block/ delete block 七、IPSAN多链路部署在上边的环境中的共享设备是通过单链路共享的，如果这条链路出现了故障，那么就会出现连接不上共享设备的问题，所以在生产环境中都会配置多链路进行部署。 iSCSI服务端和客户端分别拥有两张不同网段的网卡，就可以配置多链路部署。 7.1 部署多链路 在服务端上设置共享设备，并用两个IP进行共享 在客户端发现共享设备 123456789vim /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.2021-04.com.cqm:clientsystemctl start iscsiiscsiadm --mode discoverydb --type sendtargets --portal 192.168.88.132:3260 --discover 192.168.88.132:3260,1 iqn.2021-04.com.cqm:storage 192.168.99.130:3260,1 iqn.2021-04.com.cqm:storageiscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.88.132:3260 --loginiscsiadm --mode node --targetname iqn.2021-04.com.cqm:storage --portal 192.168.99.130:3260 --login 这时候通过lsblk命令可以看到多了两块设备，但其实是同一个设备不同名 安装多路径软件 123yum -y install device-mapper-multipathcp /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf /etc/systemctl start multipathd 再用lsblk命令查看可发现 配置多路径运行模式 123456789101112131415161718192021222324252627282930multipath -ll# wwid:36001405ac25fe1abdfd4eecb1d0624b2mpatha (36001405ac25fe1abdfd4eecb1d0624b2) dm-2 LIO-ORG ,block...vim /etc/multipath.confmultipaths { multipath { wwid 36001405ac25fe1abdfd4eecb1d0624b2 # wwid alias cqm # 起名 path_grouping_policy multibus # 多路径组策略 path_selector &quot;round-robin 0&quot; # 负载均衡模式 failback manual rr_weight priorities # 按优先级轮询 no_path_retry 5 # 重试时间5s } multipath { wwid 1DEC_____321816758474 alias red }}systemctl restart multipathdsystemctl restart iscsimultipath -llcqm (36001405ac25fe1abdfd4eecb1d0624b2) dm-2 LIO-ORG ,blocksize=1023M features='1 queue_if_no_path' hwhandler='0' wp=rw`-+- policy='round-robin 0' prio=1 status=active |- 5:0:0:0 sdb 8:16 active ready running `- 6:0:0:0 sdc 8:32 active ready running 挂载 12mkdir /root/testmount /dev/mapper/cqm1 /root/test 7.2 测试将一块网卡断掉，看是否还能使用iSCSI设备 断开网卡ens33 1ifdown ens33 依旧可以在iSCSI设备上写入数据","link":"/2024/02/18/linux%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1/"},{"title":"MySQL","text":"MySQL 是一款关系型数据库管理系统。 一、MySQL基础1.1 安装MySQLdocker-compose 12345678910111213141516version: '3.8'services: db: image: mysql:5.7.35 command: --default-authentication-plugin=mysql_native_password restart: always ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: toortoor adminer: image: adminer:latest restart: always ports: - 8080:8080 1.2 基础SQL语句 刷新权限 1flush privileges; 数据库基本操作 123456-- 创建create database [if not exists] `db_name`;-- 删除drop database [if exists] `db_name`;-- 使用use `db_name`; 表基本操作 12345678910111213141516171819202122232425-- 创建create table [if not exists] `table_name`;-- 删除drop table [if exists] `table_name`;-- 查看describe `table_name`;-- 创建一个学生表create table if not exists `students`( -- 创建id列，数据类型为4位的int类型，不允许为空，自增 `id` int(4) not null auto_increment comment '学号', -- 创建name列，数据类型为30位的varchar类型，不允许为空，默认值为匿名 `name` varchar(30) not null default '匿名' comment '姓名', `pwd` varchar(20) not null default '123456' comment '密码', `gender` varchar(2) not null default '女' comment '性别', -- 创建birthday列，datetime类型，默认为空 `brithday` datetime default null comment '出生日期', `address` varchar(100) default null comment '家庭住址', `email` varchar(50) default null comment '邮箱', -- 设置主键，一般一个表只有一个主键 primary key(`id`))-- 设置引擎engine=innodb-- 默认编码default charset=utf8; 修改表 12345678910-- 修改表名称alter table `table_name` rename as new_`table_name`;-- 增加字段alter table `table_name` add age int(10);-- 修改约束alter table `table_name` modify age varchar(10);-- 重命名字段alter table `table_name` change age new_age int(10);-- 删除字段alter table `table_name` drop age; 查看创建语句 12show create database `db_name`;show create table `table_name`; 1.3 引擎INNODB和MYISAM区别 特性 INNODB MYISAM 事务 支持 不支持 数据行锁定 支持 不支持 外键 支持 不支持 全文索引 不支持 支持 空间占用 约为MYSIAM的2倍 较小 不同引擎在文件上的区别 innodb： *.frm：表结构定义文件 ibdata1：数据文件 mysiam： *.frm：表结构定义文件 *.MYD：数据文件 *.MYI：索引文件 二、MySQL数据操作2.1 外键外键就是一个表的某一列去引用另一个表的某一列 12345678910111213141516-- 该示例为students中的grade_id列引用grade中的grade_id列create table `grade`( `grade_id` int(10) not null auto_increment comment '年级' primary key(`grade_id`))engine=innodb default charset=utf8create table `students`( `id` int(4) not null auto_increment comment '学号', `name` varchar(30) not null default '匿名' comment '姓名', `grade_id` int(10) not null comment '年级', primary key(`id`), -- 定义外键 key `FK_grade_id` (`grade_id`), -- 给外键添加约束 constraint `FK_grade_id` foreign key (`grade_id`) references `grade` (`grade_id`))engine=innodb default charset=utf8 如果要给已经存在的表添加外键 1alter table `table_name` add constraint `FK_name` foreign key (`列名称`) references `引用的表名` (`引用的列名称`) 删除外键 1alter table `table_name` drop foreign key 'FK_name' 2.2 DML数据操纵语言2.2.1 insert1234insert into `table_name`(`字段1`,`字段2`,`字段3`) values('值1'),('值2'),('值3');insert into `students`(`name`) values('cqm'),('lwt');-- 字段可以省略，但后面的值必须一一对应insert into `students`(`name`,`pwd`,`email`) values('lwt','111','lwt@qq.com'); 2.2.2 update1234567-- 如果不指定条件，那么会修改所有的值update `table_name` set `字段`='值' where 条件;-- 条件可以是 =|&lt;|&gt;|!=|between|and|or 等等update `students` set `name`='handsome_cqm' where `id`=1;update `students` set `name`='cqm' where `name`='handsome_cqm'update `students` set `name`='newlwt',`email`='new@qq.com' where `name`='lwt';update `students` set `name`='cqm' where id between 1 and 2; 2.2.3 delete1234delete from `table_name` where 条件;delete from `students` where `id`=1;-- 清空表truncate `table_name`; delete 和 truncate 区别： 都可以清空表，都不会删除表结构 truncate 可以重置自增列，且不会影响事务 delete 清空表后，如果引擎是 innodb，重启数据库自增列就会变回1，因为是存储在内存中的；如果引擎是 myisam，则不会，因为是存储在文件中的 2.3 DQL数据库查询语言2.3.1 select 查询所有数据 1select * from `table_name`; 查询某列数据 1select `name`,`gander` from `students`; 给字段结果起别名 1select `name` as '姓名',`gender` as '性别' from `students`; concat 函数 1select concat('姓名：',`name`) from `students`; 去重 1select distinct `字段` from `table_name` 批量操作数据 1select `score`+1 as `new_score` from `table_name`; 2.3.2 where 逻辑运算符运用 123select `score` from `table_name` where `score` &gt;= 95 and `score` &lt;= 100;select `score` from `table_name` where `score` between 95 and 100;select `score` from `table_name` where not `score` != 95 and `score` != 100; 模糊查询 运算符 语法 描述 is null a is null a 为空，结果为真 is not null a is not null a 不为空，结果为真 between a between b and c 若 a 在 b 和 c 之间，结果为真 like a like b 如果 a 匹配 b，结果为真 in a in ( b,c,d,e ) 如果 a 在某个集合中的值相同，结果为真 查找花名册中姓陈的名字 1234-- %:匹配一个或多个字符-- _:匹配一个字符select `name` from `table_name` where `name` like '陈%';select `name` from `table_name` where `name` like '陈_明'; 查找特定学号的信息 1select `name` from `table_name` where `id` in (1,2,3); 查找地址为空或不空的同学 12select `name` from `table_name` where `address` is null;select `name` from `table_name` where `address` is not null; 2.3.3 联表查询联表查询包括： inner（内连接）：如果表中有至少一个匹配，则返回行 left（外连接）：即使右表中没有匹配，也从左表返回所有的行 right（外连接）：即使左表中没有匹配，也从右表返回所有的行 full（外连接）：只要其中一个表中存在匹配，则返回行 inner 实际就是取两个表的交集，例如有两个表，一个表有学生的基本信息，另一个表有学生的成绩，两个表都有学生的学号列，那么就可以联合起来查询 1select `name`,`subjectno`,`score` from `students` inner join `result` where student.id = result.id left 假设学生表中有 ccc 这么一个学生，但成绩表里没有 ccc 学生的成绩，如果使用了左连接，左表是学生表，右表是成绩表，那么也会返回 cqm 学生的值，显示为空 right 相反，如果成绩表里有个 ddd 学生的成绩，但学生表里没有这个学生的信息，如果使用了右连接，左表是学生表，右表是成绩表，那么也会返回 ddd 学生的成绩，注意这时候就看不到 ccc 学生的成绩信息了，因为左表中没有 ccc 学生的成绩信息 通过联表查询就可以查出缺考的同学 查询参加考试了的同学信息 1select distinct `name` from `students` right join `result` on students.id = result.id where `score` is not null; 查询参加了考试的同学以及所对应的学科成绩 2.3.4 where和on的区别在进行联表查询的时候，数据库都会在中间生成一张临时表，在使用外连接时，on 和 where 区别如下： on 条件是在生成临时表时使用的条件，它不管 on 中的条件是否为真，都会返回左边表中的记录。 where 条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有 left join 的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。 如果是 inner join，则无区别 2.3.5 自连接自连接实际上就是一张表与自己连接，将一张表拆为两张表 原表 categoryid pid categoryname 2 1 计算机科学与技术学院 3 1 心理学院 4 1 体育学院 5 2 python 6 3 心理学 7 4 短跑 8 4 篮球 父类表 category categoryname 2 计算机科学与技术学院 3 心理学院 4 体育学院 子类表 category 所属父类 categoryname 5 计算机科学与技术学院 python 6 心理学院 心理学 7 体育学院 短跑 8 体育学院 篮球 自查询结果 2.3.6 分页和排序排序 order by desc：降序 asc：升序 12-- 语法order by desc|asc 查询语文成绩并排序 分页 limit 12-- 语法limit 起始值,页面显示多少条数据 打印第一页语文成绩 打印第二页数学成绩 分页和排序配合起来就可以查询学生的前几名成绩，例如语文成绩前三名 2.3.7 子查询本质上就是在判断语句里嵌套一个查询语句，例如要查询所有语文成绩并降序排序，可以通过联表查询和子查询两种方式实现 2.4 聚合函数2.4.1 countcount 函数用于计数，例如查询有多少学生 1select count(`name`) from students; 函数内所带的参数不同，背后运行也不同： count(字段)：会忽略空值，不计数 count(*)：不会忽略空值 count(1)：不会忽略空值 从效率上看：如果查询的列为主键，那么 count(字段) 比 count(1) 快，不为主键则反之；如果表中只有一列，则 count(*) 最快 2.4.2 sum顾名思义，用于求和，例如查询所有成绩之和 1select sum(`score`) as '总分' from result 2.4.3 avg1select avg(`score`) as '平均分' from result 2.4.4 max和min1select max(`score`) as '最高分' from result 1select min(`score`) as '最低分' from result 查询所有科目的平均分、最高分和最低分 123456select subjectname, avg(studentresult), max(studentresult), min(studentresult)from `result`inner join `subject`on `result`.subjectno = `subject`.subjectno-- 定义字段进行分组group by result.subjectno; 通过分组后的次要条件，查询平均分大于 80 分的科目 1234567select subjectname, avg(studentresult) as 平均分from `result`inner join `subject`on `result`.subjectno = `subject`.subjectnogroup by `result`.subjectno-- 分组后的次要条件having 平均分 &gt; 80; 2.5 MD5加密在数据库中，密码等敏感信息都不会以明文的形式存储的，可以通过md5 进行加密 12-- 更新密码以达成加密update students set pwd=md5(pwd); 12-- 插入的时候就进行加密insert into students values(1, 'cqm', md5('12345')) 2.6 事务事务就是一系列 SQL 语句，要么全部执行，要么全部不执行。 事务的特性（ACID）： 原子性（Atomicity）：所有操作要么全部成功，要么全部失败 一致性（Consistency）：事务的执行的前后数据的完整性保持一致 隔离性（Isolation）：一个事务执行的过程中，不受到别的事务的干扰 持久性（Durability）：事务一旦结束，就会持久到数据库 隔离所导致的一些问题： 脏读：一个事务读取到了另一个事务没提交的数据 不可重复读：一个事务的多次查询结果不同，是因为查询期间数据被另一个事务提交而修改了 虚读：一个事务A在进行修改数据的操作时，另一个事务B插入了新的一行数据，而对于事务A来看事务B添加的那行就没有做修改，就发生了虚读 事务关闭自动提交 1set autocommit = 0; 事务开启 1start transaction; 事务提交 1commit; 事务回滚 1rollback; 事务结束 1set autocommit = 1; 保存点 123456-- 添加保存点savepoint 保存点名称-- 回滚到保存点rollback to savepoint 保存点名称-- 删除保存点release savepoint 保存点名称 2.7 索引索引是帮助 MySQL 高效获取数据的数据结构。 索引的分类： 主键索引（primary key）：只有一列可以作为主键，且主键的值不可重复，一般用于用户ID之类的 唯一索引（unique key）：唯一索引可以有多个，且值唯一 常规索引（key）：例如一个表中的数据经常用到，就可以添加个常规索引 全文索引（fulltext）：快速定位数据 查看某个表的所有索引 1show index from `table_name`; 添加全文索引 1alter table `table_name` add fulltext index `index_name`(`要添加索引的字段名`); 添加常规索引 12-- 这样会给表中某个字段的数据全部都添加上索引，在数据量大的时候可以提高查询效率create index `id_table_name_字段名` on `table_name`('字段名'); 索引使用原则： 并不是索引越多越好 不要对进程变动的数据添加索引 数据量小的表不需要索引 索引一般用于常查询的字段上 2.8 权限管理在 MySQL 中，用户表为 mysql.user，而权限管理其实都是在该表上操作。 创建用户 123456-- host可以为以下的值-- %:允许所有ip连接-- localhost:只允许本地连接-- 192.168.88.%:只允许给网段连接-- 192.168.88.10:只允许该ip连接create user 'user_name'@'host' identified by 'user_password'; 修改当前用户密码 1set password = password('new_password'); 修改指定用户密码 1set password for user_name = password('new_password'); 重命名 1rename user user_name to new_user_name; 授权 12-- 权限:select、insert、delete等等，所有权限则为allgrant 权限 on `db_name`.`table_name` to 'user_name'@'host'; 授予某个用户部分权限 1grant select,insert on mysql.user to 'cqm'@'%'; 给某个用户授予全部数据库的权限 12-- 基本权限都有，但不会给grant权限grant all privileges on *.* to 'cqm'@'%'; 删除用户 1drop user 'user_name'@'host'; 取消权限 1revoke 权限 on `db_name`.`table_name` from 'user_name'@'host'; 查询用户权限 1show grants for 'user_name'@'host'; 2.9 备份备份文件都是以 .sql 为后缀的文件。 导出 12345# -d:要操作的数据库# -h:指定主机# -P:指定端口# --all-databases:操作所有数据库mysqldump -uroot -ppassword -d db1_name db2_name &gt; db_backup.sql 导入 1mysqldump -uroot -ppassword -d db_name &lt; db_backup.sql 三、MySQL配置3.1 主从同步/复制MySQL 主从同步即每当主数据库进行了数据的操作后，就会将操作写入 binlog 文件，从数据库会启动一个 IO 线程去监控主数据库的 binlog 文件，并将 binlog 文件的内容写入自己的 relaylog 文件中，同时会启动一个 SQL 线程去监控 relaylog 文件，如果发生变化就更新数据。 主从复制的类型： statement模式（sbr）：只有修改数据的 SQL 语句会记录到 binlog 中，优点是减少了日志量，节省 IO，提高性能，不足是可能会导致主从节点之间的数据有差异。 row模式（rbr）：仅记录被修改的数据，不怕无法正确复制的问题，但会产生大量的 binlog。 mixed模式（mbr）：sbr 和 rbr 的混合模式，一般复制用 sbr，sbr 无法复制的用 rbr。 主从同步实现 docker-compose.yaml 1234567891011121314151617181920212223version: '3.8'services: mysql_master: image: mysql:5.7.35 command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: toortoor ports: - 3306:3306 volumes: - ./master/my.cnf:/etc/mysql/my.cnf mysql_slave: image: mysql:5.7.35 command: --default-authentication-plugin=mysql_native_password restart: always environment: MYSQL_ROOT_PASSWORD: toortoor ports: - 3307:3306 volumes: - ./slave/my.cnf:/etc/mysql/my.cnf master/my.cnf 12345678910111213[mysqld]server-id = 1 #节点ID，确保唯一log-bin = mysql-bin #开启mysql的binlog日志功能sync_binlog = 1 #控制数据库的binlog刷到磁盘上去，0不控制，性能最好；1每次事物提交都会刷到日志文件中，性能最差，最安全binlog_format = mixed #binlog日志格式，mysql默认采用statement，建议使用mixedexpire_logs_days = 7 #binlog过期清理时间max_binlog_size = 100m #binlog每个日志文件大小binlog_cache_size = 4m #binlog缓存大小max_binlog_cache_size= 512m #最大binlog缓存大binlog-ignore-db=mysql #不生成日志文件的数据库，多个忽略数据库可以用逗号拼接，或者复制这句话，写多行auto-increment-offset = 1 #自增值的偏移量auto-increment-increment = 1 #自增值的自增量slave-skip-errors = all #跳过从库错误 slave/my.cnf 1234567[mysqld]server-id = 2log-bin=mysql-binrelay-log = mysql-relay-binreplicate-wild-ignore-table=mysql.%replicate-wild-ignore-table=test.%replicate-wild-ignore-table=information_schema.% 在 master 创建复制用户并授权 123create user 'repl_user'@'%' identified by 'toortoor';grant replication slave on *.* to 'repl_user'@'%' identified by 'toortoor';flush privileges; 查看 master 状态 123456show master status;+------------------+----------+...| File | Position |...+------------------+----------+...| mysql-bin.000003 | 844 |...+------------------+----------+... 在从数据库配置 123456789change master toMASTER_HOST = '172.19.0.3',MASTER_USER = 'repl_user', MASTER_PASSWORD = 'toortoor',MASTER_PORT = 3306,MASTER_LOG_FILE='mysql-bin.000003',MASTER_LOG_POS=844,MASTER_RETRY_COUNT = 60,MASTER_HEARTBEAT_PERIOD = 10000; 启动从配置 12start slave;show slave status\\G; 如果配置失败，可以执行 12stop slave;set global sql_slave_skip_counter=1; 3.2 Mycat读写分离在一般项目中，对于数据库的操作读要远大于写，而如果所有的操作都放在一个节点上，那么就很容易出现压力过大而宕机，读写分离就可以很好解决该问题，主要通过 mycat 的中间件来实现。 分库分表类型： 水平拆分：将不同等级的会员信息写到不同的表中 垂直拆分：将买家信息、卖家信息、商品信息、支付信息等不同信息写到不同的表中 Mycat的主要文件： 文件 说明 server.xml 设置 Mycat 账号、参数等 schema.xml 设置 Mycat 对应的物理数据库和表等 rule.xml 分库分表设置 server.xml 123456789101112131415161718192021222324252627282930&lt;!-- Mycat用户名 --&gt;&lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;!-- 密码 --&gt; &lt;property name=&quot;password&quot;&gt;123456&lt;/property&gt; &lt;!-- 逻辑库名 --&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;!-- 默认逻辑库 --&gt; &lt;property name=&quot;defaultSchema&quot;&gt;TESTDB&lt;/property&gt; &lt;!--No MyCAT Database selected 错误前会尝试使用该schema作为schema，不设置则为null,报错 --&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- 0为禁止，1为开启 --&gt; &lt;!-- 按顺序分别为insert、update、select、delete --&gt; &lt;!-- &lt;privileges check=&quot;false&quot;&gt; &lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt; &lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt; &lt;/user&gt; &lt;!-- 其他用户设置 --&gt; &lt;user name=&quot;user&quot;&gt; &lt;property name=&quot;password&quot;&gt;user&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt; &lt;property name=&quot;defaultSchema&quot;&gt;TESTDB&lt;/property&gt; &lt;/user&gt; schema.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;!-- name为逻辑库名称，与server.xml文件对应 checkSQLschema为true，sql为select * from table_name checkSQLschema为false，sql为select * from TESTDB.table_name sqlMaxLimit是指如果sql中没有limit，则自动添加，如果有则不添加 --&gt; &lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot; randomDataNode=&quot;dn1&quot;&gt; &lt;!-- name为逻辑表名 dataNode为数据节点名称 rule为规则 --&gt; &lt;table name=&quot;customer&quot; primaryKey=&quot;id&quot; dataNode=&quot;dn1,dn2&quot; rule=&quot;sharding-by-intfile&quot; autoIncrement=&quot;true&quot; fetchStoreNodeByJdbc=&quot;true&quot;&gt; &lt;childTable name=&quot;customer_addr&quot; primaryKey=&quot;id&quot; joinKey=&quot;customer_id&quot; parentKey=&quot;id&quot;&gt; &lt;/childTable&gt; &lt;/table&gt; &lt;/schema&gt; &lt;!-- name为数据节点名称 dataHost为数据库地址 database为mysql中的database 实际就是将TESTDB逻辑库拆成dn1和dn2，而dn1和dn2又对应db1和db2 --&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;db1&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;localhost1&quot; database=&quot;db2&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;localhost1&quot; database=&quot;db3&quot; /&gt; &lt;!-- balance: 0:不开启读写分离，所有操作都在writeHost上 1:所有读操作都随机发送到当前的writeHost对应的readHost和备用的writeHost 2:所有读操作都随机发送到所有的主机上 3:所有读操作只发送到readHost上 writeType: 0:所有写操作都在第一台writeHost上，第一台挂了再切到第二台 1:所有写操作都随机分配到writeHost switchType: 用于是否允许writeHost和readHost之间自动切换 -1:不允许 1:允许 2:基于mysql的主从同步的状态决定是否切换 --&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;jdbc&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;!-- 用此命令来进行心跳检测 --&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- 设置读写分离 --&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;jdbc:mysql://localhost:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;hostS1&quot; url=&quot;jdbc:mysql://localhost:3306&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/readHost&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; rule.xml 12345678910111213&lt;!-- 平均分算法 --&gt;&lt;tableRule name=&quot;mod-long&quot;&gt; &lt;rule&gt; &lt;!-- 根据id值平均分 --&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;mod-long&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt;...&lt;function name=&quot;mod-long&quot; class=&quot;io.mycat.route.function.PartitionByMod&quot;&gt; &lt;!-- 切片个数 --&gt; &lt;property name=&quot;count&quot;&gt;2&lt;/property&gt;&lt;/function&gt; 在 master 节点创建数据库 123-- 与schema.xml中的database参数相同create database db1;create database db2; 在每个库里创建表 12345create table students( id int(4), name varchar(10), primary key(`id`))engine=innodb default charset=utf8; 开启 Mycat，默认开启8066服务端端口和9066管理端端口 1./mycat start 在有安装 mysql 的主机登录 Mycat，也可以通过 navicat 连接 1mysql -uroot -ptoortoor -h 192.168.88.136 -P 8066 只要在 Mycat 进行 SQL 操作，都会流到 mysql 集群中被处理，也可以看到已经实现了分库分表 3.3 使用haproxy实现Mycat高可用haproxy 可以实现 Mycat 集群的高可用和负载均衡，而 haproxy 的高可用通过 keepalived 来实现。 安装 haproxy 1yum -y install haproxy 修改日志文件 12345vim /etc/rsyslog.conf# Provides UDP syslog reception$ModLoad imudp$UDPServerRun 514systemctl restart rsyslog 配置 haproxy 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263vim /etc/haproxy/haproxy.cfg# haproxy的配置文件由两个部分构成，全局设定和代理设定# 分为五段:global、defaults、frontend、backend、listenglobal # 定义全局的syslog服务器 log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy # 设置haproxy后台守护进程形式运行 daemon stats socket /var/lib/haproxy/statsdefaults # 处理模式 # http:七层 # tcp:四层 # health:状态检查,只会返回OK mode tcp # 继承global中log的定义 log global option tcplog option dontlognull option http-server-close # option forwardfor except 127.0.0.0/8 # serverId对应的服务器挂掉后,强制定向到其他健康的服务器 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000frontend mycat # 开启本地监控端口 bind 0.0.0.0:8066 bind 0.0.0.0:9066 mode tcp log global default_backend mycatbackend mycat balance roundrobin # 监控的Mycat server mycat1 192.168.88.135:8066 check inter 5s rise 2 fall 3 server mycat2 192.168.88.135:8066 check inter 5s rise 2 fall 3 server mycatadmin1 192.168.88.136:9066 check inter 5s rise 2 fall 3 server mycatadmin2 192.168.88.136:9066 check inter 5s rise 2 fall 3 listen stats mode http # 访问haproxy的端口 bind 0.0.0.0:9999 stats enable stats hide-version # url路径 stats uri /haproxy stats realm Haproxy\\ Statistics # 用户名/密码 stats auth admin:admin stats admin if TRUE 访问 haproxy 3.4 使用keepalived实现去中心化 安装 keepalived 1yum -y install keepalived 配置 Master 节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs { # 识别节点的id router_id haproxy01}vrrp_instance VI_1 { # 设置角色，由于抢占容易出现 VIP 切换而闪断带来的风险，所以该配置为不抢占模式 state BACKUP # VIP所绑定的网卡 interface ens33 # 虚拟路由ID号，两个节点必须一样 virtual_router_id 30 # 权重 priority 100 # 开启不抢占 # nopreempt # 组播信息发送间隔，两个节点必须一样 advert_int 1 # 设置验证信息 authentication { auth_type PASS auth_pass 1111 } # VIP地址池，可以多个 virtual_ipaddress { 192.168.88.200 } # 将 track_script 块加入 instance 配置块 track_script{ chk_haproxy }}# 定义监控脚本vrrp_script chk_haproxy { script &quot;/etc/keepalived/haproxy_check.sh&quot; # 时间间隔 interval 2 # 条件成立权重+2 weight 2} 配置 Slave 节点 12345678910111213141516171819202122232425262728293031vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs { router_id haproxy02}vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 30 priority 80 # nopreempt advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.88.200 } track_script{ chk_haproxy }}vrrp_script chk_haproxy { script &quot;/etc/keepalived/haproxy_check.sh&quot; interval 2 weight 2} 编写监控脚本 1234567891011121314151617181920212223242526vim /etc/keepalived/haproxy_check.sh#!/bin/bashSTART_HAPROXY=&quot;systemctl start haproxy&quot;STOP_KEEPALIVED=&quot;systemctl stop keepalived&quot;LOG_DIR=&quot;/etc/keepalived/haproxy_check.log&quot;HAPS=`ps -C haproxy --no-header | wc -l`date &quot;+%F %H:%M:%S&quot; &gt; $LOG_DIRecho &quot;Check haproxy status&quot; &gt;&gt; $LOG_DIRif [ $HAPS -eq 0 ];then echo &quot;Haproxy is down&quot; &gt;&gt; $LOG_DIR echo &quot;Try to turn on Haproxy...&quot; &gt;&gt; $LOG_DIR echo $START_HAPROXY | sh sleep 3 if [ `ps -C haproxy --no-header | wc -l` -eq 0 ]; then echo -e &quot;Start Haproxy failed, killall keepalived\\n&quot; &gt;&gt; $LOG_DIR echo $STOP_KEEPALIVED | sh else echo -e &quot;Start Haproxy successed\\n&quot; &gt;&gt; $LOG_DIR fielse echo -e &quot;Haproxy is running\\n&quot; &gt;&gt; $LOG_DIRfi 启动 keepalived 后可以看到 VIP 被哪台服务器抢占了，通过该 VIP 就可以访问到对应的 haproxy，haproxy 就会将流量流到后面的 Mycat，再由 Mycat 来实现分表分库；haproxy 停止后 keepalived 也会通过脚本尝试去重新开启，如果开启不成功就会停止 keepalived，VIP 就由 slave 节点抢占，用户依旧可以通过 VIP 来操控数据库，且无感知。 通过 VIP 去连接 Mycat 插入数据，尝试能否实现分库分表 可以看到插入的数据都分到了 db1、db2 中 3.5 Sharding JDBC读写分离Apache ShardingSphere 是一套开源的分布式数据库解决方案组成的生态圈，它由 JDBC、Proxy 和 Sidecar（规划中）这 3 款既能够独立部署，又支持混合部署配合使用的产品组成。 Sharding JDBC 同样也可以实现分库分表、数据分片、读写分离等功能，但与 Mycat 不同的是，Mycat 是一个独立的程序，而 Sharding JDBC 是以 jar 包的形式与应用程序融合在一起运行的。","link":"/2024/02/18/mysql/"},{"title":"Nexus","text":"Nexus 是一个用于专门搭建 Maven 仓库的软件，除了作为 Maven 仓库，它还能够作为 Docker 镜像仓库、Yum 仓库等等。 Nexus 部署deploy.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869apiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: app: nexus name: nexusspec: replicas: 1 selector: matchLabels: app: nexus strategy: {} template: metadata: creationTimestamp: null labels: app: nexus spec: initContainers: - name: volume-mount-hack image: busybox:latest command: - sh - '-c' - 'chown -R 200:200 /nexus-data' volumeMounts: - name: nexus-data mountPath: /nexus-data containers: - image: sonatype/nexus3:3.37.3 name: nexus ports: - containerPort: 8081 env: - name: INSTALL4J_ADD_VM_PARAMS value: &quot;-Xms2703m -Xmx2703m -XX:MaxDirectMemorySize=2703m -Djava.util.prefs.userRoot=${NEXUS_DATA}/javaprefs&quot; resources: limits: cpu: 2000m memory: 2048Mi requests: cpu: 2000m memory: 2048Mi volumeMounts: - name: nexus-data mountPath: /nexus-data volumes: - name: nexus-data # 自行修改挂载类型，不修改则需创建对应PV和PVC👇 persistentVolumeClaim: claimName: nexus-data ---apiVersion: v1kind: Servicemetadata: name: nexus labels: app: nexusspec: type: NodePort ports: - name: nexus port: 8081 targetPort: 8081 protocol: TCP selector: app: nexus 部署后，在容器内部获取 admin 密码 1echo $(cat /nexus-data/admin.password) 默认仓库说明 maven-central：中央仓库，默认从 https://repo1.maven.org/maven2/ 拉取 jar 包 maven-releases：私库发行 jar，建议将设置改为 Allow redeploy maven-snapshots：私库快照 jar，即库中的 jar 均为调试版本 maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地 maven 的 settings.xml 文件或项目的 pom.xml 文件设置为该仓库地址，即可调用 仓库类型说明 group：仓库组，起到了聚合的作用，在该组中的仓库都可以通过该组的 URL 进行访问 hosted：私有仓库，顾名思义，用来存储自己的 jar 包 snapshot：快照仓库 release：本地项目的正式版本仓库 proxy：代理，Nexus 的 maven-central 就是这种类型，代理地址为 https://repo1.maven.org/maven2/ ，默认会去该地址下拉取 jar 包 central：中央仓库 新增代理仓库创建仓库选择 maven2(proxy) 类型 添加到 maven-public Maven 配置使用私服要在本地 Maven 在私服拉取 jar 的方式有两种： settings.xml：全局配置模式 pom.xml：项目独享模式 如果两种方式都配置了，那么以 pom.xml 文件配置为准。 当我们通过 Maven 使用 Nexus 的 maven-public 的时候，会按照以下方式顺序访问： 本地仓库 私服 maven-releases 私服 maven-snapshots 远程阿里 maven 仓库 远程中央仓库 通过 settings.xml 文件配置1234567891011121314151617&lt;!-- servers块中添加用户认证信息 --&gt;&lt;server&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;changeme&lt;/password&gt;&lt;/server&gt;&lt;!-- mirrors块中添加maven-public信息 --&gt;&lt;mirror&gt; &lt;!-- 唯一标识符 --&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;!-- 名称 --&gt; &lt;name&gt;cqm maven&lt;/name&gt; &lt;!-- maven-public地址 --&gt; &lt;url&gt;http://192.168.159.11:35826/repository/maven-public/&lt;/url&gt; &lt;!-- *指的是访问任何仓库都使用我们的私服，可设置为central等等 --&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;&lt;/mirror&gt; 也可以设置为阿里的 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt; 通过 pom.xml 文件配置12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;mnexus&lt;/id&gt; &lt;name&gt;cqm nexus&lt;/name&gt; &lt;url&gt;http://192.168.159.11:35826/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 同样也可以设置为阿里的 1234567891011121314&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 使用 Maven 批量向 Nexus 上传首先需要将 .m2/repository 下的相应 jar 包和 pom 文件 cp 出来，再进行 deploy。 12# -DrepositoryId 参数调用了 settings.xml 中 servers 块的账号密码进行认证find . -name &quot;*.jar&quot; | awk '{ gsub(&quot;\\.jar$&quot;,&quot;&quot;,$0); print &quot;mvn deploy:deploy-file -Dfile=&quot;$0&quot;.jar -DpomFile=&quot;$0&quot;.pom -Dpackaging=jar -DrepositoryId=nexus -Durl=\\&quot;http://nexus-path\\&quot;&quot;}'","link":"/2024/02/18/nexus/"},{"title":"Prometheus","text":"Prometheus是一个开源的云原生监控系统和时间序列数据库。 一、Prometheus概述Prometheus 作为新一代的云原生监控系统，目前已经有超过 650+位贡献者参与到 Prometheus 的研发工作上，并且超过 120+项的第三方集成。 1.1 Prometheus的优点 提供多维度数据模型和灵活的查询方式，通过将监控指标关联多个 tag，来将监控数据进行任意维度的组合，并且提供简单的 PromQL 查询方式，还提供 HTTP 查询接口，可以很方便地结合 Grafana 等 GUI 组件展示数据。 在不依赖外部存储的情况下，支持服务器节点的本地存储，通过 Prometheus 自带的时序数据库，可以完成每秒千万级的数据存储；不仅如此，在保存大量历史数据的场景中，Prometheus 可以对接第三方时序数据库和 OpenTSDB 等。 定义了开放指标数据标准，以基于 HTTP 的 Pull 方式采集时序数据，只有实现了 Prometheus 监控数据才可以被 Prometheus 采集、汇总、并支持 Push 方式向中间网关推送时序列数据，能更加灵活地应对多种监控场景。 支持通过静态文件配置和动态发现机制发现监控对象，自动完成数据采集。 Prometheus 目前已经支持 Kubernetes、etcd、Consul 等多种服务发现机制。易于维护，可以通过二进制文件直接启动，并且提供了容器化部署镜像。 支持数据的分区采样和联邦部署，支持大规模集群监控。 1.2 Prometheus基本组件 Prometheus Server：是 Prometheus 组件中的核心部分，负责实现对监控数据的获取，存储以及查询。收集到的数据统称为metrics。 Push Gateway：当网络需求无法直接满足时，就可以利用 Push Gateway 来进行中转。可以通过 Push Gateway 将内部网络的监控数据主动 Push 到 Gateway 当中。而 Prometheus Server 则可以采用同样 Pull 的方式从 Push Gateway 中获取到监控数据。 Exporter：主要用来采集数据，并通过 HTTP 服务的形式暴露给 Prometheus Server，Prometheus Server 通过访问该 Exporter 提供的接口，即可获取到需要采集的监控数据。 Alert manager：管理告警，主要是负责实现报警功能。现在grafana也能实现报警功能，所以也慢慢被取代。 1.3 Prometheus数据类型 Counter（计数器类型）：Counter类型的指标的工作方式和计数器一样，只增不减（除非系统发生了重置）。 Gauge（仪表盘类型）：Gauge是可增可减的指标类，可以用于反应当前应用的状态。 Histogram（直方图类型）：主要用于表示一段时间范围内对数据进行采样（通常是请求持续时间或响应大小），并能够对其指定区间以及总数进行统计，通常它采集的数据展示为直方图。 Summary（摘要类型）：主要用于表示一段时间内数据采样结果（通常是请求持续时间或响应大小）。 二、Prometheus安装2.1 Prometheus server安装 Prometheus安装较为简单，下载解压即可 123wget https://github.com/prometheus/prometheus/releases/download/v2.26.0-rc.0/prometheus-2.26.0-rc.0.linux-amd64.tar.gztar -xf prometheus-2.26.0-rc.0.linux-amd64.tar.gzmv prometheus-2.26.0-rc.0.linux-amd64 prometheus prometheus.yml配置文件 12345678910111213141516171819202122232425# 全局配置global: scrape_interval: 15s # 设置抓取间隔，默认为1分钟 evaluation_interval: 15s # 估算规则的默认周期，每15秒计算一次规则，默认1分钟 # scrape_timeout # 默认抓取超时，默认为10s# 报警配置alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093# 规则文件列表，使用'evaluation_interval' 参数去抓取rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot;# 抓取配置列表scrape_configs: # 任务名称 - job_name: 'prometheus' # 要被监控的客户端 static_configs: - targets: ['localhost:9090'] 创建Prometheus的用户及数据存储目录 1234groupadd prometheususeradd -g prometheus -s /sbin/nologin prometheusmkdir /root/prometheus/datachown -R prometheus:prometheus /root/prometheus Prometheus的启动很简单，只需要直接启动解压目录的二进制文件Prometheus即可，但是为了更加方便对Prometheus进行管理，这里编写脚本或者使用screen工具来进行启动 123456789vim /root/prometheus/start.sh#!/bin/bashprometheus_dir=/root/prometheus${prometheus_dir}/prometheus --config.file=${prometheus_dir}/prometheus.yml --storage.tsdb.path=${prometheus_dir}/data --storage.tsdb.retention.time=24h --web.enable-lifecycle --storage.tsdb.no-lockfile# --config.file:指定配置文件路径# --storage.tsdb.path:指定tsdb路径# --storage.tsdb.retention.time:指定数据存储时间# --web.enable-lifecycle:类似nginx的reload功能# --storage.tsdb.no-lockfile:如果用k8s的deployment管理需加此项 启动Prometheus后访问 12# nohup英文全称no hang up（不挂起），用于在系统后台不挂断地运行命令，退出终端不会影响程序的运行。nohup sh start.sh 2&gt;&amp;1 &gt; prometheus.log 用screen工具进行启动 123456789101112yum -y install screen# 进入后台screen# 运行脚本/root/prometheus/prometheus --config.file=/root/prometheus/prometheus.yml --storage.tsdb.path=/root/prometheus/data --storage.tsdb.retention.time=24h --web.enable-lifecycle --storage.tsdb.no-lockfile# 输入CTRL + A + D撤回前台# 查看后台运行的脚本screen -ls# 返回后台screen -r 后台id# 删除后台screen -S 后台id -X quit 2.2 node exporter安装在Prometheus架构中，exporter是负责收集数据并将信息汇报给Prometheus Server的组件。官方提供了node_exporter内置了对主机系统的基础监控。 下载node exporter 123wget https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gztar -xf node_exporter-1.1.2.linux-amd64.tar.gzmv node_exporter-1.1.2.linux-amd64.tar.gz node_exporter 在prometheus.yml中添加被监控主机 12static_configs: - targets: ['localhost:9090','localhost:9100'] 后台启动exporter和重启prometheus 12screen./root/node_exporter/node_exporter 通过curl命令获取收集到的数据key 12curl http://localhost:9100/metrics... 用其中的一个key在Prometheus测试是否被监控 三、Prometheus命令行的使用3.1 计算cpu使用率 通过上图可以知道，linux的cpu使用是分为很多种状态的，例如用户态user，空闲态idle。 要计算cpu的使用率有两种粗略的公式： 除去idle状态的所有cpu状态时间之和 / cpu时间总和 100% - （idle状态 / cpu时间总和） 但这两种方式都存在两个问题： 如何计算某一时间段的cpu使用率？例如精确到每一分钟。 实际工作中cpu大多数都是多核的，node exporter截取到的数据精确到了每个核，如何监控所有核加起来的数据？ Prometheus提供了许多的函数，其中 increase 和 sum 就很好的解决了以上两个问题。 提取cpu的key，即node_cpu_seconds_total 把idle空闲时间和总时间过滤出来，在Prometheus中使用{}进行过滤 1node_cpu_seconds_total{mode='idle'} 使用increase函数取一分钟内的增量 1increase(node_cpu_seconds_total{mode='idle'}[1m]) 使用sum函数将每个核的数整合起来 1sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) 到这里又出现一个问题，sum函数会将所有数据整合起来，不光将一台机器的所有cpu加到一起，也将所有机器的cpu都加到了一起，最终显示的是集群cpu的总平均值，by(instance)可以解决这个问题。 1sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) by(instance) 这样就得到了空闲时cpu的数据了，用上边第一个公式即可得到单台主机cpu在一分钟内的使用率。 1(1 - ((sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) by(instance)) / (sum(increase(node_cpu_seconds_total[1m])) by(instance)))) * 100 3.2 计算内存使用率内存使用率公式为 = (available / total) * 100 1(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 3.3 rate函数rate函数是专门搭配counter类型数据使用的函数，功能是按照设置的一个时间段，取counter在这个时间段中平均每秒的增量。 举个栗子，假设我们要取ens33这个网卡在一分钟内字节的接受数量，假如一分钟内接收到的是1000bytes，那么平均每秒接收到就是1000bytes / 1m * 60s ≈ 16bytes/s。 1rate(node_network_receive_bytes_total{device='ens33'}[1m]) 如果是五分钟的话即为5000bytes / 5m * 60s ≈ 16bytes/s，结果是一样的，但曲线图就不一样了，上图为一分钟，下图为五分钟，因为五分钟的密度要更底，所以可以看到五分钟的曲线图更加平缓。 rate和increase的概念有些类似，但rate取的是一段时间增量的平均每秒数量，increase取的是一段时间增量的总量，即： rate(1m)：总量 / 60s increase(1m)：总量 3.4 sum函数sum函数就是将收到的数据全部进行整合。 假如一个集群里有20台服务器，分别为5台web服务器，10台db服务器，还有5台其他服务的服务器，这时候sum就可以分为三条曲线来代表不同功能服务器的总和数据。 3.5 topk函数topk函数的作用就是取前几位的最高值。 3.6 count函数count函数的作用是把符合条件的数值进行整合。 假如我们要查看集群中cpu使用率超过80%的主机数量的话 1count((1 - ((sum(increase(node_cpu_seconds_total{mode='idle'}[1m])) by(instance)) / (sum(increase(node_cpu_seconds_total[1m])) by(instance)))) * 100 &gt; 80) 四、Push gatewayPush gateway实际上就是一种被动推送数据的方式，与exporter主动获取不同。 4.1 Push gateway安装 下载安装Push gateway 123wget https://github.com/prometheus/pushgateway/releases/download/v1.4.0/pushgateway-1.4.0.linux-amd64.tar.gztar -xf pushgateway-1.4.0.linux-amd64.tar.gzmv pushgateway-1.4.0.linux-amd64 pushgateway 后台运行Push gateway 12screen./root/pushgateway/pushgateway 在prometheus.yml中加上 123- job_name: 'pushgateway' static_configs: - targets: ['localhost:9091'] 4.2 自定义编写脚本由于Push gateway自己本身是没有任何抓取数据的功能的，所以用户需要自行编写脚本来抓取数据。 举个例子：编写脚本抓取 TCP waiting_connection 的数量 编写自定义脚本 123456789101112131415161718192021222324252627#!/bin/bash# 获取监控主机名instance_name=`hostname -f | cut -d'.' -f1`# 如果主机名为localhost，则退出if [ $instance_name == &quot;localhost&quot; ]then echo &quot;不能监控主机名为localhost的主机&quot; exit 1fi#---# 获取TCP CONNECTED数量# 抓取TCP CONNECTED数量，定义为一个新keylable_tcp_connected=&quot;count_netstat_connected_connections&quot;count_netstat_connected_connections=`netstat -an | grep 'CONNECTED' | wc -l`# 上传至pushgatewayecho &quot;$lable_tcp_connected $count_netstat_connected_connections&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name#---# 该脚本是通过post的方式将key推送给pushgateway# http://localhost:9091 即推送给哪台pushgateway主机# job/pushgateway 即推送给prometheus.yml中定义的job为pushgateway的主机# instance/$instance_name 推送后显示的主机名 因为脚本都是运行一次后就结束了，可以配合crontab反复运行 12345crontab -e# 每分钟执行一次脚本* * * * * sh /root/pushgateway/node_exporter_shell.sh# 每10s执行一次脚本* * * * * sh /root/pushgateway/node_exporter_shell.sh 4.3 编写抓取ping丢包和延迟时间数据在node_exporter_shell.sh中加入 123456789101112131415161718192021#---# 获取ping某网站丢包率和延迟时间site_address=&quot;www.baidu.com&quot;# 获取丢包率和延迟时间，定义为两个新keylable_ping_packet_loss=&quot;ping_packet_loss&quot;ping_packet_loss_test=`ping -c3 $site_address | awk 'NR==7{print $6}'`# 字符串截取，%?为去除最后一个字符ping_packet_loss=`echo ${ping_packet_loss_test%?}`lable_ping_time=&quot;ping_time&quot;ping_time_test=`ping -c3 $site_address | awk 'NR==7{print $10}'`# 字符串截取，%??为去除最后两个字符ping_time=`echo ${ping_time_test%??}`# 上传至push_ping_timegatewayecho &quot;$lable_ping_packet_loss $ping_packet_loss&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_nameecho &quot;$lable_ping_time $ping_time&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name#--- 五、Grafana的使用Grafana是一款用Go语言开发的开源数据可视化工具，可以做数据监控和数据统计，带有告警功能。 5.1 Grafana安装1234wget https://dl.grafana.com/oss/release/grafana-7.5.1-1.x86_64.rpmyum -y install grafana-7.5.1-1.x86_64.rpmsystemctl start grafana-serversystemctl enable grafana-server 5.2 设置数据源 Grafana -&gt; Configuration -&gt; Date Sources -&gt; Prometheus New dashboard 添加一个监控和CPU内存使用率的仪表盘 5.3 json备份和还原 备份：dashboard -&gt; Settings -&gt; JSON Model，将里面内容保存为json文件 恢复：Create -&gt; import 5.4 Grafana实现报警功能 配置Grafana文件 123456789101112131415# 安装依赖和图形显示插件yum -y install libatk-bridge* libXss* libgtk*grafana-cli plugins install grafana-image-renderer# 修改配置vim /etc/grafana/grafana.inienabled = truehost = smtp.163.com:25# 发送报警邮件的邮箱user = chenqiming13@163.com# 授权码password = QXQALYMTRYRWIOOSskip_verify = truefrom_address = chenqiming13@163.comfrom_name = Grafanasystemctl restart grafana-server 创建报警规则 针对具体监控项，设置发送邮件阈值等，这里设置为发现超过阈值起5分钟后触发报警 ![Grafana实现报警功能七](Grafana实现报警功能七.png 六、Prometheus + Grafana实际案例6.1 predict_linear函数实现硬盘监控硬盘使用率公式为：（（总容量 - 剩余容量）/ 总容量）* 100，在Prometheus中表示为 1((node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes) * 100 通过df -m可以看出计算出来的值是正确的 Prometheus提供了一个predict_linear函数可以预计多长时间磁盘爆满，例如当前这1个小时的磁盘可用率急剧下降，这种情况可能导致磁盘很快被写满，这时可以使用该函数，用当前1小时的数据去预测未来几个小时的状态，实现提前报警。 12# 该式子表示用当前1小时的值来预测未来4小时后如果根目录下容量小于0则触发报警predict_linear(node_filesystem_free_bytes {mountpoint =&quot;/&quot;}[1h], 4*3600) &lt; 0 在Grafana添加监控硬盘使用率和预测硬盘使用率的仪表盘 6.2 监控硬盘IO公式为：（读取时间 / 写入时间）/ 1024 / 1024，用rate函数取一分钟内读和写的字节增长率来计算，用Prometheus表示为 1((rate(node_disk_read_bytes_total[1m]) + rate(node_disk_written_bytes_total[1m])) / 1024 / 1024) &gt; 0 6.3 监控TCP_WAIT状态的数量在被监控主机上编写监控脚本 12345678910111213141516171819202122#!/bin/bash# 获取监控主机名instance_name=`hostname -f | cut -d'.' -f1`# 如果主机名为localhost，则退出if [ $instance_name == &quot;localhost&quot; ]then echo &quot;不能监控主机名为localhost的主机&quot; exit 1fi#---# 获取TCP WAIT数量# 抓取TCP WAIT数量，定义为一个新keylable_tcp_wait=&quot;count_netstat_wait_connections&quot;count_netstat_wait_connections=`netstat -an | grep 'WAIT' | wc -l`# 上传至pushgatewayecho &quot;$lable_tcp_wait $count_netstat_wait_connections&quot; | curl --data-binary @- http://localhost:9091/metrics/job/pushgateway/instance/$instance_name#--- 6.4 监控文件描述符使用率在linux中，每当进程打开一个文件时，系统就会为其分配一个唯一的整型文件描述符，用来标识这个文件，每个进程默认打开的文件描述符有三个，分别为标准输入、标准输出、标准错误，即stdin、stout、steer，用文件描述符来表示为0、1、2。 用命令可以查看目前系统的最大文件描述符限制，一般默认设置是1024。 1ulimit -n 文件描述符使用率公式为：（已分配的文件描述符数量 / 最大文件描述符数量）* 100，在Prometheus中则表示为 1(node_filefd_allocated / node_filefd_maximum) * 100 6.5 网络延迟和丢包率监控前面我们采用的都是简单的ping + ip地址来进行测试，实际上这样测试发出去的icmp数据包是非常小的，只适合用来测试网络是否连通，因此用以下命令来进行优化： 12345ping -q ip地址 -s 500 -W 1000 -c 100-q:不显示指令执行过程，开头和结尾的相关信息除外。-s:设置数据包的大小。-W:在等待 timeout 秒后开始执行。-c:设置完成要求回应的次数。 6.6 使用Pageduty实现报警Pagerduty是一套付费监控报警系统，经常作为SRE/运维人员的监控报警工具，可以和市面上常见的监控工具直接整合。 创建新service 在Grafana新建报警渠道，并在仪表盘中设置为Pageduty报警 设置报警信息 查看是否收到报警 当问题解决可以点击已解决","link":"/2024/02/18/prometheus/"},{"title":"Kubernetes","text":"一、概念1.1 k8s概述Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。 容器是打包和运行应用程序的好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果系统处理此行为，会不会更容易？ 这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 简称 k8s，主要功能有： 服务发现和负载均衡 存储编排 自动部署和回滚 自动完成装箱计算 自我修复 密钥与配置管理 1.2 k8s组件一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。 工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。 为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。 1.2.1 控制平面组件kube-apiserver： API 服务器是 Kubernetes 控制面的组件，该组件公开了 Kubernetes API。API 服务器是 Kubernetes 控制面的前端。 etcd： etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。 kube-scheduler： 控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。 调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 kube-controller-manager： 在主节点上运行控制器的组件。 控制器包括： 节点控制器（Node Controller）: 负责在节点出现故障时进行通知和响应 任务控制器（Job controller）: 监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成 端点控制器（Endpoints Controller）: 填充端点(Endpoints)对象(即加入 Service 与 Pod) 服务帐户和令牌控制器（Service Account &amp; Token Controllers）: 为新的命名空间创建默认帐户和 API 访问令牌 cloud-controller-manager： 云控制器管理器是指嵌入特定云的控制逻辑的控制平面组件。云控制器管理器允许您链接聚合到云提供商的应用编程接口中，并分离出相互作用的组件与您的集群交互的组件。 下面的控制器都包含对云平台驱动的依赖： 节点控制器（Node Controller）: 用于在节点终止响应后检查云提供商以确定节点是否已被删除 路由控制器（Route Controller）: 用于在底层云基础架构中设置路由 服务控制器（Service Controller）: 用于创建、更新和删除云提供商负载均衡器 1.2.2 Node组件节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 kubelet： 一个在集群中每个节点（node）上运行的代理。它保证容器（containers）都运行在 Pod 中。 kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。 容器运行时（Container Runtime）： 容器运行环境是负责运行容器的软件，例如 Docker 。 1.2.3 插件（Addons）插件使用 Kubernetes 资源（DaemonSet、Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。 DNS： 尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该 有集群 DNS， 因为很多示例都需要 DNS 服务。 Web 界面（仪表盘）： Dashboard 是Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 容器资源监控： 容器资源监控将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面。 集群层面日志： 集群层面日志机制负责将容器的日志数据 保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。 1.3 Pod1.3.1 Pod概念Pod 是 k8s 中的最小单元，一个 Pod 包含一个或多个容器，一个 Pod 不会跨越多个节点（Node）。 而每个 Pod 里都会有一个根容器 pause，Pod 中的其他容器都共享 pause 根容器的网络栈和volume挂载卷，因此容器之间的通信和数据交换会更为高效。 1.3.2 RC、RS、Deploymentk8s 管理 Pod 主要用到以下三个组件： Replication Controller（RC）：用来确保容器应用的副本数始终保持在用户定义的副本数，如果有容器异常退出，会自动创建新的 Pod 来代替，如果有异常多出的容器也会自动回收。 ReplicaSet（RS）：相比 RC 多了支持 selector，推荐使用 RS。 Deployment：用来管理 RS。 来看看 Deployment 和 RS 是如何实现更新的： Deployment 创建新的 RS RS1删除一个容器，接着 RS2 新建一个新版本的 Pod 全部更新完之后，RS1 并不会删除，而是保留着处于停用状态，如果新版本出了问题需要回滚，就可以反过来操作实现回滚 1.3.3 Horizontal Pod AutoscalerHorizontal Pod Autoscaler（HPA）在k8s集群中用于 Pod 水平自动弹性伸缩，它是基于 CPU 和内存利用率对 Deployment 和 RS 中的 Pod 数量进行自动扩缩容（除了 CPU 和内存利用率之外，也可以基于其他应程序提供的度量指标 custom metrics 进行自动扩缩容）。 假如 HPA 检测到当前 Deployment 和 RS 所管理的 Pod 的 CPU 或内存使用率超过了设定之后，就会创建新的 Pod 来实现降压，新建 Pod 的数量限制由 Max 和 Min 设定。 1.3.4 StatefulSetRS 和 Deployment 都是面向无状态的服务，它们所管理的 Pod 的 IP、名字，启停顺序等都是随机的，而 StatefulSet 是有状态的集合，管理所有有状态的服务，比如 MySQL、MongoDB 集群等。 StatefulSet 的特点有： Pod 的一致性：包含次序（启停顺序，例如 mysql -&gt; php-fpm -&gt; nginx 的启动顺序）、网络一致性（与 Pod 相关，与被调度的 Node 节点无关）。 稳定的存储：即 Pod 重新调度之后还是访问到相同的持久化数据，基于 PVC（PV 是集群中由管理员提供或使用存储类动态提供的一块存储。它是集群中的资源，就像节点是集群资源一样。而 PVC 是用户对存储的请求。它类似于 Pod，Pod 消耗 Node 资源，而 PVC 消耗 PV 资源。） 来实现。 稳定的次序：对于N个副本的 StatefulSet，每个 Pod 都在 [0，N) 的范围内分配一个数字序号，且是唯一的。 稳定的网络：Pod 的 hostname 模式为：( StatefulSet 名称 ) - ( 序号 )。 1.3.5 DaemonSetDaemonSet 确保全部或者一部分 Node 上运行一个 Pod 的副本，当有 Node 加入集群时，也会为他们新增一个 Pod，当这些 Node 退出集群时，这些 Pod 也会被回收。 DaemonSet 的一些典型用法： 运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph等。 运行日志收集daemon，例如fluentd、logstash等。 运行监控daemon，例如 Prometheus 的 Node exporter、zabbix等。 1.3.6 Job和Cron JobJob 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 这里可能有人会想，那在 linux 上直接执行脚本不就行了吗？其实这就会有个问题，如果脚本执行失败那么久退出且不会再执行了，需要执行就必须手动，但 Job 设置的任务只有在正常执行结束后才会结束，否则一直执行到成功为止。Job 也可以设置成功的次数要达到几次才允许退出。 Cron Job 是基于时间管理控制 Job，即在给定的时间只运行一次、周期性的在指定时间运行。 1.3.7 ServicePod 的生命是有限的，如果 Pod 重启 IP 也可能会发生变化。如果我们将 Pod 的 IP 写死，Pod 如果挂了或重启，其它的服务也会不可用。我们可以把我们的服务（各种 Pod）注册到服务发现中心去，让服务发现中心去动态更新其它服务的配置就可以了，k8s 就给我们提供了这么一个服务，即 Service。 我们这样就可以不用去管后端的 Pod 如何变化，只需要指定 Service 的地址就可以了，因为我们在中间添加了一层服务发现的中间件，Pod 销毁或者重启后，把这个 Pod 的地址注册到这个服务发现中心去。 1.4 k8s的网络通讯方式k8s 的网络模型假定了所有的 Pod 都在一个可以直接连通的扁平化网络空间中，在这 GCE（Google Compute Engine）里面是现成的网络模型。 Flannel 是 CoreOS 团队针对 Kubernetes 设计一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建 Docker 容器都具有全集群唯一的虚拟 IP 地址。而且它还能在这些 IP 地址之间建立一个覆盖网络（Overlay Network），通过覆盖网络将数据包原封不动的传递到目标容器内。 通过一个架构图来看看不同情况下的通讯是怎么样的： 通讯情况主要分为以下几种： 同一 Pod 不同容器之间的通信：采用 pause 网络栈。 同一 Node 不同 Pod 之间的通信：通过 docker0 网桥进行通信。 不同 Node 的 Pod 之间的通信：不同 Node 的 Pod 之间的通信是 k8s 网络通信的难点，是通过 Flannel 网络通讯方式来实现的，通讯的过程分为以下几个步骤： 数据包从 Node1 的 Pod 到达 docker0 网桥 Flanneld 会开启一个 Flannel0 的网桥，用来抓取到达 docker0 的数据，可以理解为一个钩子函数 Flanneld 会有很多路由表信息，是存储在 etcd 中由 Flanneld 自动获取的，通过路由表知道了转发信息后就通过物理网卡进行转发 发送到目标主机的物理网卡后，就会再通过 Flanneld -&gt; Flannel0网桥 -&gt; docker0网桥，最终到达目的 Pod Pod 与 service 之间的通信：采用各节点的 iptables 规则来实现。 Pod 到外网：通过 Flanneld 到达物理网卡，经过路由选择后，iptables 执行 Masquerade，把Pod 的虚拟 IP 改为 物理网卡的 IP，在向外网服务器发出请求。 外网到 Pod：通过 service 进行访问，一般使用 NodePort。 二、k8s部署本次 k8s 部署为以下环境 2.1 基本环境配置（所有节点） 在各主机设置主机名以及host文件解析 1234567hostnamectl set-hostname k8s-master01hostnamectl set-hostname k8s-node01hostnamectl set-hostname k8s-node02vim /etc/hosts192.168.88.10 k8s-master01192.168.88.20 k8s-node01192.168.88.21 k8s-node02 安装依赖 1yum -y install conntrack ntpdate ntp ipvsadm ipset jp iptables curl stsstat libseccomp wget vim net-tools git 设置防火墙为iptables并设置空规则 12345systemctl stop firewalld &amp;&amp; systemctl disable firewalldyum -y install iptables-servicessystemctl start iptablessystemctl enable iptablesiptables -F &amp;&amp; service iptables save 关闭虚拟内存和selinux 12swapoff -a &amp;&amp; sed -i '/ swap / s/^\\(.*\\)$/#/g' /etc/fstabsetenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 调整内核参数 1234567891011121314151617cat &gt; kubernetes.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ipv6tables=1net.ipv4.ip_forward=1net.ipv4.tcp_tw_recycle=0vm.swappiness=0 # 禁用swap，只有系统OOM时才允许使用vm.overcommit_memory=1 # 不检查物理内存是否够用vm.panic_on_oom=0 # 开启OOMfs.inotify.max_user_instances=8192fs.inotify.max_user_watches=1048576fs.file-max=52706963fs.nr_open=52706963net.ipv6.conf.all.disable_ipv6=1net.netfilter.nf_conntrack_max=2310720EOFcp kubernetes.conf /etc/sysctl.d/kubernetes.confsysctl -p /etc/sysctl.d/kubernetes.conf 调整系统时区 1234567# 设为中国/上海timedatectl set-timezone Asia/Shanghai# 将当前UTC时间写入硬件时钟timedatectl set-local-rtc 0# 重启依赖于时间的服务systemctl restart rsyslogsystemctl restart crond 关闭不需要的服务 12# 关闭邮件服务systemctl stop postfix &amp;&amp; systemctl disable postfix 设置rsyslog和systemd journald 1234567891011121314151617181920212223242526272829# 持久化保存日志目录mkdir /var/log/journalmkdir /etc/systemd/journald.conf.dcat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt;EOF[Journal]#持久化保存到磁盘Storage=persistent#压缩历史日志Compress=yes SyncIntervalSec=5mRateLimitInterval=30sRateLimitBurst=1000#最大占用空间 10GSystemMaxUse=10G#单日志文件最大 200MSystemMaxFileSize=200M#日志保存时间 2 周MaxRetentionSec=2week#不将日志转发到 syslogForwardToSyslog=noEOFsystemctl restart systemd-journald 升级内核 12345678910# 导入公钥rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org# 安装elrepo源yum -y install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm# 升级内核yum --enablerepo=elrepo-kernel install -y kernel-lt# 设置开机从新内核启动grub2-set-default &quot;CentOS Linux (5.4.116-1.e17.elrepo.x86_64) 7 (Core)&quot;# 查看内核启动项grub2-editenv list 2.2 kubeadm部署（所有节点） kube-proxy开启ipvs前置条件 123456789101112131415modprobe br_netfiltervim /etc/sysconfig/modules/ipvs.modules#!/bin/bashipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack&quot;for kernel_module in ${ipvs_modules};do /sbin/modinfo -F filename ${kernel_module} &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe ${kernel_module} fidonechmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack 安装docker 123456789101112131415161718192021yum -y install yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repoyum makecache fastyum -y install docker-ce docker-ce-climkdir /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF{ &quot;registry-mirrors&quot;: [&quot;http://f1361db2.m.daocloud.io&quot;], &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;:&quot;json-file&quot;, &quot;log-opts&quot;:{ &quot;max-size&quot;:&quot;100m&quot; }}EOFmkdir -p /etc/systemd/system/docker.service.dsystemctl daemon-reload &amp;&amp; systemctl start docker &amp;&amp; systemctl enable docker 2.3 安装kubeadm（所有节点） 安装 kubeadm kubectl kubelet 123456789101112cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum -y install kubeadm kubectl kubeletsystemctl enable kubelet 通过kubeadm查看目前各镜像版本 12345678kubeadm config images listk8s.gcr.io/kube-apiserver:v1.21.0k8s.gcr.io/kube-controller-manager:v1.21.0k8s.gcr.io/kube-scheduler:v1.21.0k8s.gcr.io/kube-proxy:v1.21.0k8s.gcr.io/pause:3.4.1k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns/coredns:v1.8.0 编写脚本，通过阿里镜像安装 1234567891011121314151617181920212223242526vim k8s_images.sh#!/bin/bashapiserver_var=v1.21.0controller_manager_var=v1.21.0scheduler_var=v1.21.0proxy_var=v1.21.0pause_var=3.4.1etcd_var=3.4.13-0coredns_var=v1.8.0image_aliyun=(kube-apiserver:$apiserver_var kube-controller-manager:$controller_manager_var kube-scheduler:$scheduler_var kube-proxy:$proxy_var pause:$pause_var etcd:$etcd_var coredns/coredns:$coredns_var)for image in ${image_aliyun[@]}do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$image docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$image k8s.gcr.io/${image} docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imagedone./k8s_images.sh# 问题# 由于杭州阿里源里目前没有coredns:v1.8.0版本，所以在北京阿里源下载docker pull registry.cn-beijing.aliyuncs.com/dotbalo/coredns:1.8.0docker tag registry.cn-beijing.aliyuncs.com/dotbalo/coredns:1.8.0 k8s.gcr.io/coredns/coredns:v1.8.0docker rmi registry.cn-beijing.aliyuncs.com/dotbalo/coredns:1.8.0 2.4 初始化Master节点1kubeadm config print init-defaults &gt; kubeadm.config.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546vim kubeadm.config.ymlapiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: # 改成master节点IP地址 advertiseAddress: 192.168.88.10 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: node taints: null---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: {}dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: 1.21.0networking: dnsDomain: cluster.local podSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/12scheduler: {}# 添加这一段---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationkubeproxy: config: mode: ipvs 123456kubeadm init --config=kubeadm.config.yml --upload-certs | tee kubeadm-init.log...# 初始化后操作mkdir -p $HOME/.kubecp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config 2.5 部署网络123mkdir -p k8s/plugin/flannel &amp;&amp; cd k8s/plugin/flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl create -f kube-flannel.yml kube-flannel.yml文件内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223---apiVersion: policy/v1beta1kind: PodSecurityPolicymetadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/defaultspec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: &quot;/etc/cni/net.d&quot; - pathPrefix: &quot;/etc/kube-flannel&quot; - pathPrefix: &quot;/run/flannel&quot; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: ['NET_ADMIN', 'NET_RAW'] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: 'RunAsAny'---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: flannelrules:- apiGroups: ['extensions'] resources: ['podsecuritypolicies'] verbs: ['use'] resourceNames: ['psp.flannel.unprivileged']- apiGroups: - &quot;&quot; resources: - pods verbs: - get- apiGroups: - &quot;&quot; resources: - nodes verbs: - list - watch- apiGroups: - &quot;&quot; resources: - nodes/status verbs: - patch---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: flannelroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannelsubjects:- kind: ServiceAccount name: flannel namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: flannel namespace: kube-system---kind: ConfigMapapiVersion: v1metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flanneldata: cni-conf.json: | { &quot;name&quot;: &quot;cbr0&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;plugins&quot;: [ { &quot;type&quot;: &quot;flannel&quot;, &quot;delegate&quot;: { &quot;hairpinMode&quot;: true, &quot;isDefaultGateway&quot;: true } }, { &quot;type&quot;: &quot;portmap&quot;, &quot;capabilities&quot;: { &quot;portMappings&quot;: true } } ] } net-conf.json: | { &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } }---apiVersion: apps/v1kind: DaemonSetmetadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannelspec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.14.0-rc1 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.14.0-rc1 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; limits: cpu: &quot;100m&quot; memory: &quot;50Mi&quot; securityContext: privileged: false capabilities: add: [&quot;NET_ADMIN&quot;, &quot;NET_RAW&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg 部署完后可以看到 flannel 网卡 通过命令查看各模块运行情况 1kubectl get pod -n kube-system 2.6 添加Node节点 查看kubeadm-init.log 123...kubeadm join 192.168.88.10:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:37c9645a7a2ee75301f132537240157ecd4f464494022cc442f77489c1978989 在node主机上执行 12kubeadm join 192.168.88.10:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:37c9645a7a2ee75301f132537240157ecd4f464494022cc442f77489c1978989 在master主机查看是否加入成功 1234kubectl get node...kubectl get pod -n kube-system -o wide... 三、k8s资源清单在 k8s 中，一般使用 yaml 格式的文件来创建符合我们预期期望的 pod，这样的 yaml 文件我们一般称为资源清单。 3.1 资源类型名称空间级别 工作负载型资源：Pod、RS、Deployment、StatefulSet、DaemonSet、Job、CronJob 服务发现及负载均衡型资源：Service 配置与存储型资源：Volume、CSI 特殊类型的存储卷：ConfigMap(当配置中心来使用的资源类型)、Secret(保存敏感数据)、DownwarAPI(把外部环境中的信息输出给容器) 集群级资源：Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding 元数据型资源：HPA、PodTemplate、LimitRange 3.2 常用字段必要属性 参数名 字段类型 说明 apiVersion String K8S API 的版本，目前基本是v1，可以用 kubectl api-versions 命令查询 kind String 这里指的是 yaml 文件定义的资源类型和角色，比如: Pod metadata Object 元数据对象，固定值写 metadata metadata.name String 元数据对象的名字，这里由我们编写，比如命名Pod的名字 metadata.namespace String 元数据对象的命名空间，由我们自身定义 spec Object 详细定义对象，固定值写Spec spec.containers[] list 这里是Spec对象的容器列表定义，是个列表 spec.containers[].name String 这里定义容器的名字 spec.containers[].image String 这里定义要用到的镜像名称 spec 主要对象 参数名 字段类型 说明 spec.containers[].name String 定义容器的名字 spec.containers[].image String 定义要用到的镜像的名称 spec.containers[].imagePullPolicy String 定义镜像拉取策略，有 Always，Never，IfNotPresent 三个值课选 （1）Always：意思是每次尝试重新拉取镜像 （2）Never：表示仅使用本地镜像 （3）IfNotPresent：如果本地有镜像就是用本地镜像，没有就拉取在线镜像。上面三个值都没设置的话，默认是 Always spec.containers[].command[] List 指定容器启动命令，因为是数组可以指定多个，不指定则使用镜像打包时使用的启动命令 spec.containers[].args[] List 指定容器启动命令参数，因为是数组可以指定多个 spec.containers[].workingDir String 指定容器的工作目录 spec.containers[].volumeMounts[] List 指定容器内部的存储卷配置 spec.containers[].volumeMounts[].name String 指定可以被容器挂载的存储卷的名称 spec.containers[].volumeMounts[].mountPath String 指定可以被容器挂载的容器卷的路径 spec.containers[].volumeMounts[].readOnly String 设置存储卷路径的读写模式，true 或者 false，默认为读写模式 spec.containers[].ports[] List 指定容器需要用到的端口列表 spec.containers[].ports[].name String 指定端口名称 spec.containers[].ports[].containerPort String 指定容器需要监听的端口号 spec.containers[].ports.hostPort String 指定容器所在主机需要监听的端口号，默认跟上面 containerPort 相同，注意设置了 hostPort 同一台主机无法启动该容器的相同副本（因为主机的端口号不能相同，这样会冲突） spec.containers[].ports[].protocol String 指定端口协议，支持TCP和UDP，默认值为TCP spec.containers[].env[] List 指定容器运行千需设置的环境变量列表 spec.containers[].env[].name String 指定环境变量名称 spec.containers[].env[].value String 指定环境变量值 spec.containers[].resources Object 指定资源限制和资源请求的值（这里开始就是设置容器的资源上限） spec.containers[].resources.limits Object 指定设置容器运行时资源的运行上限 spec.containers[].resources.limits.cpu String 指定CPU的限制，单位为 core 数，将用于 docker run –cpu-shares 参数 spec.containers[].resources.limits.memory String 指定 MEM 内存的限制，单位为 MIB，GIB spec.containers[].resources.requests Object 指定容器启动和调度室的限制设置 spec.containers[].resources.requests.cpu String CPU请求，单位为 core 数，容器启动时初始化可用数量 spec.containers[].resources.requests.memory String 内存请求，单位为 MIB，GIB 容器启动的初始化可用数量 额外的参数项 参数名 字段类型 说明 spec.restartPolicy String 定义Pod重启策略，可以选择值为 Always、OnFailure、Never，默认值为 Always。1. Always：Pod一旦终止运行，则无论容器是如何终止的，kubelet 服务都将重启它。2. OnFailure：只有 Pod 以非零退出码终止时，kubelet 才会重启该容器。如果容器正常结束（退出码为0），则 kubelet 将不会重启它。3. Never：Pod 终止后，kubelet 将退出码报告给 Master，不会重启该 Pod。 spec.nodeSelector Object 定义 Node 的 Label 过滤标签，以 key:value 格式指定 spec.imagePullSecrets Object 定义pull 镜像是使用 secret 名称，以 name:secretkey 格式指定 spec.hostNetwork Boolean 定义是否使用主机网络模式，默认值为 false。设置 true 表示使用宿主机网络，不使用 docker 网桥，同时设置了 true 将无法在同一台宿主机上启动第二个副本。 辅助命令 12# 查看资源对象用法kubectl explain &lt;资源对象&gt; 例子： 创建 yaml 文件 123456789101112vim nginx.yaml# 此处只有必要字段apiVersion: v1kind: Podmetadata: name: nginx-pod labels: app: nginxspec: containers: - name: nginx image: daocloud.io/library/nginx:latest 创建pod 1kubectl apply -f nginx.yaml 查看状态 123456789# 查看是否生成podkubectl get pod...# 查看pod详细信息kubectl describe pod nginx-pod...# 测试能否访问curl 10.244.1.2 # 此处为flannel分配的地址... 3.3 容器生命周期每一个 Pod 被成功创立之前，都会进行初始化，会运行零个或若干个 init 容器，init 容器运行完就释放，接着才会运行 main 主容器，当然在 init 容器运行之前会先运行 pause 容器，以保证存储和网络的可用。 init 容器与普通的容器非常相似，除了以下两点： init 容器总是运行到完成。 每个 init 容器都要在下一个容器启动之前完成。 如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 “Never”，Kubernetes 不会重新启动 Pod。 如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。 每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时， Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。 init 容器的使用 因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势： Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。 例如，没有必要仅为了在安装过程中使用类似 sed、awk、python 或 dig 这样的工具而去 FROM 一个镜像来生成一个新的镜像。 Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。 应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。 Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可以访问应用容器不能访问的 Secret 的权限。 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。 一旦前置条件满足，Pod 内的所有的应用容器会并行启动。 init 容器使用例子 编写 yaml 文件 12345678910111213141516171819202122232425vim busybox.yamlapiVersion: v1kind: Pod# 定义了一个 Pod 叫 busybox-pod，busybox 封装了 linux 的大量命令metadata: name: busybox-pod labels: app: busybox# 定义该 Pod 下的资源，也就是容器spec: # 定义了一个 busybox 容器 containers: - name: busybox image: busybox:latest command: ['sh','-c','echo The busybox app is running! &amp;&amp; sleep 3600'] # 定义 init 容器，init 容器全部执行完前不会执行 busybox 容器 initContainers: # 第一个 init 容器，如果检查 service 中没有注册 myservice，则休眠2秒继续执行，直到成功 - name: init-myservice image: busybox command: ['sh','-c','until nslookup myservice; do echo waiting for myservice; sleep 2; done'] # 第一个 init 容器运行完就运行该 init 容器，检查 service 中有没有注册 mydb - name: init-mydb image: busybox command: ['sh','-c','until nslookup mydb; do echo waiting for mydb; sleep 2; done'] 运行 1kubectl create -f busybox.yaml 可以看到一直处于 Init:0/2 状态，因为第一个 init 容器一直没完成 编写 yaml 文件添加 service 1234567891011121314151617181920vim service.yamlapiVersion: v1kind: Servicemetadata: name: myservicespec: ports: - protocol: TCP port: 80 targetPort: 9376---apiVersion: v1kind: Servicemetadata: name: mydbspec: ports: - protocol: TCP port: 80 targetPort: 9377 运行 1kubectl create -f service.yaml 可以看到两个 init 容器都执行完，main 主容器也就运行了 3.4 探针探针（probe）是由 kubelet 对容器执行的定期诊断。 要执行诊断，kubelet 调用由容器实现的 Handler （处理程序）。有三种类型的处理程序： ExecAction： 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 TCPSocketAction： 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction： 对容器的 IP 地址上指定端口和路径执行 HTTP Get 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。 每次探测都将获得以下三种结果之一： Success（成功）：容器通过了诊断。 Failure（失败）：容器未通过诊断。 Unknown（未知）：诊断失败，因此不会采取任何行动。 探针可以分为以下三种： livenessProbe：指示容器是否正在运行。如果存活态探测失败，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定未来。如果容器不提供存活探针， 则默认状态为 Success。 readinessProbe：指示容器是否准备好为请求提供服务。如果就绪态探测失败， 端点控制器将从与 Pod 匹配的所有服务的端点列表中删除该 Pod 的 IP 地址。 初始延迟之前的就绪态的状态值默认为 Failure。 如果容器不提供就绪态探针，则默认状态为 Success。 startupProbe: 指示容器中的应用是否已经启动。如果提供了启动探针，则所有其他探针都会被禁用，直到此探针成功为止。如果启动探测失败，kubelet 将杀死容器，而容器依其重启策略进行重启。 如果容器没有提供启动探测，则默认状态为 Success。 3.4.1 readinessProbe就绪检测 编写 yaml 文件 123456789101112131415161718192021vim readinessProbe.yamlapiVersion: v1kind: Podmetadata: name: readiness-httpget-pod namespace: defaultspec: containers: - name: readiness-httpget-container image: daocloud.io/library/nginx:latest # 调用readinessProbe探针 readinessProbe: httpGet: # 检查80端口 port: 80 # 检查该目录下的网页 path: /test.html # 容器启动1秒后才进行检测 initialDelaySeconds: 1 # 每3秒检测一次 periodSeconds: 3 生成 Pod 1kubectl create -f readinessProbe.yaml 可以看到虽然 Pod 运行了，但 ready 状态是不对的 进入容器创建文件，问题即可解决 12kubectl exec -it readiness-httpget-pod -- /bin/shecho 'this is test' &gt; /usr/share/nginx/html/test.html 3.4.2 livenessProbe存活检测livenessProbe-exec 编写 yaml 文件 123456789101112131415161718vim livenessProbe_exec.yamlapiVersion: v1kind: Podmetadata: name: liveness-exec-pod namespace: defaultspec: containers: - name: liveness-exec-container image: busybox command: ['sh','-c','touch /tmp/test; sleep 60; rm -rf /tmp/test; sleep 3600'] # 查看本地是否有镜像，有则使用，没有则下载 imagePullPolicy: IfNotPresent livenessProbe: exec: command: ['test','-e','/tmp/test'] initialDelaySeconds: 1 periodSeconds: 3 生成 Pod 1kubectl create -f livenessProbe.yaml 可以看到当 test 文件给删除的时候，Pod 就会重启，重启后又会有 test 文件，一直循环下去 livenessProbe-httpget 编写 yaml 文件 12345678910111213141516171819202122vim livenessProbe_httpget.yamlapiVersion: v1kind: Podmetadata: name: liveness-httpget-pod namespace: defaultspec: containers: - name: liveness-httpget-container image: daocloud.io/library/nginx:latest ports: - name: http containerPort: 80 # Pod生成1秒之后会每3秒检测一次是否存在index.html文件，如果超过10秒没有则重启Pod livenessProbe: httpGet: port: http path: /index.html initialDelaySeconds: 1 periodSeconds: 3 # 允许超时时间 timeoutSeconds: 10 创建容器，可以看到是在正常运行的 1kubectl create -f livenessProbe_httpget.yaml 删除 index.html 文件，可以看到会执行重启 12kubectl exec -it liveness-httpget-pod -- /bin/shrm -rf /usr/share/nginx/html/index.html livenessProbe-tcp 编写 yaml 文件 12345678910111213141516vim livenessProbe_tcp.yamlapiVersion: v1kind: Podmetadata: name: liveness-tcp-pod namespace: defaultspec: containers: - name: liveness-tcp-container image: daocloud.io/library/nginx:latest livenessProbe: initialDelaySeconds: 5 timeoutSeconds: 1 tcpSocket: port: 8080 periodSeconds: 3 生成 Pod，会发现一直处于重启状态 1kubectl create -f livenessProbe_tcp.yaml 3.4.3 Start 和 StopStart 和 Stop 是指 Pod 在生成后执行和结束前执行的命令 编写 yaml 文件 1234567891011121314151617vim start_stop.ymlapiVersion: v1kind: Podmetadata: name: start-stop-pod namespace: defaultspec: containers: - name: start-stop-container image: daocloud.io/library/nginx:latest lifecycle: postStart: exec: command: ['/bin/sh','-c','echo this is postStart test &gt; /usr/share/message'] preStop: exec: command: ['/bin/sh','-c','echo this is preStop test &gt; /usr/share/message'] 生成 Pod，进入 Pod 可以看到 Start 执行的命令，由于 Pod 停止后就没有了，所以看不到 Stop 执行的命令 1kubectl exec -it start-stop-pod -- /bin/sh 四、k8s控制器4.1 什么是控制器k8s 中内置了很多 controller，用来控制 Pod 的具体状态和行为。 控制器的类型有： ReplicationController（已弃用） 和 ReplicaSet Deployment DaemonSet StateFulSet Job 和 CronJob Horizontal Pod Autoscaling 4.2 RS 与 Deploymentk8s 管理 Pod 主要用到以下三个组件： Replication Controller（RC）：用来确保容器应用的副本数始终保持在用户定义的副本数，如果有容器异常退出，会自动创建新的 Pod 来代替，如果有异常多出的容器也会自动回收。 ReplicaSet（RS）：相比 RC 多了支持 selector，推荐使用 RS。 Deployment：用来管理 RS。 RS 单独使用 编写 yaml 文件 1234567891011121314151617181920212223vim rs_test.yamlapiVersion: apps/v1kind: ReplicaSetmetadata: # RS名称 name: rs-testspec: # 设置副本数 replicas: 3 selector: # 设置标签 matchLabels: tier: rs-test # 设置模板，会根据此模板生成Pod template: metadata: # 与上边RS设置的标签相对应，说明根据该模板创建的Pod都由标签相对RS管理 labels: tier: rs-test spec: containers: - name: rs-nginx-container image: daocloud.io/library/nginx:latest 生成 RS，可以看到会根据 replicas设置的数量创建 Pod 1kubectl create -f rs_test.yaml 删除这些 Pod，RS 也会按照副本数重新建立新的 Pod 1kubectl delete pod --all RS 与 Deployment 编写 yaml 文件 1234567891011121314151617181920212223242526vim nginx_deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment# Deployment详细信息spec: # 副本数 replicas: 3 # 选择标签 selector: # 标签匹配，与Deployment对应 matchLabels: app: nginx-deployment # Pod模板 template: metadata: # 定义标签 labels: app: nginx-deployment spec: containers: - name: nginx-deployment-container image: daocloud.io/library/nginx:latest ports: - containerPort: 80 生成 Deployment 12kubectl create -f nginx_deployment.yaml --recordrecord:记录命令，方便每次 reversion 的变化 可以看到会生成对应的 RS 和 Pod 扩容 1kubectl scale deployment nginx-deployment --replicas 5 更新镜像 1kubectl set image deployment/nginx-deployment nginx-deployment-container=daocloud.io/library/nginx:1.19.1 可以看到会创建一个新的 RS，以实现灰度更新 回滚 1kubectl rollout undo deployment/nginx-deployment 查看回滚状态 1kubectl rollout status deployment/nginx-deployment 查看历史版本 1kubectl rollout history deployment/nginx-deployment 回到历史指定版本 1kubectl rollout undo deployment/nginx-deployment --to-revision=1 暂停更新 1kubectl rollout pause deployment/nginx-deployment 4.3 DaemonSetDaemonSet 确保全部或者一部分 Node 上运行一个 Pod 的副本，当有 Node 加入集群时，也会为他们新增一个 Pod，当这些 Node 退出集群时，这些 Pod 也会被回收。 DaemonSet 的一些典型用法： 运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph等。 运行日志收集daemon，例如fluentd、logstash等。 运行监控daemon，例如 Prometheus 的 Node exporter、zabbix等。 编写 yaml 文件 12345678910111213141516171819202122vim daemonset_test.yamlapiVersion: apps/v1kind: DaemonSetmetadata: # DaemonSet名称 name: daemonset-test # 设置标签 labels: app: daemonset-nginxspec: selector: # 该标签要与上边标签一致 matchLabels: name: daemonset-nginx template: metadata: labels: name: daemonset-nginx-pod spec: containers: - name: daemonset-nginx-container image: daocloud.io/library/nginx:latest 生成 DaemonSet 1kubectl create -f daemonset_test.yaml 删除一个 Pod 之后 DaemonSet 为保证每个节点都至少有一个副本，就会重新创建新的 Pod 4.4 Job 和 CronJob4.4.1 JobJob 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 Job spec spec.template格式同 Pod 相同 RestartPolicy仅支持Never和OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 spec.completions标志 Job 结束时需要成功运行的 Pod 个数，默认为1 spec.parallelism标志并行运行的 Pod 个数，默认为1 spec.activeDeadlineSeconds标志失败 Pod 的重试最大时间，超过该时间就不会再重试 编写 yaml 文件 1234567891011121314151617vim job.yamlapiVersion: batch/v1kind: Jobmetadata: name: jobspec: template: metadata: name: job-pod spec: containers: - name: job-pod-container image: perl imagePullPolicy: IfNotPresent # 通过perl语言进行圆周率计算，计算小数点后2000位 command: ['perl','-Mbignum=bpi','-wle','print bpi(2000)'] restartPolicy: Never 生成 Job，可以看到开始是 Running，接着就是 Completed，代表 Job 已经完成 1kubectl create -f job.yaml 查看日志可以看到计算好的圆周率 4.4.2 CronJobCron Job 是基于时间管理控制 Job，即在给定的时间只运行一次、周期性的在指定时间运行。 CronJob spec 所有 CronJob 的 schedule: 时间都是基于 kube-controller-manager 的时区。 .spec.schedule 是 .spec 需要的域。它使用了 Cron 格式串，例如 0 * * * * or @hourly ，做为它的任务被创建和执行的调度时间。 .spec.jobTemplate是任务的模版，是必须项。 .spec.startingDeadlineSeconds 域是可选项。它表示任务如果由于某种原因错过了调度时间，开始该任务的截止时间的秒数。过了截止时间，CronJob 就不会开始任务。 不满足这种最后期限的任务会被统计为失败任务。如果该域没有声明，那任务就没有最后期限。 .spec.suspend域也是可选的。如果设置为 true ，后续发生的执行都会挂起。 这个设置对已经开始的执行不起作用。默认是关闭的。 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit是可选的。 这两个字段指定应保留多少已完成和失败的任务。 默认设置为3和1。限制设置为0代表相应类型的任务完成后不会保留。 并发性规则 .spec.concurrencyPolicy声明了 CronJob 创建的任务执行时发生重叠如何处理。 spec 仅能声明下列规则中的一种： Allow (默认)：CronJob 允许并发任务执行。 Forbid： CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行。 Replace：如果新任务的执行时间到了而老任务没有执行完，CronJob 会用新任务替换当前正在运行的任务。 并发性规则仅适用于相同 CronJob 创建的任务。如果有多个 CronJob，它们相应的任务总是允许并发执行的。 编写 yaml 文件 12345678910111213141516171819202122vim crontjob.yamlapiVersion: batch/v1kind: CronJobmetadata: name: cronjobspec: # CronJob必要字段，格式为分时日月周 schedule: &quot;*/1 * * * *&quot; # job模板 jobTemplate: spec: template: spec: containers: - name: crontab-pod image: busybox imagePullPolicy: IfNotPresent command: - /bin/sh - -c - date; echo 'Welcome My K8s' restartPolicy: OnFailure 生成 CronJob，可以看到每分钟都会创建一个 Job 和 Pod 1kubectl create -f cronjob.yaml 五、Service5.1 什么是Service 到这里我们都知道，Deployment 会根据 replicas保证 Pod 的数量，当上图的其中一个 php-fpm Pod 出现问题时，就会新建一个来代替。但这时候会会出现一个问题，新的 Pod 的地址与旧的很可能不一样，那 nginx 也无法连接到新的 Pod，除非修改 nginx 的配置，这样的 k8s 集群效率是非常低的。 Service 就能解决该问题，在 nginx 和 php-fpm 中间添加一个 Service，由该 Service 来管理各 php-fpm Pod 的信息，每个 Pod 会设置一个标签，只要与 SVC 中设置的标签相匹配，就可以进行管理，其它 Pod（例如下图的 nginx）想要访问该 SVC 管理的 Pod，只要通过 SVC 的地址就可以访问到。 Service 定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常称为微服务。 5.2 Service发布服务（服务类型)对一些应用的某些部分（如前端），可能希望将其暴露给 Kubernetes 集群外部 的 IP 地址。 Kubernetes ServiceTypes 允许指定你所需要的 Service 类型，默认是 ClusterIP。 Type 的取值以及行为如下： ClusterIP：通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ServiceType。 NodePort：通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到自动创建的 ClusterIP 服务。 通过请求 &lt;节点 IP&gt;:&lt;节点端口&gt;，你可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。 ExternalName：通过返回 CNAME 和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。 无需创建任何类型代理。 ClusterIP NodePort LoadBalancer ExternalName 5.3 虚拟 IP 和 Service 代理在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。 kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 代理模式分类： userspace 代理模式 在该模式下，所有操作都要通过 kube-proxy 进行一个代理的操作，kube-proxy 的压力相对会较大。 iptables 代理模式 在该模式下，所有的访问都通过 iptables 来处理，kube-proxy 的压力就会减小很多。 IPVS 代理模式 在该模式下，iptables 换成了 IPVS，通过内核模块来实现负载均衡。 在 ipvs 模式下，kube-proxy 监视 Kubernetes 服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保 IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端Pod之一。 IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。 与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。 IPVS 提供了更多选项来平衡后端 Pod 的流量。 这些是： rr：轮替（Round-Robin） lc：最少链接（Least Connection），即打开链接数量最少者优先 dh：目标地址哈希（Destination Hashing） sh：源地址哈希（Source Hashing） sed：最短预期延迟（Shortest Expected Delay） nq：从不排队（Never Queue） 注意：当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。 5.4 ClusterIPClusterIP 主要在每个 Node 节点使用 iptables / IPVS，将发向 ClusterIP 对应端口的数据，转发到 kube-proxy 中，kube-proxy 内部可以实现负载均衡，并可以查询到该 Service 下对应 Pod 的地址和端口，进而把数据转发给对应的 Pod 的地址和端口。 示例 创建 Deployment 1234567891011121314151617181920212223242526vim nginx_svc_deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment namespace: defaultspec: replicas: 3 selector: # 创建两个标签 matchLabels: app: nginx version: latest template: metadata: labels: app: nginx version: latest spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest # 释放的端口 ports: - name: http-port containerPort: 80 创建 Service 12345678910111213141516vim clusterip_svc.yamlapiVersion: v1kind: Servicemetadata: name: nginx-clusterip-service namespace: defaultspec: type: ClusterIP # 用来匹配Pod中的标签，全部匹配才会管理该Pod selector: app: nginx version: latest ports: - name: http-port port: 80 targetPort: 80 可以看到 Deployment 和 SVC 都已经创建成功，且直接访问 SVC 的 ClusterIP 地址就可以访问到 Pod 了 测试负载均衡，在各容器内部创建一个 html 页面 1234kubectl exec -it nginx-deployment-*** -- /bin/bashecho 'this is node01-pod01' &gt; /usr/share/nginx/html/pod.htmlecho 'this is node02-pod01' &gt; /usr/share/nginx/html/pod.html... 可以看到是实现了负载均衡的 5.5 无头服务（Headless Services）有时不需要或不想要负载均衡，以及单独的 Service IP。 遇到这种情况，可以通过指定 Cluster IP（spec.clusterIP）的值为 &quot;None&quot; 来创建 Headless Service。 可以使用无头 Service 与其他服务发现机制进行接口，而不必与 Kubernetes 的实现捆绑在一起。 对这无头 Service 并不会分配 Cluster IP，kube-proxy 不会处理它们， 而且平台也不会为它们进行负载均衡和路由。 DNS 如何实现自动配置，依赖于 Service 是否定义了选择算符。 示例 创建 Headless Service 1234567891011121314vim headless_service.yamlapiVersion: v1kind: Servicemetadata: name: headless-service namespace: defaultspec: selector: app: nginx version: latest clusterIP: &quot;None&quot; ports: - port: 80 targetPort: 80 可以看到是没有分配 VIP 的 安装 bind-utils 工具来测试无头服务的作用，可以看到即使没有了 VIP，但依旧可以通过域名来访问到不同的 Pod 123456yum -y install bind-utils# k8s内部使用dns访问格式：SVC名称.命名空间.svc.集群域(默认集群域为cluster.local)# dig命令可以用来获取域名的详细信息# 10.244.0.37是其中一个coredns的地址# -t A:显示A记录，A记录是将域名指向一个IPv4地址，即一个域名解析到一个IP地址dig -t A headless-service.default.svc.cluster.local @10.244.0.37 5.6 NodePortNodePort 的原理在于在 Node 上开放了一个端口，将该端口的流量导入到 kube-proxy 中，然后再由 kube-proxy 传送给不同的 Pod。 示例 创建 Service，这里的标签仍与上面 ClusterIP 中的 Deployment 创建的 Pod 相匹配 123456789101112131415vim nodeport_svc.yamlapiVersion: v1kind: Servicemetadata: name: nginx-nodeport-service namespace: defaultspec: type: NodePort selector: app: nginx version: latest ports: - name: http-port port: 80 targetPort: 80 可以看到会暴露一个端口，外部通过这个端口就可以访问到内部的 Pod，且是每一个 Node 都开启了这个端口，所以也可以实现负载均衡 在 iptables / IPVS 规则中可以看到开启该端口的规则 12iptables -t nat -nvl | grep 31419ipvsadm -Ln | grep 31419 5.7 LoadBalancerLoadBalancer 就是在 NodePort 的基础上，通过 LAAS 来实现负载均衡，用户指要访问 LAAS即可，LAAS 会将请求通过调度转发给不同的 Node。 5.8 ExternalNameExternalName 通过返回 CNAME 和它的值，将服务映射到 ExternalName 字段的内容，ExternalName 没有 selector，也没有端口的设置，对于运行在集群之外的服务，ExternalName 是通过该外部服务的别名来提供服务的。 当这个 Service 创建成功时，就会有 externalname-service.default.svc.cluster.local 的 fqdn 被创建，如果有用户访问到这个 fqdn，就会被改写成 my.database.example.com，这就是 DNS 内部的一个 CNAME 记录，也就是别名记录。 示例 创建测试用的 Pod 123456789101112apiVersion: v1kind: Podmetadata: name: curl-podspec: containers: - name: curl-pod-container # curl镜像包含测试网络和DNS的工具 image: docker.io/appropriate/curl imagePullPolicy: IfNotPresent command: ['sh', '-c'] args: ['echo &quot;curl test&quot;; sleep 3600'] 创建 ExternalName 12345678apiVersion: v1kind: Servicemetadata: name: externalname-svcspec: type: externalname # 引入百度的网址 externalName: www.baidu.com 进入 Pod 内部测试可以看到，通过 nslookup 可以解析到百度的地址 1nolookup SVC名称.命名空间.svc.集群域 5.9 ingress5.9.1 什么是ingressingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。 ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。 ingress 由两部分构成： ingress controller：将新加入的 ingress 转化成 Nginx 的配置文件并使之生效。 ingress 服务：将 Nginx 的配置抽象成一个 ingress 对象，每添加一个新的服务只需写一个新的 ingress 的 yaml 文件即可。 ingress controller 主要有两种，nginx-ingress和traefik-ingress，这里主要讲nginx-ingress。 ingress 官方网址：https://kubernetes.github.io/nginx-ingress ingress GitHub 网址：https://github.com/kubernetes/nginx-ingress nginx-ingress功能 nginx-ingress主要负责向外暴露服务，同时提供负载均衡的功能。 Nginx 对后端运行的服务（Service1、Service2）提供反向代理，在配置文件中配置了域名与后端服务 Endpoints 的对应关系。客户端通过使用 DNS 服务或者直接配置本地的 hosts 文件，将域名都映射到 Nginx 代理服务器。当客户端访问 service1.com 时，浏览器会把包含域名的请求发送给 Nginx 服务器，Nginx 服务器根据传来的域名，选择对应的 Service，这里就是选择 Service1 后端服务，然后根据一定的负载均衡策略，选择 Service1 中的某个容器接收来自客户端的请求并作出响应。过程很简单，Nginx 在整个过程中仿佛是一台根据域名进行请求转发的“路由器”。 nginx-ingress工作过程 nginx-ingress模块在运行时主要分为三个主体： Store：Store 会与 APIServer 以协程的 Pod 方式进行一个监听状态，发生新的事件会写入循环队列里。 NginxController：NginxController 会监听循环队列里的事件，发生一个循环就会更新一个事件，并写入 SyncQueue 里。 SyncQueue：SyncQueue 协程会定期拉取需要执行的任务（如果有必要的则直接从 Store 拉取过来进行修改），接着判断是否需要 reload nginx，最后会以 nginx 模块运行。 5.9.2 ingress规则每个 HTTP 规则都包含以下信息： 可选的 host。在此示例中，未指定 host，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 host（例如 www.cqm.com），则 rules 适用于该 host。 路径列表 paths（例如，/testpath）,每个路径都有一个由 serviceName 和 servicePort 定义的关联后端。 在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。 backend（后端）是 Service 文档 中所述的服务和端口名称的组合。 与规则的 host 和 path 匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的 backend。 路径类型 Ingress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 pathType 的路径无法通过合法性检查。当前支持的路径类型有三种： ImplementationSpecific：对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。 Exact：精确匹配 URL 路径，且区分大小写。 Prefix：基于以 / 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 / 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。 5.9.3 部署nginx-ingress 通过 Bare-metal（NodePort） 方式部署，先下载 yaml 文件 123456789# 地址一# https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.46.0/deploy/static/provider/baremetal/deploy.yaml# 地址二# https://github.com/kubernetes/ingress-nginx/blob/master/deploy/static/provider/baremetal/deploy.yaml# 由于镜像地址也在国外，所以通过国内镜像源拉取docker pull registry.aliyuncs.com/google_containers/nginx-ingress-controller:v0.46.0docker tag registry.aliyuncs.com/google_containers/nginx-ingress-controller:v0.46.0 k8s.gcr.io/ingress-nginx/controller:v0.46.0 获取镜像地址，下载好需要的镜像，并导入镜像到其它 node 上 通过 deploy.yaml 文件生成 svc 1kubectl create -f deploy.yaml 5.9.4 ingress HTTP代理访问 ingress-nginx会根据配置好的 yaml 文件，自动配置 nginx.conf 和虚拟主机文件。 创建 deployment1、deployment2、service1、service2 1234567891011121314151617181920212223242526272829303132333435vim nginx_deployment_svc1.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment1spec: replicas: 2 selector: matchLabels: name: nginx1 template: metadata: labels: name: nginx1 spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service1spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx1 1234567891011121314151617181920212223242526272829303132333435vim nginx_deployment_svc2.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment2spec: replicas: 2 selector: matchLabels: name: nginx2 template: metadata: labels: name: nginx2 spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service2spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx2 创建 nginx-ingress1、nginx-ingress2 123456789101112131415161718192021222324252627282930313233343536373839vim nginx_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: nginx-ingress1spec: rules: # 设置虚拟主机，通过该域名访问 - host: www.cqm1.com http: paths: # 路径列表 - path: /pod.html # 路径类型 pathType: ImplementationSpecific # 后端 backend: # 指定连接哪个SVC serviceName: nginx-service1 servicePort: 80---apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: nginx-ingress2spec: rules: - host: www.cqm2.com http: paths: - path: /pod.html pathType: ImplementationSpecific backend: service: name: nginx-service2 port: number: 80 创建 123kubectl create -f nginx_deployment_svc1.yamlkubectl create -f nginx_deployment_svc2.yamlkubectl create -f nginx_ingress.yaml 给每个 Pod 写入信息，方便查看负载均衡 12kubectl exec -it nginx-deployment-... -- /bin/bashecho 'this is nginx-pod-1' &gt; /usr/share/nginx/html/pod.html 查看 ingress-nginx 所创建的 svc 暴露的端口，以及 service1、service2、nginx-ingress1、nginx-ingress2 123kubectl get svc -n ingress-nginxkubectl get svckubectl get ingress 在 /etc/hosts 文件下写入域名与 IP 绑定，访问测试，可以看到实现了负载均衡 可以进入 ingress 控制器内部查看 nginx 配置，可以看到自动添加的代理配置 12kubectl exec -it -n ingress-nginx ingress-nginx-controller-... -- /bin/bashcat nginx.conf 5.9.5 ingress HTTPS代理访问 创建私钥 12openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=nginxsvc/O=nginxsvc&quot;kubectl create secret tls tls-secret --key tls.key --cert tls.crt 创建 deployment3、service3、nginx-ingress3 1234567891011121314151617181920212223242526272829303132333435vim nginx_deployment_svc3.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment3spec: replicas: 2 selector: matchLabels: name: nginx3 template: metadata: labels: name: nginx3 spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-service3spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx3 12345678910111213141516171819vim nginx_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: nginx-ingress3spec: tls: - hosts: - www.cqm3.com secretName: tls-secret rules: - host: www.cqm3.com http: paths: - path: /pod.html pathType: ImplementationSpecific backend: serviceName: nginx-service3 servicePort: 80 创建 1kubectl create -f nginx_deployment_svc3.yaml nginx_ingress.yaml 在每个 Pod 写入 pod.html，便于查看负载均衡效果，并测试 12kubectl exec -it ingress-nginx-... -- /bin/bashecho 'this is node01-pod' &gt; /usr/share/nginx/html/pod.html 5.9.5 Nginx进行基础认证（BasicAuth） 通过 Apache 创建用户认证文件 123yum -y install httpdhtpasswd -c auth cqmkubectl create secret generic basic-auth --from-file=auth 创建 ingress 1234567891011121314151617181920212223vim nginx_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-nginx-auth # 添加基础认证字段 annotations: nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: basic-auth nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - cqm'spec: rules: - host: auth.cqm.com http: paths: - path: /pod.html pathType: ImplementationSpecific backend: service: # 这里使用HTTP代理实验的service1 name: nginx-service1 port: number: 80 创建 ingress 后测试 5.9.6 Nginx重写 名称 描述 类型 nginx.ingress.kubernetes.io/rewrite-target 必须重定向流量的目标 URI string nginx.ingress.kubernetes.io/ssl-redirect 指示位置部分是否仅可访问 SSL（当 Ingress 包含证书时默认为 True） bool nginx.ingress.kubernetes.io/force-ssl-redirect 即使 Ingress 未启用 TLS，也强制重定向到 HTTPS bool nginx.ingress.kubernetes.io/app-root 定义控制器必须重定向的应用程序根，如果它在“/”上下文中 string nginx.ingress.kubernetes.io/use-regex 指示 Ingress 上定义的路径是否使用正则表达式 bool 先准备好转发后的 deployment、ingress等，这里用上边的 ingress1，用户访问 www.rewritecqm.com 时就跳转到 www.cqm1.com 编写重写 ingress 1234567891011121314151617181920vim rewrite_ingress.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: rewrite-ingress annotations: # 访问 www.rewritecqm.com:30779 将跳转到 http://www.cqm1.com:30779/pod.html nginx.ingress.kubernetes.io/rewrite-target: http://www.cqm1.com:30779/pod.htmlspec: rules: - host: www.rewritecqm.com http: paths: - path: /pod.html pathType: Prefix backend: service: name: nginx-service1 port: number: 80 测试 六、k8s存储6.1 配置存储卷配置存储卷并不是用来进行容器间相互交互或 Pod 间数据共享的，而是用于向各个 Pod 注入配置信息的，主要分为以下三种： ConfigMap：可传递普通信息 Secret：可传递密码等敏感的配置信息 DownwardAPI：可传递 Pod 和容器自身的运行信息 6.1.1 ConfigMap许多应用程序都会从配置文件、命令行参数或环境变量中读取配置信息。 ConfigMap API 给我们提供了向容器内部注入信息的机制，ConfigMap 可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制对象。 ConfigMap 的创建方式有三种，分别为基于目录、文件和字面值来创建。 基于目录创建 ConfigMap 创建指定目录，下载官方测试文件 1234mkdir -p /root/k8s/plugin/configmap/dir || cd /root/k8s/plugin/configmap/dirwget https://kubernetes.io/examples/configmap/game.propertieswget https://kubernetes.io/examples/configmap/ui.propertieskubectl create configmap configmap1 --from-file=./ 查看命令 12kubectl describe cm configmap1kubectl get cm configmap1 -o yaml 基于文件创建 ConfigMap 通过 game.properties 和 ui.properties 创建 1kubectl create configmap configmap2 --from-file=game.properties --from-file=ui.properties 基于字面值创建 ConfigMap 通过--from-literal=键名=键值来创建 1kubectl create configmap configmap3 --from-literal=special.how=very 6.1.1.1 Pod中使用ConfigMap使用 ConfigMap 代替环境变量 创建两个 ConfigMap 123456789vim special_cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: special-cm namespace: defaultdata: special.how: very special.type: charm 12345678vim env_cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: env-cm namespace: defaultdata: log_level: INFO 将这两个 ConfigMap 注入到 Pod中 1234567891011121314151617181920212223242526272829303132vim pod1.yamlapiVersion: v1kind: Podmetadata: name: pod1spec: containers: - name: pod1-container image: busybox imagePullPolicy: IfNotPresent command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;env&quot;] # env:取某个 ConfigMap 中的某个键值 env: - name: SPECIAL_HOW_KEY # 定义环境变量 valueFrom: # 表示从 ConfigMap 中引用 configMapKeyRef: # 要引用的 ConfigMap 名称 name: special-cm # 引用 ConfigMap 中的哪个键值对 key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-cm key: special.type # envFrom:取某个 ConfigMap 的所有键值 envFrom: - configMapRef: name: env-cm restartPolicy: Never 生成 Pod 后查看日志，可以看到注入成功 1kubectl logs pod1 使用 ConfigMap 设置命令行参数 1234567891011121314151617181920212223242526vim pod2.yamlapiVersion: v1kind: Podmetadata: name: pod2spec: containers: - name: pod2-container image: busybox imagePullPolicy: IfNotPresent command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo $(SPECIAL_HOW_KEY) $(SPECIAL_TYPE_KEY)&quot;] env: - name: SPECIAL_HOW_KEY valueFrom: configMapKeyRef: name: special-cm key: special.how - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-cm key: special.type envFrom: - configMapRef: name: env-cm restartPolicy: Never 将 ConfigMap 数据添加到一个卷中 创建 Pod 123456789101112131415161718192021vim pod3.yamlapiVersion: v1kind: Podmetadata: name: pod3spec: containers: - name: pod3-container image: busybox imagePullPolicy: IfNotPresent command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 1200&quot; ] # 挂载config-volume，挂载在/etc/config目录下 volumeMounts: - name: config-volume mountPath: /etc/config # 创建一个卷，将special-cm内容写入config-volume卷 volumes: - name: config-volume configMap: name: special-cm restartPolicy: Never 进入 Pod，在挂载目录下查看是否写入到 config 文件里 6.1.1.2 ConfigMap热更新本例子通过 ConfigMap 来实现热更新，可以实现热更新 nginx.conf，但需进入容器内部重载配置文件，所以通过热更新一个 html 来展示效果。 编写 yaml 文件，并创建 123456789101112131415161718192021222324252627282930313233343536373839404142vim hotupdate.yaml# ConfigMapapiVersion: v1kind: ConfigMapmetadata: name: nginx-hotupdate-cm namespace: defaultdata: test.html: 'this is the fist test'---# DeploymentapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment namespace: defaultspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest ports: - containerPort: 80 # 挂载下边创建的volume volumeMounts: - name: test-html-volume mountPath: /usr/share/nginx/html # 创建一个volume，通过ConfigMap方式挂载，使用上面创建的nginx-hotupdate-cm volumes: - name: test-html-volume configMap: name: nginx-hotupdate-cm 1kubectl apply -f hotupdate.yaml 测试是否能够访问到 修改 ConfigMap，并再次访问 1kubectl edit cm nginx-hotupdate-cm 6.1.2 SecretSecret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。 Secret 的类型 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 6.1.2.1 OpaqueOpaque 使用base64编码存储信息，可以通过 base64 --decode 解码获得原始数据，因此安全性弱。 使用示例 生成 base64 编码 1234echo 'admin' | base64YWRtaW4Kecho '12345' | base64MTIzNDUK 编写 yaml 文件 123456789vim opaque.yamlapiVersion: v1kind: Secretmetadata: name: secret-opaquetype: Opaquedata: username: YWRtaW4K password: MTIzNDUK 1kubectl apply -f opaque.yaml 挂载到 volume 中使用，查看测试可以看到挂载后的信息被解码了 12345678910111213141516171819vim opaque-pod1.yamlapiVersion: v1kind: Podmetadata: name: opaque-pod1spec: containers: - name: opaque-pod1-nginx image: daocloud.io/library/nginx:latest # 挂载下边创建的volume-opaque volumeMounts: - name: volume-opaque mountPath: /etc/secrets readOnly: yes # 创建一个secret类型的volume，引用上边创建好的secret-opaque volumes: - name: volume-opaque secret: secretName: secret-opaque 导入到环境变量中并测试 123456789101112131415161718192021222324252627282930313233343536vim opaque-pod2.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: opaque-deploymentspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: name: opaque-pod2 labels: app: nginx spec: containers: - name: opaque-pod2-nginx image: daocloud.io/library/nginx:latest ports: - containerPort: 80 # 将secret导入环境变量中 env: # 命名变量 - name: USER # 引用secret-opaque中的username的键值 valueFrom: secretKeyRef: name: secret-opaque key: username - name: PASSWORD # 引用secret-opaque中的password的键值 valueFrom: secretKeyRef: name: secret-opaque key: password 6.1.2.2 ImagePullSecret可以使用下面两种 type 值之一来创建 Secret，用以存放访问 Docker 仓库来下载镜像的凭据。 kubernetes.io/dockercfg kubernetes.io/dockerconfigjson kubernetes.io/dockerconfigjson 创建 Secret 示例 创建 Secret 1kubectl create secret docker-registry cqmregistry --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL 在 Pod 中运用 1234567891011apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx-pod image: daocloud.io/library/nginx:latest # 调用创建好的Secret imagePullSecrets: - name: cqmregistry 6.1.2.3 Downward API有时候 Pod 需要获取自身的信息，这时候 Downward API 就派上用场了。 Downward API 是通过 fieldRef 参数获取信息的。 示例一 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: test-pod1spec: containers: - name: test-pod1-container image: busybox imagePullPolicy: IfNotPresent command: [ 'sh', '-c' ] args: [ 'echo &quot;EnvPodName:${EnvPodName} EnvNodeName:${EnvNodeName}&quot;, sleep 3600' ] env: - name: EnvPodName valueFrom: fieldRef: fieldPath: metadata.name - name: EnvNodeName valueFrom: fieldRef: fieldPath: spec.nodeName 示例二 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: test-pod2spec: containers: - name: test-pod2-container image: busybox imagePullPolicy: IfNotPresent command: [ 'sh', '-c' ] args: [ 'echo &quot;EnvPodName:${EnvPodName} EnvNodeName:${EnvNodeName}&quot;, sleep 3600' ] volumeMounts: - name: test-volume mountPath: /test volumes: - name: test-volume downwardAPI: items: - path: 'PodName' fieldRef: fieldPath: metadata.name - path: 'NodeName' fieldRef: fieldPath: spec.nodeName 6.2 本地存储卷Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用程序带来一些问题。问题之一是当容器崩溃时文件丢失。kubelet 会重新启动容器， 但容器会以干净的状态重启。 第二个问题会在同一 Pod 中运行多个容器并共享文件时出现。 Kubernetes Volume 这一抽象概念能够解决这两个问题。 Kubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 当 Pod 不再存在时，Kubernetes 也会销毁临时卷；不过 Kubernetes 不会销毁持久卷。对于给定 Pod 中任何类型的卷，在容器重启期间数据都不会丢失。 卷的类型分为很多种，可通过官方文档查看：https://kubernetes.io/zh/docs/concepts/storage/volumes/ 6.2.1 emptyDir当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。 需要注意的是，容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。 emptyDir 的一些用途： 缓存空间，例如基于磁盘的归并排序。 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行。 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件。 编写 yaml 文件 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: emptydir-podspec: containers: - name: emptydir-container1 image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /empty1 name: emptydir-volume - name: emptydir-container2 image: busybox imagePullPolicy: IfNotPresent command: ['/bin/sh','-c','sleep 6000'] volumeMounts: - mountPath: /empty2 name: emptydir-volume volumes: - name: emptydir-volume emptyDir: {} 进入 Pod 中的不同容器查看是否共享同一个 volume， 6.2.2 hostPathhostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中，类似 docker 中的 volume 挂载。 hostPath 的一些用法有： 运行一个需要访问 Docker 内部机制的容器；可使用 hostPath 挂载 /var/lib/docker 路径。 在容器中运行 cAdvisor（容器监控工具） 时，以 hostPath 方式挂载 /sys。 允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。 除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。 支持的 type 值如下： 取值 行为 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。 DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 Directory 在给定路径上必须存在的目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 File 在给定路径上必须存在的文件。 Socket 在给定路径上必须存在的 UNIX 套接字。 CharDevice 在给定路径上必须存在的字符设备。 BlockDevice 在给定路径上必须存在的块设备。 注意：具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为，即假如同一个 template 创建出来的 Pod 分配在了不同的 Node 上时，会因为节点的不同而产生不同的行为。 编写 yaml 文件 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: hostpath-podspec: containers: - name: hostpath-pod-container image: daocloud.io/library/nginx:latest volumeMounts: - mountPath: /test name: hostpath-volume volumes: - name: hostpath-volume hostPath: # 宿主机被挂载目录 path: /data # 如果没有/data目录则会被创建 type: DirectoryOrCreate 查看是否挂载成功 需要注意的是，在FileOrCreate下，如果被挂载的目录不存在，那么不会自动创建该目录， 为了确保这种模式能够工作，可以尝试把文件和它对应的目录分开挂载。 hostPath FileOrCreate 配置示例 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: hostpath-fileorcreate-podspec: containers: - name: hostpath-fileorcreate-pod-container image: daocloud.io/library/nginx:latest volumeMounts: - mountPath: /test name: mydir - mountPath: /test/test.txt name: myfile volumes: - name: mydir hostPath: path: /data # 如果宿主机没有/data目录则会自动创建 type: DirectoryOrCreate - name: myfile hostPath: path: /data/test.txt # 因为上面已经确保会有/data目录，所以该文件的挂载不受影响 type: FileOrCreate 6.3 持久存储卷6.3.1 PV和PVCPV 和 PVC 是 k8s 提供的两个 api 资源。 PV 持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者使用存储类来动态供应，PV 是集群资源，和普通的 Volume 一样，也是使用卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 PVC 持久卷申领（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。Pod 会耗用 Node 资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存），同样 PVC 申领也可以请求特定的大小和访问模式。 PV 的供应方式有两种： 静态供应 集群管理员创建若干 PV 卷。这些卷对象带有真实存储的细节信息，并且对集群用户可用（可见）。PV 卷对象存在于 Kubernetes API 中，可供用户消费（使用）。 动态供应 如果管理员所创建的所有静态 PV 卷都无法与用户的 PersistentVolumeClaim 匹配， 集群可以尝试为该 PVC 申领动态供应一个存储卷。 这一供应操作是基于 StorageClass 来实现的：PVC 申领必须请求某个 存储类，同时集群管理员必须 已经创建并配置了该类，这样动态供应卷的动作才会发生。 如果 PVC 申领指定存储类为 &quot;&quot;，则相当于为自身禁止使用动态供应的卷。 为了基于存储类完成动态的存储供应，集群管理员需要在 API 服务器上启用 DefaultStorageClass 准入控制器。 举例而言，可以通过保证 DefaultStorageClass 出现在 API 服务器组件的 --enable-admission-plugins 标志值中实现这点；该标志的值可以是逗号 分隔的有序列表。关于 API 服务器标志的更多信息，可以参考 kube-apiserver 文档。 绑定 通俗理解就是一旦 PV 与 PVC 进行了绑定，那么该 PV 就无法与其它 PVC 进行绑定了。 保护 当一个 PV 与 PVC 绑定之后，假设 Pod 被删除，那么该 PV 与 PVC 依旧会是一个绑定的关系。 持久卷的类型 PV 持久卷是用插件的形式来实现的。Kubernetes 目前支持以下插件： awsElasticBlockStore - AWS 弹性块存储（EBS） azureDisk - Azure Disk azureFile - Azure File cephfs - CephFS volume csi - 容器存储接口 (CSI) fc - Fibre Channel (FC) 存储 flexVolume - FlexVolume flocker - Flocker 存储 gcePersistentDisk - GCE 持久化盘 glusterfs - Glusterfs 卷 hostPath - HostPath 卷 （仅供单节点测试使用；不适用于多节点集群； 请尝试使用 local 卷作为替代） iscsi - iSCSI (SCSI over IP) 存储 local - 节点上挂载的本地存储设备 nfs - 网络文件系统 (NFS) 存储 portworxVolume - Portworx 卷 quobyte - Quobyte 卷 rbd - Rados 块设备 (RBD) 卷 storageos - StorageOS 卷 vsphereVolume - vSphere VMDK 卷 访问模式 PV 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。 如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为 对应卷所支持的模式值。 例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器 上以只读的方式导出。每个 PV 卷都会获得自身的访问模式集合，描述的是 特定 PV 卷的能力。 访问模式有： ReadWriteOnce：卷可以被一个节点以读写方式挂载； ReadOnlyMany：卷可以被多个节点以只读方式挂载； ReadWriteMany：卷可以被多个节点以读写方式挂载。 对于不同类型的存储卷访问模式也有不同，如下表 卷插件 ReadWriteOnce ReadOnlyMany ReadWriteMany AWSElasticBlockStore ✓ - - AzureFile ✓ ✓ ✓ AzureDisk ✓ - - CephFS ✓ ✓ ✓ Cinder ✓ - - CSI 取决于驱动 取决于驱动 取决于驱动 FC ✓ ✓ - FlexVolume ✓ ✓ 取决于驱动 Flocker ✓ - - GCEPersistentDisk ✓ ✓ - Glusterfs ✓ ✓ ✓ HostPath ✓ - - iSCSI ✓ ✓ - Quobyte ✓ ✓ ✓ NFS ✓ ✓ ✓ RBD ✓ ✓ - VsphereVolume ✓ - - (Pod 运行于同一节点上时可行) PortworxVolume ✓ - ✓ ScaleIO ✓ ✓ - StorageOS ✓ - - 类 每个 PV 可以属于某个类（Class），通过将其 storageClassName 属性设置为某个 StorageClass 的名称来指定。 特定类的 PV 卷只能绑定到请求该类存储卷的 PVC 申领。 未设置 storageClassName 的 PV 卷没有类设定，只能绑定到那些没有指定特定存储类的 PVC 申领。 回收策略 目前的回收策略有： Retain（保留） – 手动回收 Recycle（回收）– 基本擦除 (rm -rf /thevolume/*) Delete（删除）– 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除 目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。 阶段（状态） 每个卷会处于以下阶段（Phase）之一： Available（可用）– 卷是一个空闲资源，尚未绑定到任何申领； Bound（已绑定）– 该卷已经绑定到某申领； Released（已释放）– 所绑定的申领已被删除，但是资源尚未被集群回收； Failed（失败）– 卷的自动回收操作失败。 示例一 这个实例是先后创建 PV、PVC、Deployment 创建 PV 1234567891011121314apiVersion: v1kind: PersistentVolumemetadata: name: test-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /nfs server: 192.168.88.100 创建 PVC 1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: test-pvcspec: accessModes: - ReadWriteMany storageClassName: nfs resources: requests: storage: 1Gi 创建 Deployment 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: Deploymentmetadata: name: test-deploymentspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: name: test-deployment-pod labels: app: nginx spec: containers: - name: test-deployment-pod-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent volumeMounts: - name: test-volume mountPath: /usr/share/nginx/html volumes: - name: test-volume persistentVolumeClaim: claimName: test-pvc 示例二 这个实例会由 StatefulSet 自动创建 PVC 部署 NFS 服务器，并在每个节点安装nfs-utils 部署 PV，这里创建了四个 PV 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv1spec: # PV容量大小 capacity: storage: 1Gi # 服务模式为RWO，卷可以被一个节点以读写方式挂载 accessModes: - ReadWriteOnce # 回收策略为Retain，即手动回收 persistentVolumeReclaimPolicy: Retain # 类为nfs storageClassName: nfs # 制定nfs服务器地址和被挂载目录 nfs: path: /nfs1 server: 192.168.88.100---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv2spec: capacity: storage: 2Gi accessModes: - ReadOnlyMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /nfs2 server: 192.168.88.100---apiVersion: v1kind: PersistentVolumemetadata: name: nfs-pv3spec: capacity: storage: 3Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /nfs3 server: 192.168.88.100---apiVersion: v1kind: PersistentVolumemetadata: name: slow-pvspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: slow nfs: path: /slow server: 192.168.88.100 创建无头 SVC、StatefulSet 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657apiVersion: v1kind: Servicemetadata: name: nfs-pv-svc labels: app: nginxspec: ports: - port: 80 targetPort: 80 # 不分配ip地址，为StatefulSet使用 clusterIP: None # 匹配标签 selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: nfs-pv-statefulsetspec: # 匹配标签 selector: matchLabels: app: nginx serviceName: nfs-pv-svc replicas: 3 template: metadata: name: nfs-pv-pod # pod标签 labels: app: nginx spec: containers: - name: nfs-pv-pod-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 挂载持久卷 volumeMounts: - name: nfs-pv-volume mountPath: /usr/share/nginx/html # PVC模板 volumeClaimTemplates: - metadata: name: nfs-pv-volume spec: # 匹配规则 accessModes: [ 'ReadWriteOnce' ] storageClassName: 'nfs' resources: requests: storage: 1Gi 先后创建后查看 Pod 创建情况，可以看到只创建了一个 Pod，因为现有的 PV 只有一个符合匹配条件，而 StatefulSet 是前一个 Pod 创建成功才会创建下一个，因为第二个 Pod 创建不成功，所以第三个 Pod 不会被创建 进入容器内部可以看到挂载成功，去到 NFS 服务器的/nfs1目录下创建test.html文件测试成功 删除创建失败的 Pod 以及对应的 PVC，创建新的两个 PV，以满足 StatefulSet 的挂载要求，剩下的 Pod 就会逐一创建成功 删除上面测试的 Pod，StatefulSet 会重新创建一个 Pod，且数据不会丢失 这里会有个问题，如果删除了 StatefulSet，那么对应的 Pod 也会被删除，可是已经绑定的 PV 并不会删除，这里就需要手动回收了。 手动回收 删除 StatefulSet 删除 PVC 修改 PV 1234kubectl edit pv pv名称# 删除ClaimRef的信息ClainRef:... 6.3.2 StorageClass以上的方法都是静态创建的 PV，会出现 PVC 找不到条件符合的 PV 进行绑定。 而 StorageClass 的作用是根据 PVC 的需求动态创建 PV。 示例一 k8s 在 1.20 版本开始就禁用了 selfLink，所以需在配置文件添加以下内容 StorageClass 是通过存储分配器来动态创建 PV 的，但 k8s 内部的存储分配器不支持 NFS，所以首先要安装 NFS 存储分配器 12345678910111213141516171819202122232425262728293031323334353637383940vim nfs_provisioner.yamlkind: DeploymentapiVersion: apps/v1metadata: name: nfs-client-provisionerspec: replicas: 1 selector: matchLabels: app: nfs-client-provisioner strategy: # 设置升级策略为删除再创建(默认为滚动更新) type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /nfs-provision env: # nfs存储分配器名称 - name: PROVISIONER_NAME value: nfs-client # nfs服务器地址 - name: NFS_SERVER value: 192.168.88.100 # nfs共享目录 - name: NFS_PATH value: /nfs volumes: - name: nfs-client-root nfs: server: 192.168.88.100 path: /nfs 给 NFS 存储分配器授权 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061vim nfs_provisioner_rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner namespace: default---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner namespace: defaultrules: - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner namespace: defaultsubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 创建 StorageClass 1234567891011vim nfs_storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: # storageclass名称 name: nfs-storageclass# nfs存储分配器名称provisioner: nfs-clientparameters: # 表示pvc删除时，所绑定的pv不会被保留，true相反 archieveOnDelete: 'false' 创建 PVC，可以看到 NFS 存储分配器已经自动创建了 PV 与之绑定 12345678910111213vim nfs_storageclass_pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-storageclass-pvcspec: accessModes: - ReadWriteMany # storageclass名称 storageClassName: 'nfs-client' resources: requests: storage: 500Mi 创建一个 Deployment 测试是否能用这个 PVC 1234567891011121314151617181920212223242526vim nfs_provisioner_deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nfs-provisioner-deploymentspec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nfs-provisioner-deployment-nginx image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent volumeMounts: - name: nfs-storageclass-pvc mountPath: /usr/share/nginx/html/test volumes: - name: nfs-storageclass-pvc persistentVolumeClaim: claimName: nfs-storageclass-pvc 在 NFS 服务器的共享目录下创建一个 test.html 访问 Deployment 创建出来的 Pod，可以看到是可以访问的 6.4 StatefulSetStatefulSet 是一种提供排序和唯一性保证的特殊 Pod 控制器，当有部署顺序、持久数据或固定网络等相关的特殊需求时，可以用 StatefulSet 控制器来进行控制。 StatefulSet 提供有状态服务，主要功能如下： 实现稳定的持久化存储：通过 PVC 来实现，Pod 之间不能共用一个存储卷，每个 Pod 都要有一个自己专用的存储卷。 实现稳定的网络标识：Pod 重新调度后其 PodName 和 HostName 不变，通过无头 SVC 来实现。 实现有序部署、有序伸缩：Pod 是有顺序的，只有前一个 Pod 创建成功才会创建下一个，直到最后。 实现有序收缩、有序删除：从最后一个开始，依次删除到第一个。 无头 SVC ：为 Pod 生成可以解析的 DNS 记录。 示例一 创建无头 SVC 1234567891011121314vim statefulset_nginx_svc.yamlapiVersion: v1kind: Servicemetadata: name: statefulset-nginx-svcspec: selector: app: nginx clusterIP: None ports: - protocol: TCP port: 8080 targetPort: 80 type: ClusterIP 创建 StatefulSet 12345678910111213141516171819202122232425262728293031323334vim statefulset_nginx.yamlapiVersion: apps/v1kind: StatefulSetmetadata: name: statefulset-nginxspec: # 匹配无头SVC serviceName: statefulset-nginx-svc replicas: 4 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: statefulset-nginx-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 # PVC模板 volumeClaimTemplates: - metadata: name: statefulset-nginx-pvc spec: accessModes: [ 'ReadWriteOnce' ] storageClassName: 'nfs-storageclass' resources: requests: storage: 50Mi 可以看到 Pod 是有序创建的，且每个 Pod 都是单独使用一个 PVC 和 PV，删除 StatefulSet 也可以看到 Pod 是有序删除的，且删除后 PVC 与 PV 依旧存在，重新生成 StatefulSet 可以继续使用这些 PVC 和 PV 有序创建 PVC 与 PV 有序删除 创建一个 Pod 用来测试无头 SVC 提供的 DNS 服务 1234567891011apiVersion: v1kind: Podmetadata: name: test-podspec: containers: - name: test-pod-container image: appropriate/curl:latest imagePullPolicy: IfNotPresent command: [ 'sh', '-c' ] args: [ 'echo &quot;this is test&quot;; sleep 36000' ] 进入 Pod 后可以通过nslookup测试，形式为{ServiceName}.{NameSpace}.svc.{ClusterDomain} 通过域名也可以访问各个 Pod，形式为{PodName}{ServiceName}.{NameSpace}.svc.{ClusterDomain} 七、k8s调度器scheduler 是 k8s 集群的调度器，对每一个新创建的 Pod 或者是未被调度的 Pod，scheduler 会选择一个最优的 Node 去运行这个 Pod。然而，Pod 内的每一个容器对资源都有不同的需求，而且 Pod 本身也有不同的资源需求。因此，Pod 在被调度到 Node 上之前， 根据这些特定的资源调度需求，需要对集群中的 Node 进行一次过滤。 在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、 亲和以及反亲和要求、数据局域性、负载间的干扰等等。 调度流程： 过滤：调度器会将所有满足 Pod 调度需求的 Node 选出来。 打分：调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node。 7.1 亲和性与反亲和性7.1.1 Node亲和性节点亲和性是通过pod.spec.nodeAffinity来实现的，包括以下两种： preferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 requiredDuringSchedulingIgnoredDuringExecution硬策略 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: node-required-podspec: containers: - name: node-required-pod-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent # 亲和性设置 affinity: # 节点亲和性设置 nodeAffinity: # 硬策略 requiredDuringSchedulingIgnoredDuringExecution: # 节点选择器 nodeSelectorTerms: # 匹配表达式 - matchExpressions: # 制定key，即以这个key的键值为匹配条件 # 通过kubectl get node --show-labels可以获得更多标签 - key: kubernetes.io/hostname # In:label的值在某个列表中 # NotIn:label的值不在某个列表中 # Gt:label的值大于某个值 # Lt:label的值小于某个值 # Exists:某个label存在 # DoesNotExist:某个label不存在 # 这里设置的意思是，根据kubernetes.io/hostname的键值，永远不分配到键值为k8s-node01的节点上 operator: NotIn values: - k8s-node01 创建后可以看到 Pod 不会被创建在 k8s-node01 节点上 preferredDuringSchedulingIgnoredDuringExecution软策略 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: node-preferred-podspec: containers: - name: node-preferred-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent affinity: nodeAffinity: # 软策略 preferredDuringSchedulingIgnoredDuringExecution: # 权重，范围为1-100 - weight: 1 # 偏向于 preference: # 匹配表达式 matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node03 可以看到虽然要求部署到 k8s-node03 节点上，但由于本环境没有该节点，所以就被分配到其它的节点了 软硬合体版 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: node-preandreq-podspec: containers: - name: node-preandreq-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - { key: cpu, operator: In, values: [4core] } preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - { key: disktype, operator: In, values: [ssd] } 软策略和硬策略一起使用就会先满足硬策略再分析软策略，可以看到现有的节点没有可以满足以上条件的，所以 Pod 一直处于 Pending 状态 7.1.2 Pod亲和性有时候需要将某些 Pod 与正在运行的已具有某些特质的 Pod 调度到一起，因此就需要 Pod 亲和性调度方式。 Pod 亲和性是通过spec.affinity.podAffinity/podAntiAffinity来实现的，前者为亲和性，后者为反亲和性，包括以下两种： preferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 requiredDuringSchedulingIgnoredDuringExecution硬策略 首先创建一个标签为app:nginx的 Pod 1234567891011apiVersion: v1kind: Podmetadata: name: test-pod labels: app: nginxspec: containers: - name: test-pod-container-nginx image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent 创建硬策略亲和性 Pod 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: pod-requiredspec: containers: - name: pod-required-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent # 亲和性设置 affinity: # Pod亲和性设置 podAffinity: # 硬策略 requiredDuringSchedulingIgnoredDuringExecution: # 标签选择器 - labelSelector: # 列出规则 matchExpressions: # 甄选app这个标签 - key: app # In:label的值在某个列表中 # NotIn:label的值不在某个列表中 # Exists:某个label存在 # DoesNotExist:某个label不存在 # 表示硬性匹配标签为app=nginx的这个Pod operator: In # 键值 values: - nginx # 分配到与这个Pod所属的节点kubernetes.io/hostname值相同的节点上，可以理解为分配到同一节点上，但可能有多个 topologyKey: kubernetes.io/hostname 可以看到会分配在同一节点上 删除该 Pod，将podAffinity改为podAntiAffinity，将不会分配到一起 7.2 污点和容忍度污点（taint）表示一个节点上存在不良状况，污点会影响 Pod 的调度，其定义方式如下 1kubectl taint node {节点名称} {污点名称}={污点值}:{污点的影响} 污点的影响有三种： NoExecute：不将 Pod 调度到具备该污点的节点上，如果 Pod 已经在该节点运行，则会被驱逐。 NoSchedule：不将 Pod 调度到具备该污点的节点上，如果 Pod 已经在该节点运行，不会被驱逐。 PreferNoSchedule：不推荐将 Pod 调度到具备该污点的节点上。 添加污点 1kubectl taint node k8s-node01 cpu=1:NoExecute 删除污点 1kubectl taint node k8s-node01 cpu=1:NoExecute- 示例一 给 k8s-node01 打上污点 1kubectl taint node k8s-node01 cpu=1:NoExecute 创建 Pod 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: Podmetadata: name: toleration-podspec: containers: - name: toleration-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent # 添加节点亲和性硬策略，让该Pod调度到k8s-node01上 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node01 # 容忍设置 tolerations: # 容忍污点 - key: &quot;cpu&quot; # Equal:指等于该值 # Exist:指有该key就可，值无所谓 operator: &quot;Equal&quot; value: &quot;1&quot; # 污点影响 effect: &quot;NoExecute&quot; # 容忍时间，超过该事件就会被驱除，只有污点影响设置为NoExecute才可用 tolerationSeconds: 36 可以看到该 Pod 依旧可以调度到 k8s-node01 节点上，但超过 3600 秒后就被驱除了 容忍度设置一般用于 DaemonSet 控制器，因为 DaemonSet 控制器下的应用一般是为节点本身提供服务的。 7.3 优先级和抢占式调度当集群的资源（CPU、内存、磁盘等）不足时，新 Pod 的创建会一直处于 Pending 的状态，默认情况下，除了系统外的 Pod，其它 Pod 的优先级都是相同的，如果调高了 Pod 的优先级，那么节点就会将低优先级的 Pod 驱逐，腾出空间给优先级高的 Pod，这就被称为抢占式调度。 示例一 要调整优先级，需要先创建 PriorityClass 12345678910apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata: name: test-priorityclass# 优先级设置value: 1000000# 是否在全局下使用，只可设置一个globalDefault: false# 描述description: &quot;this priorityclass is test&quot; 在 Pod 中调用 12345678910apiVersion: v1kind: Podmetadata: name: priorityclass-podspec: containers: - name: priorityclass-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresentpriorityClassName: test-priorityclass 7.4 为Pod设置计算资源容器运行时会提供一些机制来限制容器可以使用的计算资源（CPU、内存和磁盘等），Pod 模板中也提供了这个功能，主要如下 1234567891011121314# 计算资源设置resources: # 资源限制设置，超过设置的值容器会被停止 limits: # cpu限制 cpu: # 内存限制 memory: # 资源请求设置，至少需要多少资源容器才会被运行 requests: # cpu请求 cpu: # 内存请求 memory: 示例一 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: resources-podspec: containers: - name: resources-container # 压测工具 image: vish/stress imagePullPolicy: IfNotPresent # 进行压测，阈值为150Mi,每一秒增加5Mi压力 args: [ '-mem-total','150Mi','-mem-alloc-size','5Mi','-mem-alloc-sleep','1s' ] resources: limits: cpu: '1' memory: '100Mi' requests: cpu: '200m' memory: '50Mi' 可以看到 Pod 最初可以运行，但20秒后就不行了 7.5 命名空间管理命名空间的主要作用是对 k8s 集群的资源进行划分，这种划分是一种逻辑划分，用于实现多租户的资源隔离。 命名空间的创建 1kubectl create namespace 命名空间名称 命名空间的资源配额 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: ResourceQuotametadata: name: namespace:spec: hard: # 计算资源配额 limits.cpu: limits.memory: requests.cpu: requests.memory: # 存储资源配额 # 在所有PVC中，存储资源的需求不能超过该值 requests.storage: # 允许存在的PVC数量 persistentvolumeclaims: # 允许与{storage-class-name}相关的PVC总量 {storage-class-name}.storageclass.storage.k8s.io/requests.storage: # 对象数量配额 # 允许存在ConfigMap数量 configmaps: # 允许存在的非终止状态的Pod数量 pods: # 允许存在的RC数量 replicationcontrollers: # 允许存在的资源配额数量 resourcequotas: # 允许存在的SVC数量 services: # 允许存在的LoadBalancer类型的SVC数量 services.loadbalancers: # 允许存在的NodePort类型的SVC数量 services.nodeports: # 允许存在的Secret数量 secrets: 7.5.1 命名空间的资源配额示例一 先创建一个命名空间 1kubectl create namespace test-ns 创建资源配额 12345678910apiVersion: v1kind: ResourceQuotametadata: name: test-rq namespace: test-nsspec: hard: pods: '2' services: '1' persistentvolumeclaims: '4' 创建一个 Deployment 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: test-ns-depolyment namespace: test-nsspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: test-ns-depolyment-container image: daocloud.io/library/nginx:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 配额限制的 Pod 数量是2，但 Deployment 设置的副本数是3，可以看到是创建不了第三个 Pod 的 7.5.2 命名空间单个资源的资源配额通过设置资源配额，可以限定一个命名空间下使用的资源总量，但这只是总量限制，对于单个资源没有限制，有时候一个 Pod 就可能用完整个命名空间所指定的资源，为了避免，可以通过LimitRange来对单个资源进行限定。 设置容器的限额范围 1234567891011121314151617181920apiVersion: v1kind: LimitRangemetadata: name: limitrange-container namespace: test-nsspec: limits: - max: cpu: '200m' memory: '300Mi' min: cpu: '100m' memory: '150Mi' default: cpu: '180m' memory: '250Mi' defaultRequest: cpu: '110m' memory: '160Mi' type: Container 当 LimitRange 创建成功后，创建该命名空间下的 Pod 时，各个容器的resource.limits和resource.requests必须满足 Max 和 Min 之间的范围。 而default和defaultRequest是指，当创建 Pod 时没用设置限额，则根据此属性来设置限额。 设置 Pod 的限额范围 1234567891011121314apiVersion: v1kind: LimitRangemetadata: name: limitrange-pod namespace: test-nsspec: limits: - max: cpu: '1' memory: '600Mi' min: cpu: '500m' memory: '300Mi' type: Pod 当 LimitRange 创建成功后，创建该命名空间下的 Pod 时，各个 Pod 的resource.limits和resource.requests必须满足 Max 和 Min 之间的范围。 设置 PVC 的限额范围 123456789101112apiVersion: v1kind: LimitRangemetadata: name: limitrange-pvc namespace: test-nsspec: limits: - max: storage: 1Gi min: storage: 200Mi type: PersistentVolumeClaim 设置 Pod 或容器的比例限额范围 12345678910apiVersion: v1kind: LimitRangemetadata: name: limitrange-ratio namespace: test-nsspec: limits: - maxLimitRequestRatio: memory: 2 type: Pod 设置比例限额可以限制 Pod 或容器设置的请求资源和上限资源的比值，在该示例中，Pod 所有容器的resources.limits.memory的总和要 = Pod 所有容器的resources.requests.memory的总和的两倍，即上限必须是需求的两倍，Pod 才能够创建成功。 7.6 标签、选择器、注解7.6.1 标签k8s 的标签（label）是一种语义化标记标签，可以附加到 k8s 对象上，对它们进行标记和划分。 标签的形式是键值对，每个资源对象都可以拥有多个标签，但每个键都只能有一个值。 对于标签的设置，是通过metadata属性中实现的，如下 123456metadata: name: labels: key1: value1 key2: value2 ... 而对于已有的资源，可以通过以下命令添加或删除标签 12kubectl label 资源类型 资源名称 标签名=标签值kubectl label 资源类型 资源名称 标签名=标签值- 7.6.2 选择器通过标签选择器（selector）就可以快速查找到指定标签的资源。 通过-l查找方式如下 1kubectl get pod -l 标签名=/!=标签值 通过in notin查找 1kubectl get pod -l '标签名1 in/notin (标签值1,标签值2)' 每种基于控制器的对象也可以使用标签来选择需要操作的 Pod，如 Job、Deployment、DaemonSet 等都可以在spec中指定选择器，以查找到符合条件的 Pod，如下 12345678spec: selector: matchLabels: app: nginx release: stable matchExpressions: - { key: env, operator: In, values: [dev] } - { key: track, operator: Exists } 在创建 SVC 时，都需要制定标签选择器来确定需要控制的资源，如下 12345678apiVersion: v1kind: Servicemetadata: name: svcspec: selector: app: nginx release: stable 在创建 PVC 时，除了用类匹配之外，也可以用标签来匹配适合的 PV，如下 12345678910111213141516171819202122232425apiVersion: v1kind: PersistentVolumemetadata: name: pv labels: pvnumber: pv01spec: capacity: storage: 1Gi accessModes: - ReadWriteMany storageClassName: nginx---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvcspec: selector: matchLabels: pvnumber: pv01 resources: requests: storage: 1Gi storageClassName: nginx 7.6.3 注解注解（annotation）也是一种类似标签的机制，但只是给资源添加更多信息的方式，类似注释。 设置注解，是通过metadata来实现的，如下 1234567891011apiVersion: v1kind: Podmetadata: name: pod annotation: team: 'cqm' phone: '123456' email: '123456@789.com' ...spec: ... 八、API ServerAPI Server 是集群内布各个组件通信的中介，也是外部控制的入口，k8s 的安全机制基本都是围绕着保护 API Server 来设计的，通过认证、鉴权、准入控制三步来保证 API Server 的安全。 我们在使用 k8s 时，都是通过kubectl工具来访问 API Server 的，kubectl把命令转换为对 API Server 的 REST API 调用。 8.1 身份认证身份认证主要用于确定用户能不能访问，是访问 API Server 的第一个关卡。 通过命令可以看到认证情况，可以看到是通过 6443 端口进行访问的。 要访问 API Server，首先就要进行身份认证，k8s 的身份认证分为以下两类： 常规用户认证：主要提供于普通用户或独立于 k8s 之外的其他外部应用使用，以便能从外部访问 API Server。 ServiceAccount 认证：主要提供于内部的 Pod 使用。 8.1.1 常规用户认证常规用户认证主要有三种方式： HTTPS 证书认证：基于 CA 证书签名的数字证书认证。 HTTP 令牌认证：通过令牌来识别用户。 HTTP Base 认证：通过用户名和密码认证。 令牌认证是最实用也最普及的方式，首先生成一个随机令牌 1head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 接着给 k8s 创建令牌认证文件 123vim /etc/kubernetes/pki/token_auth_file# 格式为:令牌1,用户1,用户IDec0f09bf9a3c40f9db1cc0f8e6664c12,cqm,1 认证文件创建好后，在 API Server 启动文件中进行引用 123vim /etc/kubernetes/manifests/kube-apiserver.yaml# 在spec中添加--token-auth-file=/etc/kubernetes/pki/token_auth_file 接着就可以用认证好的用户访问 API Server 来获取 Pod 的信息了 1curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer ec0f09bf9a3c40f9db1cc0f8e6664c12&quot; 但可以看到还是失败，因为还没有进行授权，后边将进行授权。 8.1.2 ServiceAccount认证ServiceAccount 认证主要提供于集群内布的 Pod 中的进程使用，常规用户认证是不限制命名空间的，但 ServiceAccount 认证的局限于它所在的命名空间中。 默认 ServiceAccount 每个命名空间都有个默认的 ServiceAccount，如果创建 Pod 时没用指定，那么就会使用默认的 ServiceAccount。 通过命令可以看到默认的 ServiceAccount 创建一个 Pod 进行测试 1234567891011apiVersion: v1kind: Podmetadata: name: sa-podspec: containers: - name: sa-pod-container image: appropriate/curl:latest imagePullPolicy: IfNotPresent command: ['sh', '-c'] args: ['echo &quot;this is sa test&quot;; sleep 3600'] 查看 Pod 的详细信息，可以看到被挂载了一个 Secret 类型的卷，实际上这里面就存放了 ServiceAccount 的认证信息 进入容器内部，通过以上地址映射令牌在进行访问 API Server，可以看到由于未授权依旧不行 12curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; 自定义 ServiceAccount 如果某些 Pod 需要访问 API Server，通常会让它引用自定义 ServiceAccount，并为其授权。 首先创建一个自定义 ServiceAccount 1234apiVersion: v1kind: ServiceAccountmetadata: name: my-serviceaccount 创建好后可以查看详细信息，包含了证书和令牌等 创建 Pod 引用 123456789101112apiVersion: v1kind: Podmetadata: name: my-sa-podspec: serviceAccountName: my-serviceaccount containers: - name: my-sa-pod-container image: appropriate/curl:latest imagePullPolicy: IfNotPresent command: ['sh', '-c'] args: ['echo &quot;this is sa test&quot;; sleep 3600'] 就可以看到 Pod 中已经引用了 8.2 RBAC授权k8s 中有基于属性的访问控制（ABAC），基于角色的访问控制（RBAC），基于 HTTP 回调机制的访问控制（Webhook）、Node 认证等授权模式，但1.6版本开始就默认启用 RBAC 模式。 RBAC 授权主要分为两步： 角色定义：指定角色名称，定义允许访问哪些资源以及允许的访问方式。 角色绑定：将角色与用户（常规用户或 ServiceAccount）进行绑定。 而角色定义和角色绑定又分为两种： 只有单一指定命名空间访问权限的角色：角色定义关键字为 Role，角色绑定关键字为 RoleBinding。 拥有集群级别（不限命名空间）访问权限的角色：角色定义为 ClusterRole，角色绑定关键字为 ClusterRoleBinding。 8.2.1 普通角色的定义与绑定普通角色定义 创建一个普通角色 1234567891011121314apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: # 角色名 name: rbac-role namespace: default# 角色规则定义rules: # 表示可以对哪些API组的资源进行操作，这里为空即不限制- apiGroups: [&quot;&quot;] # 可以访问的资源列表，这里为Pod resources: [&quot;pods&quot;] # 可以进行的访问方式 verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 访问方式可以通过开启kubectl反向代理查看，其中的verb就是可以访问的方式 12kubectl proxy --port:8080curl http://localhost:8080/{APIVersion} 普通角色绑定 定义角色后就可以进行绑定角色，绑定可以针对常规用户认证，也可以这对 ServiceAccount 认证。 创建角色绑定 123456789101112131415161718192021apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbac-rolebinding namespace: default# 将角色绑定给那些认证主体，它是一个数组，将常规用户认证cqm和ServiceAccount认证my-serviceaccount加入到rabc-role角色中subjects: # 常规用户认证绑定- kind: User name: cqm apiGroup: &quot;&quot; # ServiceAccount认证绑定- kind: ServiceAccount name: my-serviceaccount apiGroup: &quot;&quot;# 要绑定的角色roleRef: # 普通角色 kind: Role name: rbac-role apiGroup: &quot;&quot; 尝试用之前创建的常规用户认证和 ServiceAccount 认证来访问 API Server，可以发现就可以访问了 1curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer ec0f09bf9a3c40f9db1cc0f8e6664c12&quot; 12curl --insecure https://192.168.88.10:6443/api/v1/namespaces/default/pods -H &quot;Authorization:Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; 8.2.2 集群角色的定义与绑定集群角色与普通角色的区别如下： 使用的关键字不同：普通角色使用 Role，绑定使用 RoleBinding，集群角色使用 ClusterRole，绑定使用 ClusterRoleBinding。 集群角色不属于任何命名空间，模板也不需要指定命名空间，普通角色要求指定命名空间，如果没指定九默认 default。 集群角色可以访问所有命名空间下的资源，也可以访问不在命名空间下的资源。 集群角色创建和绑定如下 123456789101112131415161718192021222324252627282930# 集群角色定义apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: # 集群角色名 name: rbac-clusterrole# 规则rules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]---# 集群角色绑定apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: rbac-clusterrolebindingsubjects:- kind: User name: cqm apiGroup: &quot;&quot;- kind: ServiceAccount name: my-serviceaccount apiGroup: &quot;&quot; # 由于my-serviceaccount是某个命名空间下的，所以要指定命名空间 namespace: defaultroleRef: kind: ClusterRole name: clusterrole apiGroup: &quot;&quot; 这样一来用户（cqm，my-serviceaccount）都可以访问全局的资源，当然 ClusterRole 也可以和 RoleBinding 绑定在一起，因为 ClusterRole 是不限命名空间的，如果既想给某个认证主体绑定 ClusterRole，又想限制它能访问的命名空间，就可以通过与 RoleBinding 绑定来实现，本例中是指rbac-clusterrole的角色在绑定rbac-rolebinding后，可以访问在default命名空间下的任何资源，如下 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: rbac-clusterrole...---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbac-rolebinding namespace: default... 8.3 创建一个用户只能管理指定的命名空间在实际的生产环境中，Master 的管理者可能有很多个，但不可能给人人都赋予 root 的权限，那么就可以创建新用户给其管理指定命名空间的权限。 创建新用户，这时候该用户是使用不了 k8s 的 123useradd cqmpasswd cqm... 创建证书请求 123456789101112131415161718vim cqm-csr.json{ &quot;CN&quot;: &quot;cqm&quot;, &quot;hosts&quot;: [], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;ShenZhen&quot;, &quot;L&quot;: &quot;ShenZhen&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ]} 下载证书生成工具 12345678wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl_1.6.0_linux_amd64wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssljson_1.6.0_linux_amd64wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl-certinfo_1.6.0_linux_amd64mv cfssl_1.6.0_linux_amd64 cfsslmv cfssl-certinfo_1.6.0_linux_amd64 cfssl-certinfomv cfssljson_1.6.0_linux_amd64 cfssljsonchmod a+x cfssl*mv cfssl* /usr/local/bin 生成证书 12cd /etc/kubernetes/pkicfssl gencert -ca=ca.crt -ca-key=ca.key -profile=kubernetes ~/cqm-csr.json | cfssljson -bare cqm 设置集群参数 123456export KUBE_APISERVER=&quot;192.168.88.10:6443&quot;kubectl config set-cluster kubernetes \\--certificate-authority=/etc/kubernetes/pki/ca.crt \\--embed-certs=true \\--server=${KUBE_APISERVER} \\--kubeconfig=cqm.kubeconfig 设置客户端认证参数 12345kubectl config set-credentials cqm \\--client-certificate=/etc/kubernetes/pki/cqm.pem \\--client-key=/etc/kubernetes/pki/cqm-key.pem \\--embed-certs=true \\--kubeconfig=cqm.kubeconfig 设置上下文参数 12345kubectl config set-context kubernetes \\--cluster=kubernetes \\--user=cqm \\--namespace=dev \\--kubeconfig=cqm.kubeconfig 进行 RoleBinding，这里的意思是指将常规用户 cqm 与 ClusterRole 的 admin 角色进行绑定，且指定 dev 的命名空间给 cqm，最终效果是 cqm 只能够访问和管理 dev 命名空间下的所有资源 12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cqm-rolebinding namespace: devsubjects:- kind: User name: cqm apiGroup: &quot;&quot;roleRef: kind: ClusterRole name: admin apiGroup: &quot;&quot; 设置默认上下文 12345mkdir /home/cqm/.kubecp cqm.kubeconfig /home/cqm/.kube/configchown cqm:cqm /home/cqm/.kube/configsu cqmkubectl config use-context kubernetes --kubeconfig=/home/cqm/.kube/config 九、k8s扩展9.1 可视化管理——Kubernetes DashboardKubernetes Dashboard 可以实现 k8s 的可视化管理，可以实现对 Pod、控制器、Service 等资源的创建和维护，并对它们进行持续监控。 9.1.1 安装Kubernetes Dashboard 下载 1wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 模板文件如下，将拉取镜像的地址改为国内地址，同时修改 SVC 模式为 NodePort，这样在集群之外也可以访问 Dashboard 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279apiVersion: v1kind: Namespacemetadata: name: kubernetes-dashboard---apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard---kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboardtype: Opaque---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboardtype: Opaquedata: csrf: &quot;&quot;---apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboardtype: Opaque---kind: ConfigMapapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardrules: - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;, &quot;kubernetes-dashboard-certs&quot;, &quot;kubernetes-dashboard-csrf&quot;] verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;kubernetes-dashboard-settings&quot;] verbs: [&quot;get&quot;, &quot;update&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] resourceNames: [&quot;heapster&quot;, &quot;dashboard-metrics-scraper&quot;] verbs: [&quot;proxy&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;services/proxy&quot;] resourceNames: [&quot;heapster&quot;, &quot;http:heapster:&quot;, &quot;https:heapster:&quot;, &quot;dashboard-metrics-scraper&quot;, &quot;http:dashboard-metrics-scraper&quot;] verbs: [&quot;get&quot;]---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboardrules: - apiGroups: [&quot;metrics.k8s.io&quot;] resources: [&quot;pods&quot;, &quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboardroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboardsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: registery.cn-hangzhou.aliyuncs.com/google_containers/dashboard:v2.3.1 imagePullPolicy: Always ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kubernetes-dashboard volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard nodeSelector: &quot;kubernetes.io/os&quot;: linux tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---kind: ServiceapiVersion: v1metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboardspec: type: NodePort ports: - port: 8000 targetPort: 8000 selector: k8s-app: dashboard-metrics-scraper---kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboardspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dashboard-metrics-scraper template: metadata: labels: k8s-app: dashboard-metrics-scraper annotations: seccomp.security.alpha.kubernetes.io/pod: 'runtime/default' spec: containers: - name: dashboard-metrics-scraper image: registery.cn-hangzhou.aliyuncs.com/google_containers/metrics-scraper:v1.0.6 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /tmp name: tmp-volume securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 serviceAccountName: kubernetes-dashboard nodeSelector: &quot;kubernetes.io/os&quot;: linux tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: tmp-volume emptyDir: {} 安装 1kubectl apply -f Kubernetes-dashboard.yaml 创建完之后可以查看对应的 SVC，就可以通过浏览器访问了 RBAC 授权 12345678910111213141516171819202122# 创建ServiceAccount认证apiVersion: v1kind: ServiceAccountmetadata: name: dashboard-admin namespace: kubernetes-dashboard---# 创建ClusterRoleBinding将dashboard-admin与集群角色cluster-admin进行绑定apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: dashboardadmin-rbacsubjects:- kind: ServiceAccount name: dashboard-admin namespace: kubernetes-dashboardroleRef: apiGroup: &quot;&quot; kind: ClusterRole name: cluster-admin 获取令牌并填入 1kubectl describe secret dashboard-admin -n kubernetes-dashboard 9.1.2 Kubernetes Dashboard使用 创建一个简单的 Pod 查看 Pod 状态 9.2 资源监控——Prometheus和Grafana9.2.1 安装Prometheus 进行 RBAC 授权 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.rbac.yml 配置 ConfigMap 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.configmap.yml 配置 Deployment 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.deployment.yml 配置 SVC 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/prometheus.service.yml 9.2.2 安装Grafana 配置 Deployment 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/grafana.deployment.yml 配置 SVC 1kubectl apply -f https://github.com/realdigit/PrometheusAndGrafanaForK8S/blob/master/grafana.service.yml 9.2.3 Prometheus和Grafana的使用 添加数据源 使用模板，这里使用代号为 315 的 k8s 监控模板 效果 9.3 日志管理——ElasticSearch、Fluentd、Kibanak8s 推荐采用 ElasticSearch、Fluentd、Kibana（简称EFK）三者组合的方式，对集群的日志进行手机和查询，关系如下： ElasticSearch 是一种搜索引擎，用于存储日志并进行查询。 Fluentd 用于将日志消息从 k8s 发送到 ElasticSearch。 Kibana 是一种图形界面，用于查询 ElasticSearch 中的日志。 EFK 之间的交互如下： 容器运行时会将日志输出到控制台，并以 ”-json.log“ 结尾将日志文件存放到 /var/lib/docker/containers 目录中，而 /var/log 是 linux 系统的日志。 在各个 Node 上运行的 Fluentd 将是收集这些日志，并发送给 ElasticSearch。 Kibana 是直接与用户交互的界面，可以查询 ElasticSearch 中的日志。 9.3.1 安装EFK 配置命名空间 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/create-logging-namespace.yaml 配置 Fluentd 的 ConfigMap 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-configmap.yaml 安装 Fluentd 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml 安装 ElasticSearch 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/es-statefulset.yaml 配置 ElasticSearch 的 SVC 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/es-service.yaml 安装 Kibana 1kubectl apply -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml 配置 Kibana 的 SVC 1kubectl applt -f https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/kibana-service.yaml 获取 Kibana 访问地址 1kubectl cluster-info | grep Kibana 十、项目部署10.1 无状态项目部署Guestbook 是一个无状态的多层 Web 应用程序，是一个简单的留言板程序，包含一下三个部分，并拥有读写分离机制： 前端应用：Guestbook 留言板应用，将部署多个实例供用户访问。 后端存储（写）：Redis 主应用，用于写入留言信息，只部署一个案例。 后端存储（读）：Redis 从属应用，用域读取留言信息，将部署多个案例。 部署 Redis 主实例 12345678910111213141516171819202122232425262728293031apiVersion: apps/v1kind: Deploymentmetadata: name: redis-master-deployment labels: app: redisspec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: name: redis-master-pod labels: app: redis role: master tier: backend spec: containers: - name: redis-master-container image: daocloud.io/library/redis:latest imagePullPolicy: IfNotPresent resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 部署 Redis 主实例 SVC 12345678910111213141516apiVersion: v1kind: Servicemetadata: name: redis-master-service labels: app: redis role: master tier: backendspec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: master tier: backend 部署 Redis 从属实例 12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: redis-slave-deployment labels: app: redisspec: selector: matchLabels: app: redis role: slave tier: backend replicas: 2 template: metadata: name: redis-slave-pod labels: app: redis role: slave tier: backend spec: containers: - name: redis-slave-container image: daocloud.io/library/redis:latest imagePullPolicy: IfNotPresent resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns ports: - containerPort: 6379 部署 Redis 从属实例 SVC 1234567891011121314apiVersion: v1kind: Servicemetadata: name: redis-slave-service labels: app: redis role: slave tier: backendspec: ports: - port: 6379 targetPort: 6379 selector: app: redis 部署 Guestbook 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: guestbook-deployment labels: app: guestbookspec: selector: matchLabels: app: guestbook tier: frontend replicas: 3 template: metadata: name: guestbook-pod labels: app: guestbook tier: frontend spec: containers: - name: guestbook-container image: kubeguide/guestbook-php-frontend imagePullPolicy: IfNotPresent resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns ports: - containerPort: 80 部署 Guestbook 的 SVC 123456789101112131415apiVersion: v1kind: Servicemetadata: name: guestbook-service labels: app: guestbook tier: frontendspec: type: NodePort selector: app: guestbook tier: frontend ports: - port: 80 nodePort: 30222 10.2 有状态项目部署WordPress 是使用 PHP 开发的开源个人博客平台，是一套非常完善的内容管理系统，支持非常丰富的插件和模板，主要包含以下两个部分： 前端应用：WordPress。 后端应用：MySQL 数据库，使用 PVC 来存储博客的数据。 首先生成一个数据库密码 1echo -n 'toortoor' | base64 创建一个 Secret 存放密码 1234567apiVersion: v1kind: Secretmetadata: name: mysql-secrettype: Qpaquedata: mysql_password: dG9vcnRvb3I= 部署 nfs-client，实现自动分配 PV 给 PVC，步骤参照 6.3.2 部署 MySQL 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1kind: Servicemetadata: name: mysql-service labels: app: wordpressspec: selector: app: wordpress tier: mysql ports: - port: 3306 clusterIP: None---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-pvc labels: app: wordpressspec: accessModes: - ReadWriteOnce storageClassName: nfs-storageclass resources: requests: storage: 2Gi---apiVersion: apps/v1kind: Deploymentmetadata: name: mysql-deployment labels: app: wordpressspec: replicas: 1 selector: matchLabels: app: wordpress tier: mysql strategy: type: Recreate template: metadata: name: mysql-pod labels: app: wordpress tier: mysql spec: containers: - name: mysql-container image: daocloud.io/library/mysql:5.7 imagePullPolicy: IfNotPresent ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: mysql_password volumeMounts: - name: mysql-datadir mountPath: /var/lib/mysql volumes: - name: mysql-datadir persistentVolumeClaim: claimName: mysql-pvc 部署 WordPress 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778apiVersion: v1kind: Servicemetadata: name: wordpress-service labels: app: wordpressspec: ports: - port: 80 nodePort: 30111 selector: app: wordpress tier: frontend type: NodePort---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: wordpress-pvc labels: app: wordpressspec: storageClassName: nfs-storageclass accessModes: - ReadWriteOnce resources: requests: storage: 2Gi---apiVersion: apps/v1kind: Deploymentmetadata: name: wordpress-deployment labels: app: wordpress tier: frontendspec: selector: matchLabels: app: wordpress tier: frontend strategy: type: Recreate template: metadata: name: wordpress-pod labels: app: wordpress tier: frontend spec: containers: - name: wordpress-container image: daocloud.io/daocloud/dao-wordpress:latest imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: wordpress # 这里用到了MySQL的变量参数 env: # Mysql的SVC名称 - name: WORDPRESS_DB_HOST value: mysql-service - name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: mysql_password volumeMounts: - name: wordpress-datadir mountPath: /var/www/html volumes: - name: wordpress-datadir persistentVolumeClaim: claimName: wordpress-pvc 部署 Ingress，通过 www.cqm.com:30111 就能访问 WordPress 123456789101112131415161718apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: wordpress-ingress-nginx labels: app: wordpressspec: rules: - host: www.cqm.com http: paths: - path: /var/www/html pathType: ImplementationSpecific backend: service: name: wordpress-service port: number: 30111 测试 10.3 使用Helm部署项目Helm 是 k8s 的一个子项目，是一种 k8s 包管理平台，它能够定义、部署、升级非常复杂的 k8s 应用集合，并进行版本管理。 Helm 客户端：是一种远程命令客户端工具，主要用于 Chart 文件的创建、打包和发布部署，以及和 Chart 仓库的管理。Helm 发出的请求，根据 Chart 结构生成发布对象，并将 Chart 解析成各个 k8s 资源的实际部署文件，供 k8s 创建相应资源，同时还提供发布对象的更新、回滚、统一删除等功能。 Chart：是应用程序的部署定义，包含各种 yaml 文件，可以采用 TAR 格式打包。 Chart 仓库：Helm 中存放了各种应用程序的 Chart 包以供用户下载，Helm 可以同时管理多个 Chart 仓库，默认情况下管理一个本地仓库和一个远程仓库。 发布对象：在 k8s 集群中部署的 Chart 称为发布对象，Chart 和发布对象的关系类似于镜像和容器，前者是部署的定义，后者是实际部署好的应用程序。 10.3.1 Helm安装 安装 123wget https://get.helm.sh/helm-v3.6.2-linux-amd64.tar.gztar -xf helm-v3.6.2-linux-amd64.tar.gzcp linux-amd64/helm /usr/local/bin/ 设置环境变量 KUBECONFIG 来指定存有 ApiServre 的地址与 token 的配置文件地址，默认为~/.kube/config 1export KUBECONFIG=/root/.kube/config 配置 Helm 仓库 12helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/chartshelm repo add incubator https://aliacs-app-catalog.oss-cn-hangzhou.aliyuncs.com/charts-incubator/ 10.3.2 Helm Chart的基本操作Chart 的创建 创建 Chart 1helm create chart名称 执行后会在当前目录下创建一个文件，可用 tree 命令查看该目录结构 chars：存放该 Chart 以来的所有子 Chart 的目录，这些子 Chart 也拥有这 4 个部分，如果有子 Chart ，则需要在父 Chart 创建一个 requirements.yaml 文件，在文件中记录这些子 Chart。 Chart.yaml：记录该 Chart 的相关信息，并定义在文件中的 .Chart 开头的属性值。 template：存放了 k8s 部署文件的 Helm 模板，其中扩展了 Go Template 语法。 values.yaml：定义在文件中的 .Values 开头的属性值。 _helpers.tpl：它是一个i模板助手文件，用于定义通用信息，然后在其他地方使用。 NOTES.txt：会在 Chart 部署命令后，代入具体的参数值，产生说明信息。 test-connection.yaml：用于定义部署完成后需要执行的测试内容，以便测试是否部署成功。 Chart 的验证 在发布 Chart 之前，可以通过命令检查 Chart 文件是否有误，如下 1helm lint chart目录/ 同时也可以用命令将各项值组合为 k8s 的 yaml 文件，查看是否为预期内容，比如下图就是其中 Deployment 的内容 1helm install --dry-run --debug 发布名称 -name chart目录名称 Chart 的发布 Chart 的发布命令如下 1helm install 发布版本名称 chart文件目录 查看目前发布的版本 1helm list 将 Chart 打包到仓库中 查看仓库命令 1helm repo list 查看仓库中的包 123helm search repo/hub# 也可以在后面加上应用名，如helm search repo/hub nginx 首先打包 1helm package chart目录 安装 Push 插件 1helm plugin install https://github.com/chartmuseum/helm-push.git 上传 1helm push tar包名 仓库 发布版本的更新、回滚和删除 更新 1helm upgrade 发布版本名称 chart目录或tar flags 查看历史版本 1helm history 发布版本名称 回滚 1helm rollback 发布版本名称 版本号 删除 1helm delete 发布版本名称","link":"/2024/02/18/kubernetes/"},{"title":"Redis","text":"一、简介REmote DIctionary Server（Redis）是一个由 Salvatore Sanfilippo 写的 key-value 存储系统，是跨平台的非关系型数据库。 Redis 就是一款 NoSQL，而 NoSQL 就是指非关系型数据库，主要分为四种： 键值型：Redis 文档型：ElasticSearch、Mongdb 面向列：Hbase 图形化：Neo4j 二、Redis基础2.1 Redis安装 通过 docker-compose 安装 redis 12345678910version: '3.8'services: redis: image: daocloud.io/library/redis:5.0.9 restart: always container_name: redis environment: - TZ=Asia/Shanghai ports: - 6379:6379 进入容器内部测试 redis 123456# 连接redisredis-cli# set新建键值对set key value# get获取键值get key 2.2 Redis常用命令redis 的数据存储结构有以下几种： key-string（字符串）：一个 key 对应一个值 key-hash（哈希）：一个 key 对应一个 map key-list（列表）：一个 key 对应一个列表 key-set（集合）：一个 key 对应一个集合 key-zset（有序集合）：一个 key 对应一个有序的集合 2.2.1 string常用命令 设置值 1set key value 取值 1get key 批量操作 12mset key1 value1 key2 value2 ...mget key1 key2 ... 自增 1incr key 自减 1decr key 自增自减指定数量 12incrby key numberdecrby key number 设置值的同时指定生存时间 1setex key seconds value 设置值，如果当前 key 不存在如同 set，如果存在则说明都不做 1setex key value 在 key 对应的 value 后追加内容 1append key value 查看 value 字符串长度 1strlen key 2.2.2 hash常用命令 存储数据 1hset key field value 获取数据 1hget key field 批量操作 12hmset key1 field1 value1 field2 value2 ...hmget key1 firle1 field2 ... 指定自增 1hincrby key field number 设置值，如果当前 key 不存在如同 set，如果存在则说明都不做 1hsetnx key field value 检查 field 是否存在 1hexists key field 删除某个 field 1hdel key field1 field2 ... 获取当前 hash 结构中的全部 field 和 value 1hgetall key 获取当前 hash 结构中的全部 field 1hkeys key 获取当前 hash 结构中的全部 value 1hvals key 获取当前 hash 中 field 的数量 1hlen key 2.2.3 list常用命令 存储数据 1234567891011# 从左侧插入数据lpush key value1 value2 ...# 从右侧插入数据rpush key value1 value2 ...# 如果key不存在，什么都不做，如果key存在但不是list结构，也什么都不做lpushx key value1 value2 ...rpushx key value1 value2 ...# 通过索引位置添加valuelset key index value 获取数据 12345678910111213# 左侧弹出数据并移除lpop key# 右侧弹出数据并移除rpop key# 获取一定范围的数据，start从0开始，stop为-1时为最后一个value，-2时为倒数第二个valuelrange key start stop# 根据索引位置获取valuelindex key index# 获取整个list的长度llen key 删除数据 12345678# 删除list中count个value的值，当count&gt;0，从左向右删除，但count&lt;0，从右向左删除，但count==0，全部删除lrem key count value# 保留列表中指定范围内的数据，超出这个范围的都会被移除ltrim start stop# 将list1中的最后一个数据弹出，插入到list2中的第一个位置rpoplpush key1 key2 2.2.4 set常用命令 存储数据 12# value不允许重复，且数据无序排列sadd key value1 value2 ... 获取数据 1234567891011121314151617# 获取全部数据smembers key# 随机获取数据，并移除，可加弹出数量spop key number# 取多个set的交集sinter key1 key2 ...# 取多个set的并集sunion key1 key2 ...# 取多个set的差集sdiff key1 key2 ...# 查看当前set是否包含某个值sismember key value 删除数据 1srem key value1 value2 ... 2.2.5 zset常用命令 存储数据 12345# score必须是数值，value不允许重复zadd key score1 value1 score2 value2 ...# 修改score，如果value存在则增加分数，如果不存在则相当于zaddzincrby key number value 获取数据 123456789101112131415161718# 查看指定value的分数zscore key value# 获取value数量zcard key# 根据score范围查询value数量zcount key min max# 根据分数从小到大排序，获取指定范围内的数据，添加了withscores参数会返回value的具体scorezrange key start stop withscores# 从大到小zrevrange key start stop withscores# 根据分数的范围获取数据，如果不希望包括min和max的值可以用(min max)的方式，最大最小值用±inf表示zrangebyscore key min max withscores [limit,offset,count]zrevrangebyscore key max min withscores [limit,offset,count] 删除数据 1zrem key value1 value2 ... 2.2.6 key常用命令 查看所有key 1keys * 查看某个key是否存在 1exists key 删除key 1del key 设置key的生存时间 12345678910111213141516# 单位为sexpire key seconds# 单位为mspexpire key milliseconds# 指定生存到某个时间点expireat key timestamppexpireat key millseconds# 查看key的剩余生存时间，返回-2则key不存在，-1则没设置生存时间ttl keypttl key# 移除生存时间persist key 选择操作的库 12345# redis默认有16个库select 0~15# 移动key到另一个库中move key db 2.2.7 库的常用命令 清空当前所在数据库 1flushdb 清空所有数据库 1flushdball 查看当前库有多少key 1dbsize 查看最后一次操作的时间 1lastsave 实时监控redis接收到的命令 1monitor 三、Redis配置3.1 Redis的AUTH docker-compose.yaml 12345678910111213version: '3.8'services: redis: image: daocloud.io/library/redis:5.0.9 container_name: redis restart: always environment: - TZ=Asia/Shanghai ports: - 6379:6379 volumes: - ./redis.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] 设置 Redis 连接密码 12vim redis.confrequirepass toortoor 1docker-compose up -d 进入 Redis 之后都需要输入密码才可以创建 key 12redis-cliauth toortoor 3.2 Redis的事务Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证： 批量操作在发送 EXEC 命令前被放入队列缓存 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中 一个事务从开始到执行会经历以下几个阶段： 开始事务：multi 命令入队：…… 执行事务：exec 取消事务：discard 监听：在开启事务之前，先通过 watch 监听事务中要操作的 key，如果在事务过程中有其他的客户端修改了 key，那么事务将会被取消 事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。 监听 1watch name age gander 开始事务 1multi 命令入队 123set name cqmset age 22set gander male 执行事务/取消事务 1exec/discard 3.3 Redis的持久化3.3.1 RDB持久化Redis 的配置文件位于 Redis 安装目录, ROB 是默认的持久化机制。 docker-compose.yaml 123456789101112131415version: '3.8'services: redis: image: daocloud.io/library/redis:5.0.9 container_name: redis restart: always environment: - TZ=Asia/Shanghai ports: - 6379:6379 volumes: - ./redis.conf:/usr/local/redis/redis.conf - ./data:/data # 加载redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis.conf 123456789vim redis.conf# 在900秒内有1一个 key 发生了变化，就执行 RDB 持久化save 900 1save 300 10save 60 10000# 开启RDB持久化rdbchecksum yes# RDB持久化名称dbfilename dump.rdb 测试持久化 1234set name cqmset age 22# 关闭redis并保存shutdown save 可以看到 data 目录下多了个 rdb 文件，即使 redis 容器重启也不会造成 key 的丢失 3.3.2 AOF持久化AOF 比起 RDB 有更高的数据安全性，如果同时开启了 AOF 和 RDB，那么前者比后者的优先级更高，且如果先开启了 RDB 在开启 AOF，那么 RDB 中的内容会被 AOF 的内容覆盖。 redis.conf 123456789# 开启AOF持久化appendonly yes# AOF文件名appendfilename appendonly.aof# AOF持久化执行策略# always:每次执行写操作都调用fsync# everysec:最多每秒调用一次fsync# no:根据环境的不同在不确定的时间调用fsyncappendfsync always|everysec|no 重启 docker-compose 测试 123set gander male# 不RDB持久化shutdown nosave 可以看到 data 目录下多了个 aof 文件，即 AOF 持久化生成的文件 3.4 Redis主从架构Redis 的主从架构是指 Master 节点负责写操作，而其余的 Slave 节点负责读操作。 docker-compose.yaml 12345678910111213141516171819202122232425262728293031323334353637383940414243version: '3.8'services: redis-master: container_name: redis-master image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7001:6379 volumes: - ./redis1.conf:/usr/local/redis/redis.conf - ./data1:/data command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis-slave1: container_name: redis-slave1 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7002:6379 volumes: - ./redis2.conf:/usr/local/redis/redis.conf - ./data2:/data command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] # 连接redis-master容器，并将该容器ip地址映射为master links: - redis-master:master redis-slave2: container_name: redis-slave2 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7003:6379 volumes: - ./redis3.conf:/usr/local/redis/redis.conf - ./data3:/data command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] links: - redis-master:master 从节点 redis.conf 12# slaveof &lt;主节点地址&gt; &lt;端口&gt;slaveof master 6379 启动后进入容器内部通过 info 可看到节点信息 3.5 Redis哨兵模式Redis 的主从架构有一个很明显的问题，就是当 Master 节点出现问题宕机后，那么 Redis 集群就没有可以进行写操作的 Redis 了，而哨兵就可以解决该问题。 在每个节点中都会有个哨兵与 Redis 进行连接，且哨兵与哨兵之间也会进行连接，如果 Master 节点出现故障宕机了，那么哨兵们就会选出一个 Slave 来作为新的 Master 来提供写的操作。 docker-compose.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445version: '3.8'services: redis-master: container_name: redis-master image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7001:6379 volumes: - ./redis1.conf:/usr/local/redis/redis.conf - ./data1:/data - ./sentinel1.conf:/data/sentinel.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis-slave1: container_name: redis-slave1 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7002:6379 volumes: - ./redis2.conf:/usr/local/redis/redis.conf - ./data2:/data - ./sentinel2.conf:/data/sentinel.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] links: - redis-master:master redis-slave2: container_name: redis-slave2 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7003:6379 volumes: - ./redis3.conf:/usr/local/redis/redis.conf - ./data3:/data - ./sentinel3.conf:/data/sentinel.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] links: - redis-master:master Master 节点 sentinel.conf 123456# 以守护进程的方式运行redisdaemonize yes# 指定Master节点，sentinel monitor &lt;名称&gt; &lt;ip&gt; &lt;端口&gt; &lt;Slave个数&gt;sentinel monitor master localhost 6379 2# 指定哨兵每隔多久检测一次redis主从架构sentinel down-after-milliseconds master 10000 Slave 节点 sentinel.conf 123daemonize yessentinel monitor master master 6379 2sentinel down-after-milliseconds master 10000 进入容器启动哨兵，当 Master 节点出现问题后，就会在两个 Slave 中选出一个作为新的 Master，而旧的 Master 启动后就会变为新的 Slave 1redis-sentinel /data/sentinel.conf 3.6 Redis集群Redis 集群在保证主从和哨兵的基本功能之外，还能提高 Redis 存储数据的能力，主要的特点如下 Redis 集群是无中心的 Redis 集群有ping-pang的机制 投票机制，集群节点的数量必须是 2n+1 分配了 16484 个 hash 槽，在存储数据时，会对 key 进行 crc16 的算法，并对 16384 进行取余，通过结果分配到对应的节点上，每个节点都有自己维护的 hash 槽 每个主节点都要跟一个从节点，但这里的从节点只管备份，不管查询 集群中半数的节点宕机后，那么集群就瘫痪 docker-compose.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374version: '3.8'services: redis1: container_name: redis1 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7001:7001 - 17001:17001 volumes: - ./conf.d/redis1.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis2: container_name: redis2 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7002:7002 - 17002:17002 volumes: - ./conf.d/redis2.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis3: container_name: redis3 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7003:7003 - 17003:17003 volumes: - ./conf.d/redis3.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis4: container_name: redis4 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7004:7004 - 17004:17004 volumes: - ./conf.d/redis4.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis5: container_name: redis5 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7005:7005 - 17005:17005 volumes: - ./conf.d/redis5.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis6: container_name: redis6 image: daocloud.io/library/redis:5.0.9 restart: always environment: - TZ=Asia/Shanghai ports: - 7006:7006 - 17006:17006 volumes: - ./conf.d/redis6.conf:/usr/local/redis/redis.conf command: [&quot;redis-server&quot;, &quot;/usr/local/redis/redis.conf&quot;] redis{1..6}.conf，x 为 {1..6} 123456789101112# 指定redis端口port 700x# 开启集群cluster-enabled yes# 集群信息文件cluster-config-file nodes-700x.conf# 集群对外ipcluster-announce-ip 192.168.88.135# 集群对外端口cluster-announce-port 700x# 集群总线端口cluster-announce-bus-port 1700x 进入任意 reids 容器创建集群 12# --cluster-replicas:每个主节点分配的从节点个数redis-cli --cluster create 192.168.88.135:7001 192.168.88.135:7002 192.168.88.135:7003 192.168.88.135:7004 192.168.88.135:7005 192.168.88.135:7006 --cluster-replicas 1 由于每个主节点都被分配了不同的 hash 槽，所以要在容器内任意切换不同的 redis 节点需要加参数 -c 1redis-cli -h 192.168.88.135 -p 7001 -c 四、Redis常见问题4.1 Redis的删除策略当 key 的生存时间到了，Redis 并不会立即删除该 key，而是遵守以下删除策略来进行删除 定期删除：Redis 每隔一段时间就回去查看设置了生存时间的 key，默认是 100ms 查看 3 个 key 惰性删除：当用户去查询已经超过了生存时间的 key，Redis 会先查看该 key 是否已经超过了生存时间，如果超过，那么 Redis 会将该 key 删除并给用户返回一个空值 4.2 Redis的淘汰机制当 Redis 内存满的时候添加了一个新的数据，那么就会执行 Redis 的淘汰机制，通过 maxmemory-policy 来设置，参数如下 volatile-lru：当内存不足时，会删除一个设置了生存时间且最近最少使用的 key allkeys-lru：当内存不足时，会删除一个设置了最近最少使用的 key volatile-lfu：当内存不足时，会删除一个设置了生存时间且最近使用频率最低的 key allkeys-lfu：当内存不足时，会删除一个设置了最近使用频率最低的 key volatile-random：当内存不足时，会随机删除一个设置了生存时间的 key allkeys-random：当内存不足时，会随机删除一个 key volatile-ttl：当内存不足时，会删除一个生存时间最少的 key noeviction：内存不足时，直接报错 4.3 缓存问题缓存穿透 当客户查询的数据 Redis 中没有，数据库中也没有，且请求量特别大时，就会导致数据库的压力过大，解决方法如下 根据 id 查询时，如果 id 是自增的，那么可以将最大的 id 放到 Reids 中，当查询数据时直接对比 id 即可 如果 id 不是 int 型，那么可以将全部的 id 放入 set 中，用户查询之前可以先到 set 查看是否有该 id 获取用户的 ip 地址，对该地址进行访问限制 缓存击穿 当用户查询的是热点数据时，那么并发量肯定是很高的，当 Redis 中的热点数据过期了，那么数据库的压力就会很大，甚至宕机，解决方法如下 在访问热点数据时，缓存中没有的时候，可以添加一把锁，让几个请求去访问数据库，避免数据库宕机 把热点数据的生存时间去掉 缓存雪崩 当大量缓存同时到期时，导致请求都去到了数据库，也很容易导致数据库宕机，解决方法如下 对缓存中的数据设置一个随机的生存时间，避免同时过期 缓存倾斜 如果将热点数据放在集群中的某一 Redis 节点上时，那么大量的数据都会去到该 Redis 节点，导致节点宕机，解决方法如下 主从架构，准备大量的从节点 在 Tomcat 中做 JVM 缓存，在查询 Redis 前先查询 JVM 缓存","link":"/2024/02/18/redis/"},{"title":"Zookeeper","text":"ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 Hadoop 和 Hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 一、Zookeeper基础1.1 使用场景 分布式协调组件：通过 watch 机制可以协调好节点之间的数据一致性 分布式锁：通过分布式锁可以做到强一致性 无状态化实现 负载均衡 数据发布/订阅 命名服务 1.2 部署docker-compose.yaml 123456789101112version: '3.8'services: zookeeper: container_name: zk01 image: zookeeper:3.7.0 restart: always hostname: zk01 ports: - 2181:2181 environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=zk01:2888:3888;2181 zoo.cfg 1234567891011121314151617181920dataDir=/datadataLogDir=/datalog# 基本时间配置（毫秒）tickTime=2000# 初始化连接到leader的最大时长，单位为倍数，即初始化时间为tickTime * initLimitinitLimit=5# follower与leader数据同步的最大时长syncLimit=2# 保存数据的快照数量autopurge.snapRetainCount=3# 自动触发清除任务时间间隔，以小时为单位，默认为0，表不清除autopurge.purgeInterval=0# 客户端与zk的最大并发连接数maxClientCnxns=60# 开启standaloneEnabled模式，即独立部署standaloneEnabled=true# 开启adminServeradmin.enableServer=true# 2181是为客户端提供的端口server.1=zk01:2888:3888;2181 1.3 基本命令 启动|关闭|查看状态 1zkServer.sh start|stop|status 进入 zk 1zkCli.sh 查看内部数据结构 1ls [path] 二、内部数据结构2.1 是如何存储数据的Zookeeper 中的数据是保存在节点上的，即 znode，多个 znode 就构成一个树的结构。 如图，a 和 b 就是 Zookeeper 的 znode，创建 znode 方式如下 12345create /[znode_name]# 创建节点并创建一个数据create /[znode_name] [data_name]# 获取数据get [znode_name] 2.2 znode结构Zookeeper 中的 zonode，包含以下几个部分： data：保存数据 acl：权限 c：创建权限 w：写权限 r：读权限 d：删除权限 a：admin 管理者权限 stat：描述当前 znode 的元数据 child：当前节点的子节点 12345678910111213# 查看znode详细信息get -s /[znode_name]cZxid:创建节点的事务IDctime:创建节点时间mZxid:修改节点的事务IDmtime:修改节点时间pZxid:添加和删除子节点的事务IDcversion:当前节点的子节点版本号，初始值为-1，每对该节点的子节点进行操作，这个cversion都会自动增加dataVersion:数据版本初识版本为0，每对该节点的数据进行操作，这个dataVersion都会自动增加aclVersion:权限版本ephemeralOwne:如果当前节点是临时节点，该值是当前节点的session id，如果不是临时节点则为0dataLength:数据长度numChildren:该节点的子节点个数 2.3 znode类型 持久节点：在会话结束后仍会存在 持久序号节点：根据先后顺序，会在结点之后带上一个数值，适用于分布式锁的场景（单调递增） 临时节点：会话结束后会自动删除，适用于注册与服务发现的场景 临时序号节点：跟持久序号节点相同，适用于分布式锁的场景 容器节点：当容器节点中没有任何子节点时，该容器节点会被定期删除（60s） TTL 节点：可以指定节点的到期时间 持久序号节点创建 1create -s /[znode_name] 临时节点创建 1create -e /[znode_name] 临时序号节点创建 1create -e -s /[znode_name] 容器节点创建 1create -c /[znode_name] TTL 节点创建 12# 通过系统配置开启zookeeper.extendedTypesEnabled=true 持久节点 持久节点在创建后服务端会发送一个 session id，并一直保留着。 临时节点 临时节点在创建时后服务器也会发送一个 session id，在会话持续的过程中客户端会不断向服务端续约 session id 的时间，当客户端没有继续续约，而服务端内部的计时器到期时，就会将该 session id 所对应的 znode 全部删除。 2.4 持久化机制Zookeeper 的数据是运行在内存中的，所以提供了两种持久化机制： 事务日志：Zookeeper 将执行过的命令以日志的形式存储在 dataLogDir / dataDir 中，类似于 redis 的 AOF 数据快照：在一定时间间隔内做一次数据快照，存储在快照文件中（snapshot），类似于 redis 的RDB Zookeeper 通过这两种持久化机制，在恢复数据时先将快照文件中的数据恢复到内存中，再用日志文件中的数据做增量恢复，可以实现高效的持久化。 三、zkCli的使用 递归查询 1ls -R /[znode_name] 删除节点 1deleteall /[znode_name] 乐观锁删除 1delete -v [version] /[znode_name] 给当前会话注册用户，并创建节点赋予该用户权限 12addauth digest [user]:[password]create /[znode_name] auth:[user]:[password]:[privileges] 四、分布式锁在分布式的环境下，如果在一个节点去上了个锁，当请求被负载均衡分配到了其它节点，那么锁就无法形成互斥，所以节点之间使用 Zookeeper，做一个协调中心，将锁上传到 Zookeeper，其它节点要用到就去 Zookeeper 拿这个锁，这就是分布式锁。 Zookeeper 锁的分类： 读锁：大家都可以读，前提是之前没有写锁。（读锁比喻成约会，大家都有机会和女神约会，约会前提是女神没结婚） 写锁：只有写锁才能写，前提是不能有任何锁。（写锁比喻成结婚，结婚后只有老公能和女神约会，结婚前提是女神和其他人的关系断干净了） 4.1 上读锁 创建一个临时序号节点，节点数据是 read，表示为读锁 获取当前 Zookeeper 中序号比自己小的所有节点 判断最小节点是否为读锁： 如果是读锁：则上锁失败，因为如果最小节点是读锁，那么后面就不可能有写锁，接着为最小节点设置监听，Zookeeper 的 watch 机制会在最小节点发生变化时通知当前节点，再进行后面的步骤，被称为阻塞等待 如果不是读锁：则上锁成功 4.2 上写锁 创建一个临时序号节点，节点数据是 write，表示为写锁 获取 Zookeeper 中的所有节点 判断自己是否为最小节点： 如果是：上锁成功 如果不是：说明前面还有锁，所以上锁失败，接着监听最小节点，如果最小节点发生变化，则重新进行第二步 羊群效应 假设有一百个请求都是要去写锁，那么就会有一百个请求去监听最小节点，那么 Zookeeper 的压力就会非常大，解决方法是将这一百个请求按请求顺序排列，后一个请求去监听前一个请求即可，实现链式监听。 4.3 watch机制Zookeeper 的 watch 可以看作是一个触发器，当监控的 znode 发生改变，就会触发 znode 上注册的对应事件，请求 watch 的客户端就会接收到异步通知。 zkCli.sh 中使用 watch 1234567create /test# 一次性监听，监听节点内容get -w /test# 监听目录，但所监听节点下创建和删除子节点不会触发监听ls -w /test# 与上面相对，都会触发监听ls -R -w /test 五、集群部署Zookeeper 的集群角色有三个： Leader：处理集群所有事务的请求，集群只有一个 Leader Follower：只处理读请求，参与 Leader 选举 Observer：只处理读请求，提升集群的性能，但不能参与 Leader 选举 docker-compose.yaml 1234567891011121314151617181920212223242526272829303132333435363738version: '3.8'services: zk01: container_name: zk01 image: zookeeper:3.7.0 restart: always hostname: zk01 ports: - 2181:2181 environment: ZOO_MY_ID: 1 # 2888:用于集群内zk之间的通信 # 3888:用于选举投票 # 2181:客户端使用 # 要创建observer则在2181端口后加:observer ZOO_SERVERS: server.1=zk01:2888:3888;2181 server.2=zk02:2888:3888;2181 server.3=zk03:2888:3888;2181 zk02: container_name: zk02 image: zookeeper:3.7.0 restart: always hostname: zk02 ports: - 2182:2181 environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zk01:2888:3888;2181 server.2=zk02:2888:3888;2181 server.3=zk03:2888:3888;2181 zk03: container_name: zk03 image: zookeeper:3.7.0 restart: always hostname: zk03 ports: - 2183:2181 environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zk01:2888:3888;2181 server.2=zk02:2888:3888;2181 server.3=zk03:2888:3888;2181 通过命令查看节点角色 12zkServer.sh statusMode: leader 连接集群 1zkCli.sh -server zk01:2181,zk02:2181,zk03:2181 5.1 ZAB协议ZAB（Zookeeper Atomic Broadcast）即 Zookeeper 原子广播协议，通过这个协议解决了集群数据一致性和崩溃恢复的问题。 ZAB 协议中节点的四种状态 Looking：选举状态 Following Leading Observing 初始化集群时 leader 的选举 当集群中两台节点启动时，就会开始 leader 的选举，选票的格式为 (myid,zXid) 第一轮投票时，每个节点会生成自己的选票，即自己的 (myid,zXid)，然后将选票给到对方，这时候每个节点就会有两张选票，即自己的和对方节点的 接着就会比较两张选票的 zXid，如果都相同就对比 myid，将大的一票投到投票箱中 第二轮投票时，每个节点会将上一轮投出去的选票给到其它节点，然后再对比 (myid,zXid)，将大的一票投出去，就能够选出 leader 后来新启动的节点会发现已经有 leader了，就不用做选举的过程了 可以看出初始化集群时，leader 的选举主要看 myid 崩溃恢复时的 leader 选举 在 leader 确定了之后，leader 会周期性地向 follower 发送心跳包，当 follower 没有收到 leader 发送过来的心跳包，就会进入选举过程，这时候集群不能对外提供服务。 当 leader 挂了之后，follower 的状态会变成 looking 接着就进行选举投票，过程和初始化集群时一样 5.2 主从同步原理 5.3 NIO和BIONIO 用于被客户端连接的 2181 端口，使用的就是 NIO 的连接模式；客户端开启 watch 时，使用的也是 NIO。 BIO 集群在进行选举时，多个节点之间的通信端口，使用的是 BIO 的连接模式。","link":"/2024/02/18/zookeeper/"},{"title":"首页","text":"好记性不如烂笔头","link":"/2024/02/18/%E9%A6%96%E9%A1%B5/"},{"title":"Web","text":"一、Apache1.1 Apache介绍Apache HTTP Server（简称Apache）是Apache软件基金会的一个开放源码的网页服务器，是世界使用排名第一的Web服务器软件。它可以运行在几乎所有广泛使用的计算机平台上，由于其跨平台和安全性被广泛使用，是最流行的Web服务器端软件之一。它快速、可靠并且可通过简单的API扩充，将Perl/Python等解释器编译到服务器中。 Apache HTTP服务器是一个模块化的服务器，源于NCSAhttpd服务器，经过多次修改，成为世界使用排名第一的Web服务器软件。 Apache官方文档：http://httpd.apache.org/docs/ 1.2 通过脚本源码安装Apache 编写安装脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189useradd -s /sbin/nologin -r wwwvim apache-install.sh#!/bin/bashapr_version=1.7.0apr_iconv_version=1.2.2apr_util_version=1.6.1apache_version=2.4.46#检查function check(){ #检查是否为root用户 if [ $USER != 'root' ] then echo -e &quot;\\e[1;31m error:need to be root so that \\e[0m&quot; exit 1 fi #检查是否安装了wget if [ `rpm -qa | grep wget | wc -l` -lt 1 ] then echo -e &quot;\\e[1;31m error:not found wget \\e[0m&quot; exit 1 fi}#安装前准备function install_pre(){ #安装依赖 if [ ! `yum -y install zlib-devel pcre-devel libxml2 expat-devel &amp;&gt; /dev/null` ] then echo -e &quot;\\e[1;31m error:yum install dependency package failed \\e[0m&quot; exit 1 fi #下载apr cd /usr/local if [ ! `wget https://downloads.apache.org/apr/apr-${apr_version}.tar.bz2 &amp;&gt; /dev/null` ] then tar -xf apr-${apr_version}.tar.bz2 if [ ! -d apr-${apr_version} ] then echo -e &quot;\\e[1;31m error:not found apr-${apr_version} \\e[0m&quot; exit 1 else cd apr-${apr_version} fi else echo -e &quot;\\e[1;31m error:Failed to download apr-${apr_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装apr echo &quot;apr configure...&quot; ./configure --prefix=/usr/local/apr &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;apr make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m apr installed successfully \\e[0m&quot; else echo -e &quot;\\e[1;31m apr installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:apr configure failed \\e[0m&quot; exit 1 fi #下载apr-iconv cd /usr/local if [ ! `wget https://www.apache.org/dist/apr/apr-iconv-${apr_iconv_version}.tar.bz2 &amp;&gt; /dev/null` ] then tar -xf apr-iconv-${apr_iconv_version}.tar.bz2 if [ ! -d apr-iconv-${apr_iconv_version} ] then echo -e &quot;\\e[1;31m error:not found apr-iconv-${apr_iconv_version} \\e[0m&quot; exit 1 else cd apr-iconv-${apr_iconv_version} fi else echo -e &quot;\\e[1;31m error:Failed to download apr-iconv-${apr_iconv_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装apr-iconv echo &quot;apr-iconv configure...&quot; ./configure --prefix=/usr/local/apr-iconv --with-apr=/usr/local/apr &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;apr-iconv make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m apr-iconv installed successfully \\e[0m&quot; else echo -e &quot;\\e[1;31m apr-iconv installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:apr-iconv configure failed \\e[0m&quot; exit 1 fi #下载apr-util cd /usr/local if [ ! `wget https://www.apache.org/dist/apr/apr-util-${apr_util_version}.tar.bz2 &amp;&gt; /dev/null` ] then tar -xf apr-util-${apr_util_version}.tar.bz2 if [ ! -d apr-util-${apr_util_version} ] then echo -e &quot;\\e[1;31m error:not found apr-util-${apr_util_version} \\e[0m&quot; exit 1 else cd apr-util-${apr_util_version} fi else echo -e &quot;\\e[1;31m error:Failed to download apr-util-${apr_util_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装apr-util echo &quot;apr-util configure...&quot; ./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr/ &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;apr-util make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m apr-util installed successfully \\e[0m&quot; else echo -e &quot;\\e[1;31m apr-util installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:apr-util configure failed \\e[0m&quot; exit 1 fi}#下载安装Apachefunction apache_install(){ #下载Apache cd /usr/local if [ ! `wget https://downloads.apache.org/httpd/httpd-${apache_version}.tar.gz &amp;&gt; /dev/null` ] then tar -xf httpd-${apache_version}.tar.gz if [ ! -d httpd-${apache_version} ] then echo -e &quot;\\e[1;31m error:not found httpd-${apache_version} \\e[0m&quot; exit 1 else cd httpd-${apache_version} fi else echo -e &quot;\\e[1;31m error:Failed to download httpd-${apache_version} \\e[0m&quot; exit 1 fi #安装Apache echo &quot;Apache configure...&quot; ./configure --prefix=/usr/local/apache --enable-mpms-shared=all --with-mpm=event --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr-util --enable-so --enable-remoteip --enable-proxy --enable-proxy-fcgi --enable-proxy-uwsgi --enable-deflate=shared --enable-expires=shared --enable-rewrite=shared --enable-cache --enable-file-cache --enable-mem-cache --enable-disk-cache --enable-static-support --enable-static-ab --disable-userdir --enable-nonportable-atomics --disable-ipv6 --with-sendfile &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;Apache make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m Apache installed sucessfully \\e[0m&quot; else echo -e &quot;\\e[1;31m Apache installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m Apache configure failed \\e[0m&quot; exit 1 fi}checkinstall_preapache_install 编写启动脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/bin/bashapache_doc=/usr/local/apache/binapache_pid=/usr/local/apache/logs/httpd.pidfunction apache_start(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -gt 1 ] &amp;&amp; [ -f $apache_pid ] then echo -e &quot;Apache [\\e[1;32m running \\e[0m]&quot; exit 1 elif [ $apache_num -eq 1 ] &amp;&amp; [ -f $apache_pid ] then killall httpd fi cd /usr/local/apache/bin;./apachectl echo -e &quot;start Apache [\\e[1;32m OK \\e[0m]&quot;}function apache_stop(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -eq 1 ] then echo -e &quot;Apache [\\e[1;31m stopping \\e[0m]&quot; else killall httpd echo -e &quot;stop Apache [\\e[1;32m OK \\e[0m]&quot; fi}function apache_restart(){ cd /usr/local/apache/bin;./apachectl restart echo -e &quot;restart Apache [\\e[1;32m OK \\e[0m]&quot;}function apache_status(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then echo -e &quot;Apache [\\e[1;32m running \\e[0m]&quot; else echo -e &quot;Apache [\\e[1;31m stopping \\e[0m]&quot; fi}function apache_reload(){ apache_num=`ps -ef | grep httpd | wc -l` if [ $apache_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then cd /usr/local/apache/bin;./apachectl graceful echo -e &quot;reload Apache [\\e[1;32m OK \\e[0m]&quot; else echo -e &quot;Apache [\\e[1;31m stopping \\e[0m]&quot; fi}case $1 instart) apache_start;;stop) apache_stop;;restart) apache_restart;;status) apache_status;;reload) apache_reloadesac 1.3 多处理模块MPMApache HTTP 服务器被设计为一个功能强大，并且灵活的 web 服务器， 可以在很多平台与环境中工作。不同平台和不同的环境往往需要不同 的特性，或可能以不同的方式实现相同的特性最有效率。Apache 通过模块化的设计来适应各种环境。这种设计允许网站管理员通过在 编译时或运行时，选择哪些模块将会加载在服务器中，来选择服务器特性。 实际上就是用来接受请求和处理请求的。 Apache的三种工作方式： Prefork MPM：使用多个进程，每个进程只有一个线程，每个进程再某个确定的时间只能维持一个连接，有点是稳定，缺点是内存消耗过高。 ![Prefork MPM](Prefork MPM.png) Worker MPM：使用多个进程，每个进程有多个线程，每个线程在某个确定的时间只能维持一个连接，内存占用比较小，是个大并发、高流量的场景，缺点是一个线程崩溃，整个进程就会连同其任何线程一起挂掉。 ![Worker MPM](Worker MPM.png) Event MPM：使用多进程多线程+epoll的模式。 ![Event MPM](Event MPM.png) 1.4 虚拟主机默认情况下，一个web服务器只能发布一个默认网站，也就是只能发布一个web站点，对于大网站来说还好，但对于访问量较少的小网站那就显得有点浪费了。 而虚拟主机就可以实现在一个web服务器上发布多个站点，分为基于IP地址、域名和端口三种。 Apache的虚拟主机和默认网站不能够同时存在，如果设置了虚拟主机那么默认网站也就失效了，需要在用虚拟主机发布默认站点才可解决。 基于IP：基于IP的虚拟主机需要耗费大量的IP地址，只适合IP地址充足的环境。 基于端口：需要耗费较多的端口，适合私网环境。 基于域名：需要耗费较多的域名，适合公网环境。 1.4.1 基于IP的虚拟主机 在主配文件中调用虚拟主机文件 123vim /usr/local/apache/conf/httpd.conf# Virtual hostsInclude conf/extra/httpd-vhosts.conf 修改虚拟主机文件 123456789101112131415161718192021# 添加一个逻辑网卡，重启即失效ifconfig eth0:1 192.168.88.100/24 upvim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost 49.232.160.75:80&gt; # 管理员邮箱 # ServerAdmin webmaster@dummy-host.example.com # web目录 DocumentRoot &quot;/usr/local/apache/htdocs/web1&quot; # 域名 # ServerName dummy-host.example.com # 给域名起别名，起到重定向作用 # ServerAlias www.dummy-host.example.com # 错误日子 # ErrorLog &quot;logs/dummy-host.example.com-error_log&quot; # 访问日志 # CustomLog &quot;logs/dummy-host.example.com-access_log&quot; common&lt;/VirtualHost&gt;&lt;VirtualHost 192.168.88.100:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web2&quot;&lt;/VirtualHost&gt; 创建站点目录和文件 1234mkdir /usr/local/apache/htdocs/web{1..2}echo 'this is web1' &gt; /usr/local/apache/htdocs/web1/index.htmlecho 'this is web2' &gt; /usr/local/apache/htdocs/web2/index.html./apache start 测试 1.4.2 基于端口的虚拟主机 修改虚拟主机文件 12345678&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web1&quot;&lt;/VirtualHost&gt;Listen 81&lt;VirtualHost *:81&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web2&quot;&lt;/VirtualHost&gt; 测试 1.4.3 基于域名的虚拟主机 修改虚拟主机文件 123456789&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web1&quot; ServerName www.cqm1.com&lt;/VirtualHost&gt;&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web2&quot; ServerName www.cqm2.com&lt;/VirtualHost&gt; 测试 1.5 LAMPLAMP：Linux + Apache + Mysql + PHP 作用就是构建一个PHP业务环境，用来发布PHP网站。 1.5.1 Mysql通过脚本源码安装 编写安装脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208#!/bin/bashmysql_version=5.7.35install_dir=/optdata_dir=/datawget_url=&quot;https://mirrors.tuna.tsinghua.edu.cn/mysql/downloads/MySQL-5.7/mysql-${mysql_version}-linux-glibc2.12-x86_64.tar.gz&quot;function loginfo(){ if [[ $? -eq 0 ]];then echo -e &quot;\\033[32m[INFO][$(date +&quot;%F %T&quot;)] $1 succeed! \\033[0m&quot; else echo -e &quot;\\033[31m[ERROR][$(date +&quot;%F %T&quot;)] $1 failed! \\033[0m&quot; fi}function mysql_install(){ echo -e &quot;\\033[32mBegin install mysql V${mysql_version} ...\\033[0m&quot; # 安装依赖 sudo yum -y install libaio &gt;/dev/null 2&gt;&amp;1 loginfo &quot;libaio install&quot; # 下载mysql echo -e &quot;\\033[32mBegin download mysql V${mysql_version} ...\\033[0m&quot; curl -O $wget_url &gt;/dev/null 2&gt;&amp;1 mv ./mysql-${mysql_version}-linux-glibc2.12-x86_64.tar.gz $install_dir loginfo &quot;mysql software download&quot; # 解压缩mysql sudo tar -xf $install_dir/mysql-${mysql_version}-linux-glibc2.12-x86_64.tar.gz -C $install_dir loginfo &quot;mysql software decompression&quot; # 创建配置文件目录和数据目录 if [[ -d $install_dir/mysql ]];then rm -rf $install_dir/mysql fi sudo ln -s $install_dir/mysql-${mysql_version}-linux-glibc2.12-x86_64 $install_dir/mysql loginfo &quot;create mysql config dir soft link&quot; if [[ -d $data_dir/mysql ]];then rm -rf $data_dir/mysql fi sudo mkdir -p $data_dir/mysql loginfo &quot;create mysql data dir&quot; # 修改启动脚本 sudo sed -i &quot;46s#basedir=#basedir=${install_dir}/mysql#&quot; ${install_dir}/mysql/support-files/mysql.server sudo sed -i &quot;47s#datadir=#datadir=${data_dir}/mysql#&quot; ${install_dir}/mysql/support-files/mysql.server sudo cp ${install_dir}/mysql/support-files/mysql.server /etc/init.d/mysqld sudo chmod 755 /etc/init.d/mysqld # 创建用户组及用户 if ! grep -q '^mysql:' /etc/group then sudo groupadd mysql loginfo &quot;create user mysql&quot; fi if ! grep -q '^mysql:' /etc/passwd then sudo useradd -r -g mysql -s /bin/false mysql loginfo &quot;create group mysql&quot; fi # 授权 sudo chown -R mysql:mysql $install_dir/mysql sudo chown -R mysql:mysql $data_dir/mysql # 为二进制文件创建软连接 if [ ! -f /usr/bin/mysql ] then sudo ln -s /opt/mysql/bin/mysql /usr/bin/ fi # 创建配置文件 if [ -f /etc/my.cnf ] then sudo rm -f /etc/my.cnf fi sudo bash -c &quot;cat &gt;&gt; /etc/my.cnf&quot; &lt;&lt;EOF[mysqld]datadir = /data/mysqlbasedir = /opt/mysql#tmpdir = /data/mysql/tmp_mysqlport = 3306socket = /data/mysql/mysql.sockpid-file = /data/mysql/mysql.pidmax_connections = 8000max_connect_errors = 100000max_user_connections = 3000check_proxy_users = onmysql_native_password_proxy_users = onlocal_infile = OFFsymbolic-links = FALSEgroup_concat_max_len = 4294967295max_join_size = 18446744073709551615max_execution_time = 20000lock_wait_timeout = 60autocommit = 1lower_case_table_names = 1thread_cache_size = 64disabled_storage_engines = &quot;MyISAM,FEDERATED&quot;character_set_server = utf8mb4character-set-client-handshake = FALSEcollation_server = utf8mb4_general_ciinit_connect = 'SET NAMES utf8mb4'transaction-isolation = &quot;READ-COMMITTED&quot;skip_name_resolve = ONexplicit_defaults_for_timestamp = ONlog_timestamps = SYSTEMlocal_infile = OFFevent_scheduler = OFFquery_cache_type = OFFquery_cache_size = 0sql_mode = NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZEROlog_error = /data/mysql/mysql.errslow_query_log = ONslow_query_log_file = /data/mysql/slow.loglong_query_time = 1general_log = OFFgeneral_log_file = /data/mysql/general.logexpire_logs_days = 99log-bin = /data/mysql/mysql-binlog-bin-index = /data/mysql/mysql-bin.indexmax_binlog_size = 500Mbinlog_format = mixedbinlog_rows_query_log_events = ONbinlog_cache_size = 128kbinlog_stmt_cache_size = 128klog-bin-trust-function-creators = 1max_binlog_cache_size = 2Gmax_binlog_stmt_cache_size = 2Grelay_log = /data/mysql/relayrelay_log_index = /data/mysql/relay.indexmax_relay_log_size = 500Mrelay_log_purge = ONrelay_log_recovery = ONserver_id = 1read_buffer_size = 1Mread_rnd_buffer_size = 2Msort_buffer_size = 64Mjoin_buffer_size = 64Mtmp_table_size = 64Mmax_allowed_packet = 128Mmax_heap_table_size = 64Mconnect_timeout = 43200wait_timeout = 43200back_log = 512interactive_timeout = 300net_read_timeout = 30net_write_timeout = 30skip_external_locking = ONkey_buffer_size = 16Mbulk_insert_buffer_size = 16Mconcurrent_insert = ALWAYSopen_files_limit = 65000table_open_cache = 16000table_definition_cache = 16000default_storage_engine = InnoDBdefault_tmp_storage_engine = InnoDBinternal_tmp_disk_storage_engine = InnoDB[client]socket = /data/mysql/mysql.sockdefault_character_set = utf8mb4[mysql]default_character_set = utf8mb4[ndatad default]TransactionDeadLockDetectionTimeOut = 20000EOF sudo chown -R mysql:mysql /etc/my.cnf loginfo &quot;configure my.cnf&quot; # 创建SSL证书 # sudo mkdir -p ${install_dir}/mysql/ca-pem/ # sudo ${install_dir}/mysql/bin/mysql_ssl_rsa_setup -d ${install_dir}/mysql/ca-pem/ --uid=mysql # sudo chown -R mysql:mysql ${install_dir}/mysql/ca-pem/ # sudo bash -c &quot;cat &gt;&gt; ${data_dir}/mysql/init_file.sql&quot; &lt;&lt;EOF# set global sql_safe_updates=0;# set global sql_select_limit=50000;# EOF # sudo chown -R mysql:mysql ${data_dir}/mysql/init_file.sql # sudo chown -R mysql:mysql /etc/init.d/mysqld # 初始化 ${install_dir}/mysql/bin/mysqld --initialize --user=mysql --basedir=${DEPLOY_PATH}/mysql --datadir=/data/mysql loginfo &quot;initialize mysql&quot; # 客户端环境变量 echo &quot;export PATH=\\$PATH:${install_dir}/mysql/bin&quot; | sudo tee /etc/profile.d/mysql.sh source /etc/profile.d/mysql.sh loginfo &quot;configure envirement&quot; # 获取初始密码 mysql_init_passwd=$(grep 'A temporary password is generated' ${data_dir}/mysql/mysql.err | awk '{print $NF}') # 启动服务 chkconfig --add mysqld sudo systemctl start mysqld loginfo &quot;start mysqld&quot; # 修改密码 mysql --connect-expired-password -uroot -p${mysql_init_passwd} -e 'alter user user() identified by &quot;toortoor&quot;;' &gt;/dev/null 2&gt;&amp;1 loginfo &quot;edit mysql root password&quot;}mysql_install 1.5.2 PHP通过脚本源码安装 编写安装脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176#!/bin/bashcmake_version=3.22.0libzip_version=1.8.0php_version=7.4.16#检查function check(){ #检查是否为root用户 if [ $USER != &quot;root&quot; ] then echo -e &quot;\\e[1;31m error:need to be root so that \\e[0m&quot; exit 1 fi #检查是否安装了wget if [ `rpm -qa | grep wget | wc -l` -lt 1 ] then echo -e &quot;\\e[1;31m error:not found wget \\e[0m&quot; exit 1 fi}#安装前准备function pre(){ #安装依赖包 if [ ! `yum -y install gcc-c++ libxml2 libxml2-devel openssl openssl-devel bzip2 bzip2-devel libcurl libcurl-devel libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel gmp gmp-devel libmcrypt libmcrypt-devel readline readline-devel libxslt libxslt-devel gd net-snmp-* sqlite-devel oniguruma-devel &amp;&gt; /dev/null` ] then echo -e &quot;\\e[1;31m error:yum install dependency package failed \\e[0m&quot; exit 1 fi #下载最新版cmake cd /usr/local if [ ! `wget https://github.com/Kitware/CMake/releases/download/v${cmake_version}/cmake-${cmake_version}.tar.gz &amp;&gt; /dev/null` ] then tar -xf cmake-${cmake_version}.tar.gz if [ ! -d cmake-${cmake_version} ] then echo -e &quot;\\e[1;31m error:no found cmake-${cmake_version} \\e[0m&quot; exit 1 else cd cmake-${cmake_version} fi else echo -e &quot;\\e[1;31m error:Failed to download cmake-${cmake_version} \\e[0m&quot; exit 1 fi #安装cmake echo &quot;cmake configure...&quot; ./configure &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;cmake make &amp;&amp; make install...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m cmake installed sucessfully \\e[0m&quot; else echo -e &quot;\\e[1;31m cmake installed failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:cmake configure failed \\e[0m&quot; exit 1 fi #下载libzip1.1以上版本 cd /usr/local if [ ! `wget --no-check-certificate https://libzip.org/download/libzip-${libzip_version}.tar.gz &amp;&gt; /dev/null` ] then echo &quot;tar libzip...&quot; tar -xf libzip-${libzip_version}.tar.gz if [ ! -d libzip-${libzip_version} ] then echo -e &quot;\\e[1;31m error:not found libzip-${libzip_version} \\e[0m&quot; exit 1 else cd libzip-${libzip_version} fi else echo -e &quot;\\e[1;31m error:Failed to download libzip-${libzip_version}.tar.gz \\e[0m&quot; exit 1 fi #安装libzip mkdir build;cd build echo &quot;cmake libzip...&quot; cmake .. &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;make &amp;&amp; make install libzip...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m libzip install sucessfully \\e[0m&quot; echo -e '/usr/local/lib64\\n/usr/local/lib\\n/usr/lib\\n/usr/lib64'&gt;&gt; /etc/ld.so.conf ldconfig -v &amp;&gt; /dev/null else echo -e &quot;\\e[1;31m error:libzip install failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:lipzip cmake failed \\e[0m&quot; exit 1 fi}function php_install(){ #下载php cd /usr/local if [ ! `wget https://www.php.net/distributions/php-${php_version}.tar.bz2 &amp;&gt; /dev/null` ] then echo &quot;tar php...&quot; tar -xf php-${php_version}.tar.bz2 if [ ! -d php-${php_version} ] then echo -e &quot;\\e[1;31m error:not found php-${php_version} \\e[0m&quot; exit 1 else cd php-${php_version} fi else echo -e &quot;\\e[1;31m error:Failed to download php-${php_version}.tar.bz2 \\e[0m&quot; exit 1 fi #安装php echo &quot;configure php...&quot; #要php以apache模块运行需加上--with-apxs2=/usr/localapache/bin/apxs参数 ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-mysqli=mysqlnd --enable-pdo --with-pdo-mysql=mysqlnd --with-iconv-dir=/usr/local/ --enable-fpm --with-fpm-user=www --with-fpm-group=www --with-pcre-regex --with-zlib --with-bz2 --enable-calendar --disable-phar --with-curl --enable-dba --with-libxml-dir --enable-ftp --with-gd --with-jpeg-dir --with-png-dir --with-zlib-dir --with-freetype-dir --enable-gd-jis-conv --with-mhash --enable-mbstring --enable-opcache=yes --enable-pcntl --enable-xml --disable-rpath --enable-shmop --enable-sockets --enable-zip --enable-bcmath --with-snmp --disable-ipv6 --with-gettext --disable-rpath --disable-debug --enable-embedded-mysqli --with-mysql-sock=/var/lib/mysql/ &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;make &amp;&amp; make install php...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m php install sucessfully \\e[0m&quot; else echo -e &quot;\\e[1;31m php install failed \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m configure php failed \\e[0m&quot; exit 1 fi}function php_set(){ if [ ! -f /usr/local/php-${php_version}/sapi/fpm/php-fpm.service ] then echo -e &quot;\\e[1;31m No found php-fpm.service \\e[0m&quot; exit 1 else cp /usr/local/php-${php_version}/sapi/fpm/php-fpm.service /etc/systemd/system if [ `echo $?` -ne 0 ] then echo -e &quot;\\e[1;31m Copy php-fpm.service failed \\e[0m&quot; exit 1 else sed -i '/PrivateTmp=true/a\\ProtectSystem=false' /etc/systemd/system/php-fpm.service systemctl daemon-reload echo -e &quot;\\e[1;32m php set sucessfully \\e[0m&quot; fi fi}checkprephp_installphp_set 配置PHP 123456789101112131415161718192021222324cd /usr/local/php/etccp php-fpm.conf.default php-fpm.confcp php-fpm.d/www.conf.default php-fpm.d/www.confegrep -v '^;|^$' php-fpm.conf[global]pid = run/php-fpm.piderror_log = log/php-fpm.logdaemonize = yesinclude=/usr/local/php/etc/php-fpm.d/*.confegrep -v '^;|^$' php-fpm.d/www.conf[www]user = wwwgroup = wwwlisten = 127.0.0.1:9000listen.owner = wwwlisten.group = wwwlisten.mode = 0660pm = dynamicpm.max_children = 5pm.start_servers = 2pm.min_spare_servers = 1pm.max_spare_servers = 3 启动 1systemctl start php-fpm 1.5.3 PHP作为Apache模块运行 在apache主配置文件中调用子配置文件 12vim /usr/local/apache/conf/httpd.confinclude conf/extra/php.conf 配置子配置文件 123vim /usr/local/apache/conf/extra/php.conLoadModule php7_module modules/libphp7.soAddType application/x-httpd-php .php 配置虚拟主机 1234vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt; 写web目录 12345echo 'this is cqm web' &gt; /usr/local/apache/htdocs/web/index.htmlvim /usr/local/apache/htdocs/web/phpinfo.php&lt;?phpphpinfo()?&gt; 测试 1.5.4 PHP作为独立服务运行PHP作为独立服务运行有两种模式： TCP socket模式 UNIX socket模式 TCP socket模式 修改www.conf文件 12vim /usr/local/php/etc/php-fpm.d/www.conflisten = 127.0.0.1:9000 配置虚拟主机文件 123456789101112131415161718vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;&lt;Directory &quot;/usr/local/apache/htdocs/web&quot;&gt; Options Indexes FollowSymLinks AllowOverride None Require all granted&lt;/Directory&gt;&lt;IfModule dir_module&gt; DirectoryIndex index.php index.html&lt;/IfModule&gt;&lt;FilesMatch \\.php$&gt; SetHandler &quot;proxy:fcgi://127.0.0.1:9000&quot;&lt;/FilesMatch&gt; 在apache主配文件添加关联 12vim /usr/local/apache/conf/httpd.confinclude conf/extra/php-fpm.conf 配置子配文件 1234vim /usr/local/apache/conf/php-fpm.conf# 载入需要的模块LoadModule proxy_module modules/mod_proxy.soLoadModule proxy_fcgi_module modules/mod_proxy_fcgi.so UNIX socket模式 修改www.conf文件 12vim /usr/local/php/etc/php-fpm.d/www.conflisten = /usr/local/php/etc/php-fpm.socket 配置虚拟主机 123&lt;FilesMatch \\.php$&gt; SetHandler &quot;proxy:unix:/usr/local/php/etc/php-fpm.socket|fcgi://localhost/&quot;&lt;/FilesMatch&gt; 1.6 Apache常用模块1.6.1 长连接HTTP采用TCP进行传输，是面向连接的协议，每完成一次请求就要经历以下过程： 三次握手 发起请求 响应请求 四次挥手 那么N个请求就要建立N次连接，如果希望用户能够更快的拿到数据，服务器的压力降到最低，那么靠长连接就可以解决。 长连接实际上就是优化了TCP连接。 Apache默认开启了长连接，持续时间为5秒，在httpd-default.conf中可以定义。 1234567vim /usr/local/apache/conf/extra/httpd-default.conf# 开启长连接KeepAlive On# 限制每个连接允许的请求数MaxKeepAliveRequests 500# 长连接时间KeepAliveTimeout 5 1.6.2 静态缓存用户每次访问网站都会将页面中的所有元素都请求一遍，全部下载后通过浏览器渲染，展示到浏览器中。但是，网站中的某些元素我们一般都是固定不变的，比如logo、框架文件等。用户每次访问都需要加载这些元素。这样做好处是保证了数据的新鲜，可是这些数据不是常变化的，很久才变化一次。每次都请求、下载浪费了用户时间和公司带宽。 所以我们通过静态缓存的方式，将这些不常变化的数据缓存到用户本地磁盘，用户以后再访问这些请求，直接从本地磁盘打开加载，这样的好处是加载速度快，且节约公司带宽及成本。 在apache主配文件中加载缓存模块 12vim /usr/local/apache/conf/httpd.confLoadModule expires_module modules/mod_expires.so 修改虚拟主机文件调用模块 1234567891011121314151617181920vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; &lt;IfMoudle expires_module&gt; #开启缓存 ExpiresActive on #针对不同类型元素设置缓存时间 ExpiresByType image/gif &quot;access plus 1 days&quot; ExpiresByType image/jpeg &quot;access plus 24 hours&quot; ExpiresByType image/png &quot;access plus 24 hours&quot; #now 相当于 access ExpiresByType text/css &quot;now plus 2 hour&quot; ExpiresByType application/x-javascript &quot;now plus 2 hours&quot; ExpiresByType application/x-shockwave-flash &quot;now plus 2 hours” #其他数据不缓存 ExpiresDefault &quot;now plus 0 min&quot; &lt;/IfModule&gt; &lt;/VirtualHost&gt; 1.6.3 数据压缩数据从服务器传输到客户端，需要传输时间，文件越大传输时间就越长，为了减少传输时间，我们一般把数据压缩后在传给客户端。 apache支持两种模式的压缩： default gzip 两者的区别： mod_deflate 压缩速度快。 mod_gzip 的压缩比略高。 一般情况下，mod_gzip 会比 mod_deflate 多出 4%~6％ 的压缩量。 mod_gzip 对服务器CPU的占用要高一些，所以 mod_deflate 是专门为确保服务器的性能而使用的一个压缩模块，只需较少的资源来进行压缩。 在apache主配文件中加载压缩模块 12vim /usr/local/apache/conf/httpd.confLoadModule deflate_module modules/mod_deflate.so 修改虚拟主机文件调用模块 12345678910111213141516171819vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; &lt;IfMoudle deflate_module&gt; #压缩等级1-9，数字越大压缩能力越好，相应地也越耗CPU性能 DeflateCompressionLevel 4 #压缩类型，html、xml、php、css、js AddOutputFilterByType DEFLATE text/html text/plain text/xml application/x-javascript application/x-httpd-php AddOutputFilter DEFLATE js css #浏览器匹配为IE1-6的不压缩 BrowserMatch \\bMSIE\\s[1-6] dont-vary #设置不压缩的文件 SetEnvIfNoCase Request_URI .(?:gif|jpe?g|png)$ no-gzip dont-vary SetEnvIfNoCase Request_URI .(?:exe|t?gz|zip|bz2|sit|rar)$ no-gzip dont-vary SetEnvIfNoCase Request_URI .(?:pdf|doc)$ no-gzip dont-vary &lt;/IfModule&gt; &lt;/VirtualHost&gt; 1.6.4 限速网站除了能共享页面给用户外，还能作为下载服务器存在。但是作为下载服务器时，我们应该考虑服务器的带宽和IO的性能，防止部分邪恶分子会通过大量下载的方式来攻击你的带宽和服务器IO性能。 问题： 假如你的服务器被邪恶分子通过下载的方式把带宽占满了，那么你或其他用户在访问的时候就会造成访问慢或者根本无法访问。 假如你的服务器被邪恶分子通过下载的方式把服务器IO占满了，那么你的服务器将会无法处理用户请求或宕机。 以上问题可以通过限速来解决，apache自带了基于宽带限速的模块： ratelimit_module：只能对连接下载速度做限制，且是单线程的下载，迅雷等下载工具使用的是多线程下载。 mod_limitipconn：限制每 IP 的连接数，需要额外安装该模块。 ratelimit_module模块 在apache主配文件中加载压缩模块 12vim /usr/local/apache/conf/httpd.confLoadModule ratelimit_module modules/mod_ratelimit.so 修改虚拟主机文件调用模块 123456789101112vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;# Location相对路径：/usr/local/apache/htdocs/...# Directory绝对路径：...&lt;Location /download&gt; SetOutputFiler RATE_LIMIT #限速100k SetEnv rate-limit 100&lt;/Location&gt; mod_limitipconn模块 下载安装模块 123456wget http://dominia.org/djao/limit/mod_limitipconn-0.24.tar.bz2tar -xf mod_limitipconn-0.24.tar.bz2cd mod_limitipconn-0.24vim Makefile apxs = &quot;/usr/local/apache/bin/apxs&quot;make &amp;&amp; make install 在apache主配文件启用模块 12vim /usr/local/apache/conf/httpd.confLoadModule limitipconn_module modules/mod_limitipconn.so 修改虚拟主机文件调用模块 123456789&lt;Location /download&gt; SetOutputFiler RATE_LIMIT #限速100k SetEnv rate-limit 100 #限制线程数 MaxConnPerIP 3 #对index.html文件不作限制 NoIPLimit index.html&lt;/Location&gt; 1.6.5 访问控制在生产环境中，网站分为公站和私站，公站允许所有人访问，但私站就只允许内部人员访问，Require就可以实现访问控制的功能。 容器： RequireAny：一个符合即可通过 RequireAll：所有符合才可通过 Requirenone：所有都不符合才可通过 普通的访问控制 修改虚拟主机文件 1234567891011121314vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;&lt;Directory &quot;/usr/local/apache/htdocs/web/test&quot;&gt; AllowOverride None # 拒绝所有人访问 Require all denied # 允许该地址段的用户访问 Require ip 192.168.88 # 允许该主机访问 Require host www.cqm.com&lt;/Directory&gt; 用户登录验证访问控制 修改虚拟主机文件 123456789101112131415161718192021vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot;&lt;/VirtualHost&gt;&lt;Directory &quot;/usr/local/apache/htdocs/web/test&quot;&gt; # 定义提示信息，用户访问时提示信息会出现在认证的对话框中 AuthName &quot;Private&quot; # 定义认证类型，在HTTP1.0中，只有一种认证类型：basic。在HTTP1.1中有几种认证类型，如：MD5 AuthType Basic # 定义包含用户名和密码的文本文件，每行一对 AuthUserFile &quot;/usr/local/apache/user.dbm&quot; # 配合容器使用，只有条件全部符合才能通过 &lt;RequireAll&gt; Require not ip 192.168.88 # require user user1 user2 (只有用户user1和user2可以访问) # requires groups group1 (只有group1中的成员可以访问) # require valid-user (在AuthUserFile指定的文件中的所有用户都可以访问) Require valid-user &lt;/RequireAll&gt;&lt;/Directory&gt; 生成用户文件 123# 生成cqm用户/usr/local/apache/bin/htpasswd -cm /usr/local/apache/user.dbm cqm... 1.6.6 URL重写Apache通过mod_rewrite模块可以实现URL重写的功能，URL重写其实就是改写用户浏览器中的URL地址。 在主配文件开启模块 12vim /usr/local/apache/conf/httpd.confLoadModule rewrite_module modules/mod_rewrite.so 修改虚拟主机文件 123456789101112131415161718192021222324vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; # 开启URL重写功能 RewriteEngine on # 重写规则，跳转到百度 RewriteRule &quot;^/$&quot; &quot;http://www.baidu.com&quot; [NC,L] # 匹配条件，根据请求头进行匹配 RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;chrome&quot; [NC,OR] RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;curl&quot; # 重写规则，和匹配到的条件配合使用，请求头匹配到chrome或curl则返回403状态码 RewriteRule &quot;^/$&quot; - [F]&lt;/VirtualHost&gt;RewreteRule [flag] 部分标记规则R:强制外部重定向F:禁用URL，返回403HTTP状态码G:强制URL为GONE，返回410HTTP状态码P:强制使用代理转发L:表明当前规则是最后一条规则，停止分析以后规则的重写N:重新从第一条规则开始运行重写过程C:与下一条规则关联NS:只用于不是内部子请求NC:不区分大小写 通过URL重写实现分流功能 123456789vim /usr/local/apache/conf/extra/httpd-vhosts.conf&lt;VirtualHost *:80&gt; DocumentRoot &quot;/usr/local/apache/htdocs/web&quot; RewriteEngine on RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;(chrome|curl)&quot; [NC,OR] RewriteRule &quot;^/$&quot; &quot;http://pc.cqm.com&quot; [NC] RewriteCond &quot;%{HTTP_USER_AGENT}&quot; &quot;(iPhone|Blackberry|Android|ipad)&quot; [NC] RewriteRule &quot;^/$&quot; &quot;http://phone.cqm.com&quot; [NC]&lt;/VirtualHost&gt; 1.6.7 压力测试Apache压力测试使用ab命令 123456789101112131415161718ab-A:指定连接服务器的基本的认证凭据-c:指定一次向服务器发出请求数-C:添加cookie-g:将测试结果输出为“gnuolot”文件-h:显示帮助信息-H:为请求追加一个额外的头-i:使用“head”请求方式-k:激活HTTP中的“keepAlive”特性-n:指定测试会话使用的请求数-p:指定包含数据的文件-q:不显示进度百分比-T:使用POST数据时，设置内容类型头-v:设置详细模式等级-w:以HTML表格方式打印结果-x:以表格方式输出时，设置表格的属性-X:使用指定的代理服务器发送请求-y:以表格方式输出时，设置表格属性 123/usr/local/apache/bin/ab -n 10000 -c 200 http:...# 并发数per second... 二、Nginx2.1 Nginx介绍Nginx是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理服务器，也是一个 IMAP/POP3/SMTP 代理服务器。和apache一样，都是web服务器软件，因为其性能优异，所以被广大运维喜欢。又因为nginx是一个轻量级的web服务器，相比apache来说资源消耗更低。 Nginx中文文档：https://www.nginx.cn/doc/index.html 2.2 通过脚本源码安装Nginx 编写安装脚本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576vim nginx-install.sh#!/bin/bashnginx_version=1.21.3#检测function check(){ #检测是否为root if [ $USER != &quot;root&quot; ] then echo -e &quot;\\e[1;31m error:need to be root so that \\e[0m&quot; exit 1 fi #检测wget是否安装 if [ ! -e /usr/bin/wget ] then echo -e &quot;\\e[1;31m error:not found command /usr/bin/wget \\e[0m&quot; exit 1 fi}#安装前准备function install_pre(){ # 安装依赖 #0:stdin标准输入 1:stdout标准输出 2:stderr错误输出 if [ ! `yum -y install gcc-* pcre-devel zlib-devel &amp;&gt; /dev/null` ] then echo -e &quot;\\e[1;31m error:yum install dependency package failed \\e[0m&quot; exit 1 fi #下载源码包 cd /usr/local/ if [ ! `wget http://nginx.org/download/nginx-${nginx_version}.tar.gz &amp;&gt; /dev/null` ] then tar -xf nginx-${nginx_version}.tar.gz if [ ! -d nginx-${nginx_version} ] then echo -e &quot;\\e[1;31m error:not found nginx-${nginx_version} \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:wget file nginx-${nginx_version}.tar.gz failed \\e[0m&quot; exit 1 fi}#安装function install_nginx(){ cd /usr/local/nginx-${nginx_version} echo &quot;nginx configure...&quot; ./configure --prefix=/usr/local/nginx &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo &quot;nginx make...&quot; make &amp;&amp; make install &amp;&gt; /dev/null if [ `echo $?` -eq 0 ] then echo -e &quot;\\e[1;32m nginx install success \\e[0m&quot; else echo -e &quot;\\e[1;31m error:nginx install fail \\e[0m&quot; exit 1 fi else echo -e &quot;\\e[1;31m error:nginx configure fail \\e[0m&quot; exit 1 fi}checkinstall_preinstall_nginx 编写启动脚本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889#!/bin/bash#Source function libiaryif [ -f /etc/init.d/functions ]then . /etc/init.d/functionselse echo &quot;Not found file /etc/init.d/functions&quot; exitfinginxd=/usr/local/nginx/sbin/nginxnginx_pid=/usr/local/nginx/logs/nginx.pidfunction nginx_start(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then echo -e &quot;nginx [\\e[1;32m running \\e[0m]&quot; exit 1 elif [ $nginx_num -eq 1 ] &amp;&amp; [ -f $nginx_pid ] then killall nginx fi $nginxd}function nginx_stop(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -eq 1 ] then echo -e &quot;nginx [\\e[1;31m stopping \\e[0m]&quot; exit 1 elif [ $nginx_num -gt 1 ] then killall nginx fi}function nginx_status(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then echo -e &quot;nginx [\\e[1;32m running \\e[0m]&quot; else echo -e &quot;nginx [\\e[1;31m stopping \\e[0m]&quot; fi}function nginx_restart(){ nginx_stop nginx_start}function nginx_reload(){ nginx_num=`ps -ef | grep nginx | wc -l` if [ $nginx_num -gt 1 ] &amp;&amp; [ -f $nginx_pid ] then $nginxd -s reload else echo -e &quot;nginx [\\e[1;31m stopping \\e[0m]&quot; fi}case $1 instart) nginx_start echo -e &quot;nginx start [\\e[1;32m OK \\e[0m]&quot;;;stop) nginx_stop echo -e &quot;nginx stop [\\e[1;32m OK \\e[0m]&quot;;;status) nginx_status;;restart) nginx_restart echo -e &quot;nginx restart [\\e[1;32m OK \\e[0m]&quot;;;reload) nginx_reload echo -e &quot;nginx reload [\\e[1;32m OK \\e[0m]&quot;esac 2.3 Nginx的Server块当Nginx配置文件只有一个Server块时，那么该Server块就被Nginx认为是默认网站，所有发给Nginx的请求都会传给该Server块。 123456789101112131415161718192021222324252627server { # 监听80端口 listen 80; # 域名 server_name localhost; # 字符集 charset koi8-r; # 访问日志路径 access_log logs/host.access.log main; # web根路径 # /代表相对路劲，这里代表/usr/local/nginx location / { # 根目录路径，这里代表/usr/local/nginx/html root html; # 索引页 index index.html index.htm; } # 404状态码 error_page 404 /404.html; location = /404.html{ root html; } # 50x状态码 error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } 2.4 Nginx的访问控制 编写主配文件 12345678910111213location / { root html; index index.html index.htm; # 允许192.168.88.0/24的用户访问 allow 192.168.88.0/24; # 拒绝所有 deny all; # 基于客户端IP做过滤，符合条件的允许访问，不符合的返回404 # 这里为不是192.168.88的就返回404 if ( $remote_addr !~ &quot;192.168.88&quot; ){ return 404; }} 2.5 Nginx的用户验证 编写主配文件 12345678location / { root html; index index.html index.htm; # 欢迎词 auth_basic &quot;welcome to cqm's web&quot;; # 存放用户文件 auth_basic_user_file /usr/local/nginx/htpasswd;} 生成用户文件 1/usr/local/apache/bin/htpasswd -cm /usr/local/nginx/htpasswd cqm 2.6 Nginx参数1234567891011# nginx中的log_format可以用来自定义日志格式# log_format变量：$remote_addr:记录访问网站的客户端地址$remote_user:远程客户端用户名$time_local:记录访问时间与时区$request:用户的http请求起始行信息$status:http状态码，记录请求返回的状态码，例如：200、301、404等$body_bytes_sent:服务器发送给客户端的响应body字节数$http_referer:记录此次请求是从哪个连接访问过来的，可以根据该参数进行防盗链设置。$http_user_agent:记录客户端访问信息，例如：浏览器、手机客户端等$http_x_forwarded_for:当前端有代理服务器时，设置web节点记录客户端地址的配置，此参数生效的前提是代理服务器也要进行相关的x_forwarded_for设置 2.7 Nginx防盗链盗链用大白话讲就是抓取别人网站的资源，加以利用，以至于被抓取资源的网站消耗了带宽，而收益的是抓取资源的人。 而反盗链就可以防止别人抓取自身网站的资源。 编写主配文件 1234567location / { # 除了www.cqm.com之外，都返回403 valid_referers none blocked www.cqm.com; if ($invalid_referer){ return 403; }} 2.8 Nginx虚拟主机Nginx的虚拟主机是通过server块来实现的。 2.8.1 基于IP的虚拟主机 修改主配文件 12vim /usr/local/nginx/conf/nginx.confinclude /usr/local/nginx/conf/conf.d/nginx_vhosts.conf; 修改虚拟主机文件 12345678910111213141516vim /usr/local/nginx/conf/conf.d/nginx_vhosts.confserver { listen 192.168.88.100; location / { root html/web1; index index.html index.htm index.php; }}server { listen 192.168.88.101; location / { root html/web2; index index.html index.htm index.php; }} 其它配置 12345# 添加一个逻辑网卡，重启即失效ifconfig eth0:1 192.168.88.100/24 upmkdir /usr/local/nginx/html/web{1..2}echo 'this is web1' &gt; /usr/local/nginx/html/web1/index.htmlecho 'this is web2' &gt; /usr/local/nginx/html/web2/index.html 测试 1234curl http://192.168.88.100/this is web1curl http://192.168.88.101/this is web2 2.8.2 基于端口的虚拟主机 修改虚拟主机文件 12345678910111213141516vim /usr/local/nginx/conf/conf.d/nginx_vhosts.confserver { listen 80; location / { root html/web1; index index.html index.htm index.php; }}server { listen 81; location / { root html/web2; index index.html index.htm index.php; }} 2.8.3 基于域名的虚拟主机123456789101112131415161718vim /usr/local/nginx/conf/conf.d/nginx_vhosts.confserver { listen 80; server_name www.cqm1.com; location / { root html/web1; index index.html index.htm index.php; }}server { listen 80; server_name www.cqm2.com; location / { root html/web2; index index.html index.htm index.php; }} 2.9 Nginx反向代理代理最常见的使用方式就是翻墙，能够实现让国内的用户访问国外的网站。 原理： 用户讲请求发给代理服务器 代理服务器代替用户去获取数据 代理服务器将数据发送给用户 正常没有代理的上网 使用代理服务器的上网 = 代理服务器又分为两种：正向代理、反向代理 正向代理：代理用户向服务器获取资源 反向代理：代理服务器去管理网络资源，用户有请求找反向代理就可以了 编写反向代理服务器主配文件 123456vim /usr/local/nginx/conf/nginx.conflocation / { index index.html index.htm index.php; # 访问代理服务器就会跳转到http://192.168.88.100 proxy_pass http://192.168.88.100;} 反向代理其它配置 1234567891011121314151617181920212223proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;client_max_body_size 10m; #允许客户端请求的最大单文件字节数client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数，proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时)proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时)proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时)proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2）proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 2.10 Nginx下载限速限速方法主要分为： 下载速度限制 单位时间内请求数限制 基于客户端的并发数限制 Nginx官方提供的限制IP连接和并发的模块有两个： limit_req_zone：用来限制单位时间内的请求数，即速率限制，采用的漏桶算法 limit_req_conn：来限制同一时间连接数，即并发限制 单位时间内请求数限制 修改主配文件 12345678910111213# 在http快下调用模块# $binary_remote_addr:基于ip地址做限制# zone:创建缓存域和缓存大小# rate:设置访问频数limit_req_zone $binary_remote_addr zone=cqm:10m rate=1r/s;# server块location /test { ... # 调用模块 # 当请求数超过5次时，就拒绝访问，并返回503状态码 limit_req zone=cqm burst=5 nodelay;} 限制并发连接数 修改主配文件 1234567891011# 在http快下调用模块# $binary_remote_addr:基于ip地址做限制# zone:创建缓存域和缓存大小limit_req_conn $binary_remote_addr zone=cqm:10m;# server块location /test { ... # 限制同一时间内下载数为1个 limit_conn cqm 1;} 限制下载速度 修改主配文件 12345location /test { ... # 限制下载速度为1k limit_rate 1k;} 2.11 Nginx的URL重写rewrite的主要功能是实现URL地址的重定向。Nginx的rewrite功能需要PCRE软件的支持，即通过perl兼容正则表达式语句进行规则匹配的。默认参数编译nginx就会支持rewrite的模块，但是也必须要PCRE的支持。 URL模板语块： set：设置变量 if：判断 return：返回值或URL break：终止 rewrite：重定向URL 例一：根据不同域名跳转到主域名的不同目录下 创建测试目录 1234mkdir /usr/local/nginx/html/{cn,jp,us}echo 'this is China' &gt; /usr/local/nginx/html/cn/index.htmlecho 'this is Japan' &gt; /usr/local/nginx/html/jp/index.htmlecho 'this is America' &gt; /usr/local/nginx/html/us/index.html 修改hosts文件，以便解析 12345vim /etc/hosts192.168.88.100 www.cqm.com192.168.88.100 www.cqm.com.cn192.168.88.100 www.cqm.com.jp192.168.88.100 www.cqm.com.us 修改主配文件 1include /usr/local/nginx/conf/conf.d/rewrite.conf 修改重定向文件 1234567891011121314151617181920212223242526272829vim /usr/local/nginx/conf/conf.d/rewrite.conf# 设置重定向serverserver { listen 80; server_name www.cqm.com.cn www.cqm.com.jp www.cqm.com.us; location / { # 模糊匹配到cn的话，就跳转到http://www.cqm.com/cn下 if ($http_host ~ (cn)$){ set $nation cn; rewrite ^/$ http://www.cqm.com/$nation; } if ($http_host ~ (jp)$){ set $nation jp; rewrite ^/$ http://www.cqm.com/$nation; } if ($http_host ~ (us)$){ set $nation us; rewrite ^/$ http://www.cqm.com/$nation; } }}server { listen 80; server_name www.cqm.com; location / { root html; index index.html; }} 测试 1234567curl -L http://www.cqm.com.cnthis is Chinacurl -L http://www.cqm.com.jpthis is Japancurl -L http://www.cqm.com.usthis is America-L:自动获取重定向 例二：retuen以及break的简单实用 修改重定向文件 12345678910111213141516171819vim /usr/local/nginx/conf/conf.d/rewrite.confserver { listen 80; server_name www.cqm.com; location / { root html; index index.html; # 模糊匹配 ~ # 精确匹配 = # 不匹配 !~ # 如果匹配请求头不是chrome的话，就返回403 if ($http_user_agent !~ 'chrome'){ return 403; # break放在return上面的话就不会执行return操作 # break; # return http://www.baidu.com; } }} flag flag是放在rewrite重定向的URL后边的，格式为：rewrite URL flag flag的选项有： last：本条规则匹配完成后继续执行到最后。 break：本条规则匹配完成即终止。 redirect：返回302临时重定向。 permanent：返回301永久重定向。 redirect和permanent的区别：设置permanent的话，新网址就会完全继承旧网址，旧网址的排名等完全清零，如果不是暂时迁移的情况下都建议使用permanent；设置redirect的话，新网址对旧网址没有影响，且新网址也不会有排名。 2.12 Nginx优化2.12.1 大并发Nginx的工作模式：主进程 + 工作进程 假如Nginx服务器有4个CPU 设置主配文件来实现高并发 12345678vim /usr/local/nginx/conf/nginx.confworker_processes 4;# 指定运行的核的编号，采用掩码的方式设置编号worker_cpu_affinity 0001 0010 0100 1000;events { # 单个工作进程维护的请求队列长度，根据实际情况调整 worker_connections 1024;} 2.12.2 长连接 修改主配文件 1234567vim /usr/local/nginx/conf/nginx.conf# keepalive_timeout用来设置长连接，0代表关闭keepalive_timeout 0;# 设置长连接时间100s#keepalive_timeout 100;# 设置每秒可以接受的请求数#keepalive_requests 8192; 2.12.3 压缩Nginx是采用gzip进行压缩。 修改主配文件 1234567891011121314151617181920212223242526272829303132vim /usr/local/nginx/conf/nginx.conf# 开启缓存gzip on;# Nginx做为反向代理的时候启用# off:关闭所有的代理结果数据压缩# expired:如果header中包含”Expires”头信息，启用压缩# no-cache:如果header中包含”Cache-Control:no-cache”头信息，启用压缩# no-store:如果header中包含”Cache-Control:no-store”头信息，启用压缩# private:如果header中包含”Cache-Control:private”头信息，启用压缩# no_last_modified:启用压缩，如果header中包含”Last_Modified”头信息，启用压缩# no_etag:启用压缩，如果header中包含“ETag”头信息，启用压缩# auth:启用压缩，如果header中包含“Authorization”头信息，启用压缩# any:无条件压缩所有结果数据gzip_proxied any;# 启用gzip压缩的最小文件，小于设置值的文件将不会压缩gzip_min_length 1k;# 设置压缩所需要的缓冲区大小# 32 4K表示按照内存页（one memory page）大小以4K为单位（即一个系统中内存页为4K），申请32倍的内存空间# 建议此项不设置，使用默认值gzip_buffers 32 4k;# 设置gzip压缩级别，级别越底压缩速度越快文件压缩比越小，反之速度越慢文件压缩比越大gzip_comp_level 1;# 用于识别http协议的版本，早期的浏览器不支持gzip压缩，用户会看到乱码，所以为了支持前期版本加了此选项。默认在http/1.0的协议下不开启gzip压缩gzip_http_version 1.1;# 设置需要压缩的MIME类型,如果不在设置类型范围内的请求不进行压缩gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png application/vnd.ms-fontobject font/ttf font/opentype font/x-woff image/svg+xml; 2.12.4 静态缓存将部分数据缓存在用户本地磁盘，用户加载时，如果本地和服务器的数据一致，则从本地加载。提升用户访问速度，提升体验度。节省公司带宽成本。 修改主配文件 12345# 模糊匹配以png或gif结尾的文件location ~* \\.(png|gif)$ { # 缓存时间为1小时 expires 1h;} 三、Tomcat3.1 Tomcat介绍Tomcat 服务器是一个免费的开放源代码的 Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试 JSP 程序的首选。 实际上 Tomcat 是 Apache 服务器的扩展，但运行时它是独立运行的，所以当你运行 tomcat 时，它实际上作为一个与 Apache 独立的进程单独运行的。 Tomcat 官方文档：https://tomcat.apache.org/ 3.2 Tomcat安装 安装jdk和tomcat 123456yum -y install java-1.8.0-openjdk*wget https://downloads.apache.org/tomcat/tomcat-9/v9.0.43/bin/apache-tomcat-9.0.43.tar.gztar -xf apache-tomcat-9.0.43.tar.gzmkdir /root/tomcatmv apache-tomcat-9.0.43.tar.gz/* /root/tomcat./root/tomcat/bin/startup.sh","link":"/2024/02/18/web/"},{"title":"ELK","text":"ELK 即 ElasticSearch + Logstash + Kibana，Elasticsearch 是一个搜索和分析引擎。Logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。Kibana 则可以让用户在 Elasticsearch 中使用图形和图表对数据进行可视化。 ELK 目前官方已整合为 Elastic Stack。 一、部署ELK1.1 Elasticsearch部署 准备 java 环境 1yum -y install jaba-1.8.0-openjdk* 创建用户 12groupadd elkuseradd -g elk elk 下载 es 并授权 1234wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.15.1-linux-x86_64.tar.gztar -xf elasticsearch-7.15.1-linux-x86_64.tar.gzmv elasticsearch-7.15.1 eschown -R elk:elk ./es 配置 es 123456789101112131415vim ./es/config/elasticsearch.yml# 集群名称cluster.name: elk# 节点名称node.name: es# 数据存储目录path.data: /home/elk/es/data# 日志存储目录path.logs: /home/elk/es/logs# 节点IPnetwork.host: 192.168.88.130# 端口http.port: 9200# 集群初始化master节点cluster.initial_master_nodes: [&quot;es&quot;] 启动 es，通过 9200 端口就可以验证是否启动成功 12su elk./es/bin/elasticsearch 1.2 Kibana部署 下载 kibana 1wget https://artifacts.elastic.co/downloads/kibana/kibana-7.15.1-linux-x86_64.tar.gz 解压授权 123tar -xf kibana-7.15.1-linux-x86_64.tar.gzmv kibana-7.15.1 kibanachown -R elk:elk ./kibana 配置 kibana 123456789vim ./kibana/config/kibana.yml# 端口server.port: 5601# kibana的IPserver.host: &quot;192.168.88.130&quot;# es的IPelasticsearch.hosts: [&quot;http://192.168.88.130:9200&quot;]# kibana索引kibana.index: &quot;.kibana&quot; 1.3 Logstash部署logstash 是一个数据分析软件，主要目的是分析log日志。 首先将数据传给 logstash，它将数据进行过滤和格式化（转成 JSON 格式），然后传给 Elasticsearch 进行存储、建搜索的索引，kibana 提供前端的页面再进行搜索和图表可视化，它是调用 Elasticsearch 的接口返回的数据进行可视化。 它组要组成部分是数据输入，数据源过滤，数据输出三部分。 数据输入input input 是指数据传输到 logstash 中，常见的配置如下： file：从文件系统中读取一个文件 syslog：监听 514 端口 redis：从 redis 服务器读取数据 lumberjack：使用 lumberjack 协议来接收数据，目前已经改为 logstash-forwarder input 配置一般为： 1234567891011121314151617181920212223242526272829303132333435363738394041# 从控制台中输入来源stdin {}# 从文件中输入来源file { path =&gt; &quot;E:/software/logstash-1.5.4/logstash-1.5.4/data/*&quot; #单一文件 #监听文件的多个路径 path =&gt; [&quot;E:/software/logstash-1.5.4/logstash-1.5.4/data/*.log&quot;,&quot;F:/*.log&quot;] #排除不想监听的文件 exclude =&gt; &quot;1.log&quot; #添加自定义的字段 add_field =&gt; {&quot;test&quot;=&gt;&quot;test&quot;} #增加标签 tags =&gt; &quot;tag1&quot; #设置新事件的标志 delimiter =&gt; &quot;\\n&quot; #设置多长时间扫描目录，发现新文件 discover_interval =&gt; 15 #设置多长时间检测文件是否修改 stat_interval =&gt; 1 #监听文件的起始位置，默认是end start_position =&gt; beginning #监听文件读取信息记录的位置 sincedb_path =&gt; &quot;E:/software/logstash-1.5.4/logstash-1.5.4/test.txt&quot; #设置多长时间会写入读取的位置信息 sincedb_write_interval =&gt; 15}# 系统日志方式syslog { # 定义类型 type =&gt; &quot;system-syslog&quot; # 定义监听端口 port =&gt; 10514}# filebeats方式beats { port =&gt; 5044} 数据过滤filter fillter 在 logstash 中担任中间处理组件。 常见的 filter 如下： grok：解析无规则的文字并转化为有结构的格式。Grok 是目前最好的方式来将无结构的数据转换为有结构可查询的数据,有120多种匹配规则 mutate：允许改变输入的文档，可以从命名，删除，移动或者修改字段在处理事件的过程中 drop：丢弃一部分 events 不进行处理，例如：debug events clone：拷贝 event，这个过程中也可以添加或移除字段 geoip：添加地理信息（为 kibana 图形化展示使用） filter 的配置一般为： 12345678910111213141516171819filter { #定义数据的格式 grok { match =&gt; { &quot;message&quot; =&gt; &quot;%{DATA:timestamp}\\|%{IP:serverIp}\\|%{IP:clientIp}\\|%{DATA:logSource}\\|%{DATA:userId}\\|%{DATA:reqUrl}\\|%{DATA:reqUri}\\|%{DATA:refer}\\|%{DATA:device}\\|%{DATA:textDuring}\\|%{DATA:duringTime:int}\\|\\|&quot;} } #定义时间戳的格式 date { match =&gt; [ &quot;timestamp&quot;, &quot;yyyy-MM-dd-HH:mm:ss&quot; ] locale =&gt; &quot;cn&quot; } #定义客户端的IP是哪个字段（上面定义的数据格式） geoip { source =&gt; &quot;clientIp&quot; } } 输出配置output output 是整个 logstash 的最终端。 常见的 output 如下： elasticsearch：高效的保存数据，并且能够方便和简单的进行查询 file：将 event 数据保存到文件中。 graphite：将 event 数据发送到图形化组件中（一个很流行的开源存储图形化展示的组件：http://graphite.wikidot.com/） statsd：statsd是一个统计服务，比如技术和时间统计，通过udp通讯，聚合一个或者多个后台服务 output 的配置一般为： 1234output { elasticsearch { hosts =&gt; &quot;127.0.0.1:9200&quot;} 1.3.1 Logstash处理Nginx日志 下载 logstash 1wget https://artifacts.elastic.co/downloads/logstash/logstash-7.15.1-linux-x86_64.tar.gz 解压并授权 123tar -xf logstash-7.15.1-linux-x86_64.tar.gzmv logstash-7.15.1 logstashchown -R elk:elk ./logstash 配置 logstash 123vim ./logstash/config/logstash.ymlhttp.host: 192.168.88.130http.port: 9600-9700 这里以处理 nginx 日志文件为例，配置 nginx 日志格式 12345678910111213141516171819# 在http块下添加log_format json '{&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,' '&quot;host&quot;:&quot;$server_addr&quot;,' '&quot;clientip&quot;:&quot;$remote_addr&quot;,' '&quot;remote_user&quot;:&quot;$remote_user&quot;,' '&quot;request&quot;:&quot;$request&quot;,' '&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,' '&quot;size&quot;:$body_bytes_sent,' '&quot;responsetime&quot;:$request_time,' '&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,' '&quot;upstreamhost&quot;:&quot;$upstream_addr&quot;,' '&quot;http_host&quot;:&quot;$host&quot;,' '&quot;url&quot;:&quot;$uri&quot;,' '&quot;domain&quot;:&quot;$host&quot;,' '&quot;xff&quot;:&quot;$http_x_forwarded_for&quot;,' '&quot;referer&quot;:&quot;$http_referer&quot;,' '&quot;status&quot;:&quot;$status&quot; }';access_log /var/log/nginx/access.log json; 添加处理配置文件 1234567891011121314151617181920212223242526272829303132vim ./logstash/config/elk_nginx_log.confinput { file { path =&gt; &quot;/var/log/messages&quot; type =&gt; &quot;system&quot; start_position =&gt; &quot;beginning&quot; } file { path =&gt; &quot;/var/log/nginx/access.log&quot; type =&gt;&quot;nginx-log&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; }}output { if [type] == &quot;system&quot;{ elasticsearch { hosts =&gt; [&quot;192.168.88.130:9200&quot;] index =&gt; &quot;systemlog-%{+YYYY.MM.dd}&quot; } } if [type] == &quot;nginx-log&quot;{ elasticsearch { hosts =&gt; [&quot;192.168.88.130:9200&quot;] index =&gt; &quot;nginx-log-%{+YYYY.MM.dd}&quot; } } stdout { codec =&gt; rubydebug }} 测试文件是否可用 1./bin/logstash -f ./config/elk_nginx_log.conf --config.test_and_exit 开启 logstash 1./bin/logstash -f ./config/elk_nginx_log.conf 在 kibana 创建 index pattern 在 Discover 就可以看到处理好的数据 1.4 Filebeat部署beat 是一个轻量级的日志采集器，早期的 ELK 架构都是由 logstash 去采集数据，这样对内存等资源的消耗会比较高，而 beat 用于采集日志的话，占用的资源几乎可以忽略不计。 beat 的种类有很多种，主要包括以下几种： Filebeat：日志文件（收集文件数据） Metricbeat：指标（收集系统、进程和文件系统级别的CPU和内存使用情况等数据），支持 Apache、HAProxy、MongoDB、MySQL、Nginx、PostgreSQL、Redis、System、Zookeeper 等服务 Packetbeat：网络数据（收集网络流量数据），支持 ICMP (v4 and v6)、DNS、HTTP、AMQP 0.9.1、Cassandra、Mysql、PostgreSQL、Redis、Thrift-RPC、MongoDB、Memcache 等 Winlogbeat：windows 事件日志（收集Windows事件日志数据） Audibeat：审计数据（收集审计日志） Heartbeat：运行时间监控（收集系统运行时的数据），支持 ICMP (v4 and v6) 、TCP、HTTP 等协议 Functionbeat：收集、传送并监测来自您的云服务的相关数据 Journalbeat：读取journald日志 1.4.1 FilebeatFilebeat 代替了 logstash 收集日志的工作，将收集好的日志直接发送给 logstash 进行过滤，在很大程度上减轻了服务器的压力，工作流程如下： Filebeat 会启动一个或多个实例去指定的日志目录查找数据（Input） 对于每个日志，Filebeat 都会启动一个 Harvester，每个 Harvester 都会将数据发送个 Spooler，再由 Spooler 发送给后端程序（Logstash、ES） Filebeat Nginx模块 下载 filebeat 123curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.15.1-linux-x86_64.tar.gztar -xf filebeat-7.15.1-linux-x86_64.tar.gzmv filebeat-7.15.1-linux-x86_64 filebeat 查看所有支持的模块 1./filebeat modules list 启动 nginx 模块 1./filebeat modules enable nginx 修改 logstash 规则文件并启动 123456789101112131415161718192021222324252627282930313233343536373839404142434445vim ./logstash/config/conf.d/elk_filebeat_nginx_log.confinput { beats { port =&gt; 5044 }}filter { grok { match =&gt; { &quot;message&quot; =&gt; &quot;%{IP:remote_addr} (?:%{DATA:remote_user}|-) \\[%{HTTPDATE:timestamp}\\] %{IPORHOST:http_host} %{DATA:request_method} %{DATA:request_uri} %{NUMBER:status} (?:%{NUMBER:body_bytes_sent}|-) (?:%{DATA:request_time}|-) \\&quot;(?:%{DATA:http_referer}|-)\\&quot; \\&quot;%{DATA:http_user_agent}\\&quot; (?:%{DATA:http_x_forwarded_for}|-) \\&quot;(?:%{DATA:http_cookie}|-)\\&quot;&quot; } } geoip { source =&gt; &quot;remote_addr&quot; } date { match =&gt; [ &quot;timestamp&quot;,&quot;dd/MMM/YYYY:HH:mm:ss Z&quot;] } useragent { source=&gt;&quot;http_user_agent&quot; } # 由于host中包含name，而es会把host看作一个json对象，需要转变成字符，否则会导致logstash无法传输数据给es mutate { rename =&gt; { &quot;[host][name]&quot; =&gt; &quot;host&quot; } }}output { elasticsearch { hosts =&gt; [&quot;192.168.88.130:9200&quot;] index =&gt; &quot;nginx-log-%{+YYYY.MM.dd}&quot; } stdout { codec =&gt; rubydebug }}./logstash/bin/logstash -f ./logstash/config/conf.d/elk_filebeat_nginx_log.conf 修改 nginx 模块文件 12345678vim modules.d/nginx.yml- module: nginx access: enabled: true var.paths: [&quot;/var/log/nginx/access.log*&quot;] error: enabled: true var.paths: [&quot;/var/log/nginx/error.log*&quot;] 配置 filebeat 12345678910111213vim ./filebeat/filebeat.yml# 通过nginx模块来实现，所以不开启filebeat.inputs:- type: log enabled: false paths: - /var/log/nginx/*.logfilebeat.config.modules: path: /home/elk/filebeat/modules.d/*.yml reload.enabled: false# 主要配置output.logstash: hosts: [&quot;192.168.88.130:5044&quot;] 启动 filebeat 123su elk./filebeat setup./filebeat -e 在 logstash 或 kibana 中就可以看到新的数据 Filebeat MySQL模块收集日志","link":"/2024/02/18/elk/"}],"tags":[{"name":"cisco","slug":"cisco","link":"/tags/cisco/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"ansible","slug":"ansible","link":"/tags/ansible/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"nexus","slug":"nexus","link":"/tags/nexus/"},{"name":"prometheus","slug":"prometheus","link":"/tags/prometheus/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"},{"name":"首页","slug":"首页","link":"/tags/%E9%A6%96%E9%A1%B5/"},{"name":"web","slug":"web","link":"/tags/web/"},{"name":"elk","slug":"elk","link":"/tags/elk/"}],"categories":[],"pages":[]}